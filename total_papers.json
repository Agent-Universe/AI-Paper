[
    {
        "Title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your\n  Phone",
        "Abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on\n3.3 trillion tokens, whose overall performance, as measured by both academic\nbenchmarks and internal testing, rivals that of models such as Mixtral 8x7B and\nGPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite\nbeing small enough to be deployed on a phone. The innovation lies entirely in\nour dataset for training, a scaled-up version of the one used for phi-2,\ncomposed of heavily filtered web data and synthetic data. The model is also\nfurther aligned for robustness, safety, and chat format. We also provide some\ninitial parameter-scaling results with a 7B and 14B models trained for 4.8T\ntokens, called phi-3-small and phi-3-medium, both significantly more capable\nthan phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on\nMT-bench).",
        "ArXiv Link": "https://arxiv.org/abs/2404.14219",
        "PDF Link": "https://arxiv.org/pdf/2404.14219",
        "Date": "2024-04-23",
        "Upvotes": "91"
    },
    {
        "Title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged\n  Instructions",
        "Abstract": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other\nattacks that allow adversaries to overwrite a model's original instructions\nwith their own malicious prompts. In this work, we argue that one of the\nprimary vulnerabilities underlying these attacks is that LLMs often consider\nsystem prompts (e.g., text from an application developer) to be the same\npriority as text from untrusted users and third parties. To address this, we\npropose an instruction hierarchy that explicitly defines how models should\nbehave when instructions of different priorities conflict. We then propose a\ndata generation method to demonstrate this hierarchical instruction following\nbehavior, which teaches LLMs to selectively ignore lower-privileged\ninstructions. We apply this method to GPT-3.5, showing that it drastically\nincreases robustness -- even for attack types not seen during training -- while\nimposing minimal degradations on standard capabilities.",
        "ArXiv Link": "https://arxiv.org/abs/2404.13208",
        "PDF Link": "https://arxiv.org/pdf/2404.13208",
        "Upvotes": "21",
        "Date": "2024-04-23"
    },
    {
        "Title": "How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study",
        "Abstract": "Meta's LLaMA family has become one of the most powerful open-source Large\nLanguage Model (LLM) series. Notably, LLaMA3 models have recently been released\nand achieve impressive performance across various with super-large scale\npre-training on over 15T tokens of data. Given the wide application of low-bit\nquantization for LLMs in resource-limited scenarios, we explore LLaMA3's\ncapabilities when quantized to low bit-width. This exploration holds the\npotential to unveil new insights and challenges for low-bit quantization of\nLLaMA3 and other forthcoming LLMs, especially in addressing performance\ndegradation problems that suffer in LLM compression. Specifically, we evaluate\nthe 10 existing post-training quantization and LoRA-finetuning methods of\nLLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's\nlow-bit quantization performance. Our experiment results indicate that LLaMA3\nstill suffers non-negligent degradation in these scenarios, especially in\nultra-low bit-width. This highlights the significant performance gap under low\nbit-width that needs to be bridged in future developments. We expect that this\nempirical study will prove valuable in advancing future models, pushing the\nLLMs to lower bit-width with higher accuracy for being practical. Our project\nis released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized\nLLaMA3 models are released in https://huggingface.co/LLMQ.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14047",
        "PDF Link": "https://arxiv.org/pdf/2404.14047",
        "Upvotes": "19",
        "Date": "2024-04-23"
    },
    {
        "Title": "FlowMind: Automatic Workflow Generation with LLMs",
        "Abstract": "The rapidly evolving field of Robotic Process Automation (RPA) has made\nsignificant strides in automating repetitive processes, yet its effectiveness\ndiminishes in scenarios requiring spontaneous or unpredictable tasks demanded\nby users. This paper introduces a novel approach, FlowMind, leveraging the\ncapabilities of Large Language Models (LLMs) such as Generative Pretrained\nTransformer (GPT), to address this limitation and create an automatic workflow\ngeneration system. In FlowMind, we propose a generic prompt recipe for a\nlecture that helps ground LLM reasoning with reliable Application Programming\nInterfaces (APIs). With this, FlowMind not only mitigates the common issue of\nhallucinations in LLMs, but also eliminates direct interaction between LLMs and\nproprietary data or code, thus ensuring the integrity and confidentiality of\ninformation - a cornerstone in financial services. FlowMind further simplifies\nuser interaction by presenting high-level descriptions of auto-generated\nworkflows, enabling users to inspect and provide feedback effectively. We also\nintroduce NCEN-QA, a new dataset in finance for benchmarking question-answering\ntasks from N-CEN reports on funds. We used NCEN-QA to evaluate the performance\nof workflows generated by FlowMind against baseline and ablation variants of\nFlowMind. We demonstrate the success of FlowMind, the importance of each\ncomponent in the proposed lecture recipe, and the effectiveness of user\ninteraction and feedback in FlowMind.",
        "ArXiv Link": "https://arxiv.org/abs/2404.13050",
        "PDF Link": "https://arxiv.org/pdf/2404.13050",
        "Upvotes": "11",
        "Date": "2024-04-23"
    },
    {
        "Title": "Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image\n  Synthesis",
        "Abstract": "Recently, a series of diffusion-aware distillation algorithms have emerged to\nalleviate the computational overhead associated with the multi-step inference\nprocess of Diffusion Models (DMs). Current distillation techniques often\ndichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii)\nODE Trajectory Reformulation. However, these approaches suffer from severe\nperformance degradation or domain shifts. To address these limitations, we\npropose Hyper-SD, a novel framework that synergistically amalgamates the\nadvantages of ODE Trajectory Preservation and Reformulation, while maintaining\nnear-lossless performance during step compression. Firstly, we introduce\nTrajectory Segmented Consistency Distillation to progressively perform\nconsistent distillation within pre-defined time-step segments, which\nfacilitates the preservation of the original ODE trajectory from a higher-order\nperspective. Secondly, we incorporate human feedback learning to boost the\nperformance of the model in a low-step regime and mitigate the performance loss\nincurred by the distillation process. Thirdly, we integrate score distillation\nto further improve the low-step generation capability of the model and offer\nthe first attempt to leverage a unified LoRA to support the inference process\nat all steps. Extensive experiments and user studies demonstrate that Hyper-SD\nachieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.\nFor example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and\n+0.51 in Aes Score in the 1-step inference.",
        "ArXiv Link": "https://arxiv.org/abs/2404.13686",
        "PDF Link": "https://arxiv.org/pdf/2404.13686",
        "Upvotes": "10",
        "Date": "2024-04-23"
    },
    {
        "Title": "A Multimodal Automated Interpretability Agent",
        "Abstract": "This paper describes MAIA, a Multimodal Automated Interpretability Agent.\nMAIA is a system that uses neural models to automate neural model understanding\ntasks like feature interpretation and failure mode discovery. It equips a\npre-trained vision-language model with a set of tools that support iterative\nexperimentation on subcomponents of other models to explain their behavior.\nThese include tools commonly used by human interpretability researchers: for\nsynthesizing and editing inputs, computing maximally activating exemplars from\nreal-world datasets, and summarizing and describing experimental results.\nInterpretability experiments proposed by MAIA compose these tools to describe\nand explain system behavior. We evaluate applications of MAIA to computer\nvision models. We first characterize MAIA's ability to describe (neuron-level)\nfeatures in learned representations of images. Across several trained models\nand a novel dataset of synthetic vision neurons with paired ground-truth\ndescriptions, MAIA produces descriptions comparable to those generated by\nexpert human experimenters. We then show that MAIA can aid in two additional\ninterpretability tasks: reducing sensitivity to spurious features, and\nautomatically identifying inputs likely to be mis-classified.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14394",
        "PDF Link": "https://arxiv.org/pdf/2404.14394",
        "Upvotes": "8",
        "Date": "2024-04-23"
    },
    {
        "Title": "SEED-X: Multimodal Models with Unified Multi-granularity Comprehension\n  and Generation",
        "Abstract": "The rapid evolution of multimodal foundation model has demonstrated\nsignificant progresses in vision-language understanding and generation, e.g.,\nour previous work SEED-LLaMA. However, there remains a gap between its\ncapability and the real-world applicability, primarily due to the model's\nlimited capacity to effectively respond to various user instructions and\ninteract with diverse visual data. In this work, we focus on bridging this gap\nthrough integrating two enhanced features: (1) comprehending images of\narbitrary sizes and ratios, and (2) enabling multi-granularity image\ngeneration. We present a unified and versatile foundation model, namely,\nSEED-X, which is able to model multi-granularity visual semantics for\ncomprehension and generation tasks. Besides the competitive results on public\nbenchmarks, SEED-X demonstrates its effectiveness in handling real-world\napplications across various domains after instruction tuning. We hope that our\nwork will inspire future research into what can be achieved by versatile\nmultimodal foundation models in real-world applications. The models, codes, and\ndatasets will be released in https://github.com/AILab-CVC/SEED-X.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14396",
        "PDF Link": "https://arxiv.org/pdf/2404.14396",
        "Upvotes": "8",
        "Date": "2024-04-23"
    },
    {
        "Title": "Music Consistency Models",
        "Abstract": "Consistency models have exhibited remarkable capabilities in facilitating\nefficient image/video generation, enabling synthesis with minimal sampling\nsteps. It has proven to be advantageous in mitigating the computational burdens\nassociated with diffusion models. Nevertheless, the application of consistency\nmodels in music generation remains largely unexplored. To address this gap, we\npresent Music Consistency Models (MusicCM), which leverages the\nconcept of consistency models to efficiently synthesize mel-spectrogram for\nmusic clips, maintaining high quality while minimizing the number of sampling\nsteps. Building upon existing text-to-music diffusion models, the\nMusicCM model incorporates consistency distillation and adversarial\ndiscriminator training. Moreover, we find it beneficial to generate extended\ncoherent music by incorporating multiple diffusion processes with shared\nconstraints. Experimental results reveal the effectiveness of our model in\nterms of computational efficiency, fidelity, and naturalness. Notable,\nMusicCM achieves seamless music synthesis with a mere four sampling\nsteps, e.g., only one second per minute of the music clip, showcasing the\npotential for real-time application.",
        "ArXiv Link": "https://arxiv.org/abs/2404.13358",
        "PDF Link": "https://arxiv.org/pdf/2404.13358",
        "Upvotes": "7",
        "Date": "2024-04-23"
    },
    {
        "Title": "MultiBooth: Towards Generating All Your Concepts in an Image from Text",
        "Abstract": "This paper introduces MultiBooth, a novel and efficient technique for\nmulti-concept customization in image generation from text. Despite the\nsignificant advancements in customized generation methods, particularly with\nthe success of diffusion models, existing methods often struggle with\nmulti-concept scenarios due to low concept fidelity and high inference cost.\nMultiBooth addresses these issues by dividing the multi-concept generation\nprocess into two phases: a single-concept learning phase and a multi-concept\nintegration phase. During the single-concept learning phase, we employ a\nmulti-modal image encoder and an efficient concept encoding technique to learn\na concise and discriminative representation for each concept. In the\nmulti-concept integration phase, we use bounding boxes to define the generation\narea for each concept within the cross-attention map. This method enables the\ncreation of individual concepts within their specified regions, thereby\nfacilitating the formation of multi-concept images. This strategy not only\nimproves concept fidelity but also reduces additional inference cost.\nMultiBooth surpasses various baselines in both qualitative and quantitative\nevaluations, showcasing its superior performance and computational efficiency.\nProject Page: https://multibooth.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2404.14239",
        "PDF Link": "https://arxiv.org/pdf/2404.14239",
        "Upvotes": "4",
        "Date": "2024-04-23"
    },
    {
        "Title": "Learning H-Infinity Locomotion Control",
        "Abstract": "Stable locomotion in precipitous environments is an essential capability of\nquadruped robots, demanding the ability to resist various external\ndisturbances. However, recent learning-based policies only use basic domain\nrandomization to improve the robustness of learned policies, which cannot\nguarantee that the robot has adequate disturbance resistance capabilities. In\nthis paper, we propose to model the learning process as an adversarial\ninteraction between the actor and a newly introduced disturber and ensure their\noptimization with H_{infty} constraint. In contrast to the actor that\nmaximizes the discounted overall reward, the disturber is responsible for\ngenerating effective external forces and is optimized by maximizing the error\nbetween the task reward and its oracle, i.e., \"cost\" in each iteration. To keep\njoint optimization between the actor and the disturber stable, our H_{infty}\nconstraint mandates the bound of ratio between the cost to the intensity of the\nexternal forces. Through reciprocal interaction throughout the training phase,\nthe actor can acquire the capability to navigate increasingly complex physical\ndisturbances. We verify the robustness of our approach on quadrupedal\nlocomotion tasks with Unitree Aliengo robot, and also a more challenging task\nwith Unitree A1 robot, where the quadruped is expected to perform locomotion\nmerely on its hind legs as if it is a bipedal robot. The simulated quantitative\nresults show improvement against baselines, demonstrating the effectiveness of\nthe method and each design choice. On the other hand, real-robot experiments\nqualitatively exhibit how robust the policy is when interfering with various\ndisturbances on various terrains, including stairs, high platforms, slopes, and\nslippery terrains. All code, checkpoints, and real-world deployment guidance\nwill be made public.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14405",
        "PDF Link": "https://arxiv.org/pdf/2404.14405",
        "Upvotes": "3",
        "Date": "2024-04-23"
    },
    {
        "Title": "Scene Coordinate Reconstruction: Posing of Image Collections via\n  Incremental Learning of a Relocalizer",
        "Abstract": "We address the task of estimating camera parameters from a set of images\ndepicting a scene. Popular feature-based structure-from-motion (SfM) tools\nsolve this task by incremental reconstruction: they repeat triangulation of\nsparse 3D points and registration of more camera views to the sparse point\ncloud. We re-interpret incremental structure-from-motion as an iterated\napplication and refinement of a visual relocalizer, that is, of a method that\nregisters new views to the current state of the reconstruction. This\nperspective allows us to investigate alternative visual relocalizers that are\nnot rooted in local feature matching. We show that scene coordinate regression,\na learning-based relocalization approach, allows us to build implicit, neural\nscene representations from unposed images. Different from other learning-based\nreconstruction methods, we do not require pose priors nor sequential inputs,\nand we optimize efficiently over thousands of images. Our method, ACE0 (ACE\nZero), estimates camera poses to an accuracy comparable to feature-based SfM,\nas demonstrated by novel view synthesis. Project page:\nhttps://nianticlabs.github.io/acezero/",
        "ArXiv Link": "https://arxiv.org/abs/2404.14351",
        "PDF Link": "https://arxiv.org/pdf/2404.14351",
        "Upvotes": "3",
        "Date": "2024-04-23"
    },
    {
        "Title": "OpenELM: An Efficient Language Model Family with Open-source Training\n  and Inference Framework",
        "Abstract": "The reproducibility and transparency of large language models are crucial for\nadvancing open research, ensuring the trustworthiness of results, and enabling\ninvestigations into data and model biases, as well as potential risks. To this\nend, we release OpenELM, a state-of-the-art open language model. OpenELM uses a\nlayer-wise scaling strategy to efficiently allocate parameters within each\nlayer of the transformer model, leading to enhanced accuracy. For example, with\na parameter budget of approximately one billion parameters, OpenELM exhibits a\n2.36% improvement in accuracy compared to OLMo while requiring 2times fewer\npre-training tokens.\n  Diverging from prior practices that only provide model weights and inference\ncode, and pre-train on private datasets, our release includes the complete\nframework for training and evaluation of the language model on publicly\navailable datasets, including training logs, multiple checkpoints, and\npre-training configurations. We also release code to convert models to MLX\nlibrary for inference and fine-tuning on Apple devices. This comprehensive\nrelease aims to empower and strengthen the open research community, paving the\nway for future open research endeavors.\n  Our source code along with pre-trained model weights and training recipes is\navailable at https://github.com/apple/corenet. Additionally, \\model\nmodels can be found on HuggingFace at:\nhttps://huggingface.co/apple/OpenELM.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14619",
        "PDF Link": "https://arxiv.org/pdf/2404.14619",
        "Upvotes": "4",
        "Date": "2024-04-24"
    },
    {
        "Title": "SnapKV: LLM Knows What You are Looking for Before Generation",
        "Abstract": "Large Language Models (LLMs) have made remarkable progress in processing\nextensive contexts, with the Key-Value (KV) cache playing a vital role in\nenhancing their performance. However, the growth of the KV cache in response to\nincreasing input length poses challenges to memory and time efficiency. To\naddress this problem, this paper introduces SnapKV, an innovative and\nfine-tuning-free approach that efficiently minimizes KV cache size while still\ndelivering comparable performance in real-world applications.\n  We discover that each attention head in the model consistently focuses on\nspecific prompt attention features during generation. Meanwhile, this robust\npattern can be obtained from an `observation' window located at the end of the\nprompts. Drawing on this insight, SnapKV automatically compresses KV caches by\nselecting clustered important KV positions for each attention head. Our\napproach significantly reduces the growing computational overhead and memory\nfootprint when processing long input sequences. Specifically, SnapKV achieves a\nconsistent decoding speed with a 3.6x increase in generation speed and an 8.2x\nenhancement in memory efficiency compared to baseline when processing inputs of\n16K tokens. At the same time, it maintains comparable performance to baseline\nmodels across 16 long sequence datasets. Moreover, SnapKV can process up to\n380K context tokens on a single A100-80GB GPU using HuggingFace implementation\nwith minor changes, exhibiting only a negligible accuracy drop in the\nNeedle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's\npotential for practical applications.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14469",
        "PDF Link": "https://arxiv.org/pdf/2404.14469",
        "Upvotes": "4",
        "Date": "2024-04-24"
    },
    {
        "Title": "Multi-Head Mixture-of-Experts",
        "Abstract": "Sparse Mixtures of Experts (SMoE) scales model capacity without significant\nincreases in training and inference costs, but exhibits the following two\nissues: (1) Low expert activation, where only a small subset of experts are\nactivated for optimization. (2) Lacking fine-grained analytical capabilities\nfor multiple semantic concepts within individual tokens. We propose Multi-Head\nMixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each\ntoken into multiple sub-tokens. These sub-tokens are then assigned to and\nprocessed by a diverse set of experts in parallel, and seamlessly reintegrated\ninto the original token form. The multi-head mechanism enables the model to\ncollectively attend to information from various representation spaces within\ndifferent experts, while significantly enhances expert activation, thus deepens\ncontext understanding and alleviate overfitting. Moreover, our MH-MoE is\nstraightforward to implement and decouples from other SMoE optimization\nmethods, making it easy to integrate with other SMoE models for enhanced\nperformance. Extensive experimental results across three tasks: English-focused\nlanguage modeling, Multi-lingual language modeling and Masked multi-modality\nmodeling tasks, demonstrate the effectiveness of MH-MoE.",
        "ArXiv Link": "https://arxiv.org/abs/2404.15045",
        "PDF Link": "https://arxiv.org/pdf/2404.15045",
        "Upvotes": "3",
        "Date": "2024-04-24"
    },
    {
        "Title": "Align Your Steps: Optimizing Sampling Schedules in Diffusion Models",
        "Abstract": "Diffusion models (DMs) have established themselves as the state-of-the-art\ngenerative modeling approach in the visual domain and beyond. A crucial\ndrawback of DMs is their slow sampling speed, relying on many sequential\nfunction evaluations through large neural networks. Sampling from DMs can be\nseen as solving a differential equation through a discretized set of noise\nlevels known as the sampling schedule. While past works primarily focused on\nderiving efficient solvers, little attention has been given to finding optimal\nsampling schedules, and the entire literature relies on hand-crafted\nheuristics. In this work, for the first time, we propose a general and\nprincipled approach to optimizing the sampling schedules of DMs for\nhigh-quality outputs, called Align Your Steps. We leverage methods\nfrom stochastic calculus and find optimal schedules specific to different\nsolvers, trained DMs and datasets. We evaluate our novel approach on several\nimage, video as well as 2D toy data synthesis benchmarks, using a variety of\ndifferent samplers, and observe that our optimized schedules outperform\nprevious hand-crafted schedules in almost all experiments. Our method\ndemonstrates the untapped potential of sampling schedule optimization,\nespecially in the few-step synthesis regime.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14507",
        "PDF Link": "https://arxiv.org/pdf/2404.14507",
        "Upvotes": "3",
        "Date": "2024-04-24"
    },
    {
        "Title": "FlashSpeech: Efficient Zero-Shot Speech Synthesis",
        "Abstract": "Recent progress in large-scale zero-shot speech synthesis has been\nsignificantly advanced by language models and diffusion models. However, the\ngeneration process of both methods is slow and computationally intensive.\nEfficient speech synthesis using a lower computing budget to achieve quality on\npar with previous work remains a significant challenge. In this paper, we\npresent FlashSpeech, a large-scale zero-shot speech synthesis system with\napproximately 5\\% of the inference time compared with previous work.\nFlashSpeech is built on the latent consistency model and applies a novel\nadversarial consistency training approach that can train from scratch without\nthe need for a pre-trained diffusion model as the teacher. Furthermore, a new\nprosody generator module enhances the diversity of prosody, making the rhythm\nof the speech sound more natural. The generation processes of FlashSpeech can\nbe achieved efficiently with one or two sampling steps while maintaining high\naudio quality and high similarity to the audio prompt for zero-shot speech\ngeneration. Our experimental results demonstrate the superior performance of\nFlashSpeech. Notably, FlashSpeech can be about 20 times faster than other\nzero-shot speech synthesis systems while maintaining comparable performance in\nterms of voice quality and similarity. Furthermore, FlashSpeech demonstrates\nits versatility by efficiently performing tasks like voice conversion, speech\nediting, and diverse speech sampling. Audio samples can be found in\nhttps://flashspeech.github.io/.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14700",
        "PDF Link": "https://arxiv.org/pdf/2404.14700",
        "Upvotes": "2",
        "Date": "2024-04-24"
    },
    {
        "Title": "Pegasus-v1 Technical Report",
        "Abstract": "This technical report introduces Pegasus-1, a multimodal language model\nspecialized in video content understanding and interaction through natural\nlanguage. Pegasus-1 is designed to address the unique challenges posed by video\ndata, such as interpreting spatiotemporal information, to offer nuanced video\ncontent comprehension across various lengths. This technical report overviews\nPegasus-1's architecture, training strategies, and its performance in\nbenchmarks on video conversation, zero-shot video question answering, and video\nsummarization. We also explore qualitative characteristics of Pegasus-1 ,\ndemonstrating its capabilities as well as its limitations, in order to provide\nreaders a balanced view of its current state and its future direction.",
        "ArXiv Link": "https://arxiv.org/abs/2404.14687",
        "PDF Link": "https://arxiv.org/pdf/2404.14687",
        "Upvotes": "2",
        "Date": "2024-04-24"
    },
    {
        "Title": "CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster\n  Pre-training on Web-scale Image-Text Data",
        "Abstract": "Contrastive learning has emerged as a transformative method for learning\neffective visual representations through the alignment of image and text\nembeddings. However, pairwise similarity computation in contrastive loss\nbetween image and text pairs poses computational challenges. This paper\npresents a novel weakly supervised pre-training of vision models on web-scale\nimage-text data. The proposed method reframes pre-training on image-text data\nas a classification task. Consequently, it eliminates the need for pairwise\nsimilarity computations in contrastive loss, achieving a remarkable 2.7times\nacceleration in training speed compared to contrastive learning on web-scale\ndata. Through extensive experiments spanning diverse vision tasks, including\ndetection and segmentation, we demonstrate that the proposed method maintains\nhigh representation quality. Our source code along with pre-trained model\nweights and training recipes is available at\nhttps://github.com/apple/corenet.",
        "ArXiv Link": "https://arxiv.org/abs/2404.15653",
        "PDF Link": "https://arxiv.org/pdf/2404.15653",
        "Upvotes": "14",
        "Date": "2024-04-26"
    },
    {
        "Title": "PuLID: Pure and Lightning ID Customization via Contrastive Alignment",
        "Abstract": "We propose Pure and Lightning ID customization (PuLID), a novel tuning-free\nID customization method for text-to-image generation. By incorporating a\nLightning T2I branch with a standard diffusion one, PuLID introduces both\ncontrastive alignment loss and accurate ID loss, minimizing disruption to the\noriginal model and ensuring high ID fidelity. Experiments show that PuLID\nachieves superior performance in both ID fidelity and editability. Another\nattractive property of PuLID is that the image elements (e.g., background,\nlighting, composition, and style) before and after the ID insertion are kept as\nconsistent as possible. Codes and models will be available at\nhttps://github.com/ToTheBeginning/PuLID",
        "ArXiv Link": "https://arxiv.org/abs/2404.16022",
        "PDF Link": "https://arxiv.org/pdf/2404.16022",
        "Upvotes": "10",
        "Date": "2024-04-26"
    },
    {
        "Title": "MoDE: CLIP Data Experts via Clustering",
        "Abstract": "The success of contrastive language-image pretraining (CLIP) relies on the\nsupervision from the pairing between images and captions, which tends to be\nnoisy in web-crawled data. We present Mixture of Data Experts (MoDE) and learn\na system of CLIP data experts via clustering. Each data expert is trained on\none data cluster, being less sensitive to false negative noises in other\nclusters. At inference time, we ensemble their outputs by applying weights\ndetermined through the correlation between task metadata and cluster\nconditions. To estimate the correlation precisely, the samples in one cluster\nshould be semantically similar, but the number of data experts should still be\nreasonable for training and inference. As such, we consider the ontology in\nhuman language and propose to use fine-grained cluster centers to represent\neach data expert at a coarse-grained level. Experimental studies show that four\nCLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and\nOpenCLIP on zero-shot image classification but with less (<35\\%) training\ncost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly\ninclude new data experts. The code is available at\nhttps://github.com/facebookresearch/MetaCLIP/tree/main/mode.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16030",
        "PDF Link": "https://arxiv.org/pdf/2404.16030",
        "Upvotes": "8",
        "Date": "2024-04-26"
    },
    {
        "Title": "Editable Image Elements for Controllable Synthesis",
        "Abstract": "Diffusion models have made significant advances in text-guided synthesis\ntasks. However, editing user-provided images remains challenging, as the high\ndimensional noise input space of diffusion models is not naturally suited for\nimage inversion or spatial editing. In this work, we propose an image\nrepresentation that promotes spatial editing of input images using a diffusion\nmodel. Concretely, we learn to encode an input into \"image elements\" that can\nfaithfully reconstruct an input image. These elements can be intuitively edited\nby a user, and are decoded by a diffusion model into realistic images. We show\nthe effectiveness of our representation on various image editing tasks, such as\nobject resizing, rearrangement, dragging, de-occlusion, removal, variation, and\nimage composition. Project page:\nhttps://jitengmu.github.io/Editable_Image_Elements/",
        "ArXiv Link": "https://arxiv.org/abs/2404.16029",
        "PDF Link": "https://arxiv.org/pdf/2404.16029",
        "Upvotes": "8",
        "Date": "2024-04-26"
    },
    {
        "Title": "MotionMaster: Training-free Camera Motion Transfer For Video Generation",
        "Abstract": "The emergence of diffusion models has greatly propelled the progress in image\nand video generation. Recently, some efforts have been made in controllable\nvideo generation, including text-to-video generation and video motion control,\namong which camera motion control is an important topic. However, existing\ncamera motion control methods rely on training a temporal camera module, and\nnecessitate substantial computation resources due to the large amount of\nparameters in video generation models. Moreover, existing methods pre-define\ncamera motion types during training, which limits their flexibility in camera\ncontrol. Therefore, to reduce training costs and achieve flexible camera\ncontrol, we propose COMD, a novel training-free video motion transfer model,\nwhich disentangles camera motions and object motions in source videos and\ntransfers the extracted camera motions to new videos. We first propose a\none-shot camera motion disentanglement method to extract camera motion from a\nsingle source video, which separates the moving objects from the background and\nestimates the camera motion in the moving objects region based on the motion in\nthe background by solving a Poisson equation. Furthermore, we propose a\nfew-shot camera motion disentanglement method to extract the common camera\nmotion from multiple videos with similar camera motions, which employs a\nwindow-based clustering technique to extract the common features in temporal\nattention maps of multiple videos. Finally, we propose a motion combination\nmethod to combine different types of camera motions together, enabling our\nmodel a more controllable and flexible camera control. Extensive experiments\ndemonstrate that our training-free approach can effectively decouple\ncamera-object motion and apply the decoupled camera motion to a wide range of\ncontrollable video generation tasks, achieving flexible and diverse camera\nmotion control.",
        "ArXiv Link": "https://arxiv.org/abs/2404.15789",
        "PDF Link": "https://arxiv.org/pdf/2404.15789",
        "Upvotes": "7",
        "Date": "2024-04-26"
    },
    {
        "Title": "ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with\n  Reward Feedback Learning",
        "Abstract": "The rapid development of diffusion models has triggered diverse applications.\nIdentity-preserving text-to-image generation (ID-T2I) particularly has received\nsignificant attention due to its wide range of application scenarios like AI\nportrait and advertising. While existing ID-T2I methods have demonstrated\nimpressive results, several key challenges remain: (1) It is hard to maintain\nthe identity characteristics of reference portraits accurately, (2) The\ngenerated images lack aesthetic appeal especially while enforcing identity\nretention, and (3) There is a limitation that cannot be compatible with\nLoRA-based and Adapter-based methods simultaneously. To address these issues,\nwe present ID-Aligner, a general feedback learning framework to\nenhance ID-T2I performance. To resolve identity features lost, we introduce\nidentity consistency reward fine-tuning to utilize the feedback from face\ndetection and recognition models to improve generated identity preservation.\nFurthermore, we propose identity aesthetic reward fine-tuning leveraging\nrewards from human-annotated preference data and automatically constructed\nfeedback on character structure generation to provide aesthetic tuning signals.\nThanks to its universal feedback fine-tuning framework, our method can be\nreadily applied to both LoRA and Adapter models, achieving consistent\nperformance gains. Extensive experiments on SD1.5 and SDXL diffusion models\nvalidate the effectiveness of our approach. Project Page:\n\\url{https://idaligner.github.io/}",
        "ArXiv Link": "https://arxiv.org/abs/2404.15449",
        "PDF Link": "https://arxiv.org/pdf/2404.15449",
        "Upvotes": "7",
        "Date": "2024-04-26"
    },
    {
        "Title": "MaGGIe: Masked Guided Gradual Human Instance Matting",
        "Abstract": "Human matting is a foundation task in image and video processing, where human\nforeground pixels are extracted from the input. Prior works either improve the\naccuracy by additional guidance or improve the temporal consistency of a single\ninstance across frames. We propose a new framework MaGGIe, Masked Guided\nGradual Human Instance Matting, which predicts alpha mattes progressively for\neach human instances while maintaining the computational cost, precision, and\nconsistency. Our method leverages modern architectures, including transformer\nattention and sparse convolution, to output all instance mattes simultaneously\nwithout exploding memory and latency. Although keeping constant inference costs\nin the multiple-instance scenario, our framework achieves robust and versatile\nperformance on our proposed synthesized benchmarks. With the higher quality\nimage and video matting benchmarks, the novel multi-instance synthesis approach\nfrom publicly available sources is introduced to increase the generalization of\nmodels in real-world scenarios.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16035",
        "PDF Link": "https://arxiv.org/pdf/2404.16035",
        "Upvotes": "6",
        "Date": "2024-04-26"
    },
    {
        "Title": "BASS: Batched Attention-optimized Speculative Sampling",
        "Abstract": "Speculative decoding has emerged as a powerful method to improve latency and\nthroughput in hosting large language models. However, most existing\nimplementations focus on generating a single sequence. Real-world generative AI\napplications often require multiple responses and how to perform speculative\ndecoding in a batched setting while preserving its latency benefits poses\nnon-trivial challenges. This paper describes a system of batched speculative\ndecoding that sets a new state of the art in multi-sequence generation latency\nand that demonstrates superior GPU utilization as well as quality of\ngenerations within a time budget. For example, for a 7.8B-size model on a\nsingle A100 GPU and with a batch size of 8, each sequence is generated at an\naverage speed of 5.8ms per token, the overall throughput being 1.1K tokens per\nsecond. These results represent state-of-the-art latency and a 2.15X speed-up\nover optimized regular decoding. Within a time budget that regular decoding\ndoes not finish, our system is able to generate sequences with HumanEval\nPass@First of 43% and Pass@All of 61%, far exceeding what's feasible with\nsingle-sequence speculative decoding. Our peak GPU utilization during decoding\nreaches as high as 15.8%, more than 3X the highest of that of regular decoding\nand around 10X of single-sequence speculative decoding.",
        "ArXiv Link": "https://arxiv.org/abs/2404.15778",
        "PDF Link": "https://arxiv.org/pdf/2404.15778",
        "Upvotes": "5",
        "Date": "2024-04-26"
    },
    {
        "Title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
        "Abstract": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
        "ArXiv Link": "https://arxiv.org/abs/2404.15420",
        "PDF Link": "https://arxiv.org/pdf/2404.15420",
        "Upvotes": "4",
        "Date": "2024-04-26"
    },
    {
        "Title": "Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding",
        "Abstract": "We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16710",
        "PDF Link": "https://arxiv.org/pdf/2404.16710",
        "Upvotes": "18",
        "Date": "2024-04-26"
    },
    {
        "Title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal\n  Models with Open-Source Suites",
        "Abstract": "In this report, we introduce InternVL 1.5, an open-source multimodal large\nlanguage model (MLLM) to bridge the capability gap between open-source and\nproprietary commercial models in multimodal understanding. We introduce three\nsimple improvements: (1) Strong Vision Encoder: we explored a continuous\nlearning strategy for the large-scale vision foundation model -- InternViT-6B,\nboosting its visual understanding capabilities, and making it can be\ntransferred and reused in different LLMs. (2) Dynamic High-Resolution: we\ndivide images into tiles ranging from 1 to 40 of 448times448 pixels\naccording to the aspect ratio and resolution of the input images, which\nsupports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we\ncarefully collected a high-quality bilingual dataset that covers common scenes,\ndocument images, and annotated them with English and Chinese question-answer\npairs, significantly enhancing performance in OCR- and Chinese-related tasks.\nWe evaluate InternVL 1.5 through a series of benchmarks and comparative\nstudies. Compared to both open-source and proprietary models, InternVL 1.5\nshows competitive performance, achieving state-of-the-art results in 8 of 18\nbenchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16821",
        "PDF Link": "https://arxiv.org/pdf/2404.16821",
        "Upvotes": "15",
        "Date": "2024-04-26"
    },
    {
        "Title": "Make Your LLM Fully Utilize the Context",
        "Abstract": "While many contemporary large language models (LLMs) can process lengthy\ninput, they still struggle to fully utilize information within the long\ncontext, known as the lost-in-the-middle challenge. We hypothesize that it\nstems from insufficient explicit supervision during the long-context training,\nwhich fails to emphasize that any position in a long context can hold crucial\ninformation. Based on this intuition, our study presents information-intensive\n(IN2) training, a purely data-driven solution to overcome lost-in-the-middle.\nSpecifically, IN2 training leverages a synthesized long-context question-answer\ndataset, where the answer requires (1) fine-grained information awareness on a\nshort segment (~128 tokens) within a synthesized long context (4K-32K tokens),\nand (2) the integration and reasoning of information from two or more short\nsegments. Through applying this information-intensive training on Mistral-7B,\nwe present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of\nFILM-7B for utilizing long contexts, we design three probing tasks that\nencompass various context styles (document, code, and structured-data context)\nand information retrieval patterns (forward, backward, and bi-directional\nretrieval). The probing results demonstrate that FILM-7B can robustly retrieve\ninformation from different positions in its 32K context window. Beyond these\nprobing tasks, FILM-7B significantly improves the performance on real-world\nlong-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while\nmaintaining a comparable performance on short-context tasks (e.g., 59.3->59.2\naccuracy on MMLU). Github Link: https://github.com/microsoft/FILM.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16811",
        "PDF Link": "https://arxiv.org/pdf/2404.16811",
        "Upvotes": "13",
        "Date": "2024-04-26"
    },
    {
        "Title": "Interactive3D: Create What You Want by Interactive 3D Generation",
        "Abstract": "3D object generation has undergone significant advancements, yielding\nhigh-quality results. However, fall short of achieving precise user control,\noften yielding results that do not align with user expectations, thus limiting\ntheir applicability. User-envisioning 3D object generation faces significant\nchallenges in realizing its concepts using current generative models due to\nlimited interaction capabilities. Existing methods mainly offer two approaches:\n(i) interpreting textual instructions with constrained controllability, or (ii)\nreconstructing 3D objects from 2D images. Both of them limit customization to\nthe confines of the 2D reference and potentially introduce undesirable\nartifacts during the 3D lifting process, restricting the scope for direct and\nversatile 3D modifications. In this work, we introduce Interactive3D, an\ninnovative framework for interactive 3D generation that grants users precise\ncontrol over the generative process through extensive 3D interaction\ncapabilities. Interactive3D is constructed in two cascading stages, utilizing\ndistinct 3D representations. The first stage employs Gaussian Splatting for\ndirect user interaction, allowing modifications and guidance of the generative\ndirection at any intermediate step through (i) Adding and Removing components,\n(ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv)\nSemantic Editing. Subsequently, the Gaussian splats are transformed into\nInstantNGP. We introduce a novel (v) Interactive Hash Refinement module to\nfurther add details and extract the geometry in the second stage. Our\nexperiments demonstrate that Interactive3D markedly improves the\ncontrollability and quality of 3D generation. Our project webpage is available\nat https://interactive-3d.github.io/.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16510",
        "PDF Link": "https://arxiv.org/pdf/2404.16510",
        "Upvotes": "8",
        "Date": "2024-04-26"
    },
    {
        "Title": "NeRF-XL: Scaling NeRFs with Multiple GPUs",
        "Abstract": "We present NeRF-XL, a principled method for distributing Neural Radiance\nFields (NeRFs) across multiple GPUs, thus enabling the training and rendering\nof NeRFs with an arbitrarily large capacity. We begin by revisiting existing\nmulti-GPU approaches, which decompose large scenes into multiple independently\ntrained NeRFs, and identify several fundamental issues with these methods that\nhinder improvements in reconstruction quality as additional computational\nresources (GPUs) are used in training. NeRF-XL remedies these issues and\nenables the training and rendering of NeRFs with an arbitrary number of\nparameters by simply using more hardware. At the core of our method lies a\nnovel distributed training and rendering formulation, which is mathematically\nequivalent to the classic single-GPU case and minimizes communication between\nGPUs. By unlocking NeRFs with arbitrarily large parameter counts, our approach\nis the first to reveal multi-GPU scaling laws for NeRFs, showing improvements\nin reconstruction quality with larger parameter counts and speed improvements\nwith more GPUs. We demonstrate the effectiveness of NeRF-XL on a wide variety\nof datasets, including the largest open-source dataset to date, MatrixCity,\ncontaining 258K images covering a 25km^2 city area.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16221",
        "PDF Link": "https://arxiv.org/pdf/2404.16221",
        "Upvotes": "7",
        "Date": "2024-04-26"
    },
    {
        "Title": "Tele-FLM Technical Report",
        "Abstract": "Large language models (LLMs) have showcased profound capabilities in language\nunderstanding and generation, facilitating a wide array of applications.\nHowever, there is a notable paucity of detailed, open-sourced methodologies on\nefficiently scaling LLMs beyond 50 billion parameters with minimum\ntrial-and-error cost and computational resources. In this report, we introduce\nTele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that\nfeatures a stable, efficient pre-training paradigm and enhanced factual\njudgment capabilities. Tele-FLM demonstrates superior multilingual language\nmodeling abilities, measured by BPB on textual corpus. Besides, in both English\nand Chinese foundation model evaluation, it is comparable to strong\nopen-sourced models that involve larger pre-training FLOPs, such as Llama2-70B\nand DeepSeek-67B. In addition to the model weights, we share the core designs,\nengineering practices, and training details, which we expect to benefit both\nthe academic and industrial communities.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16645",
        "PDF Link": "https://arxiv.org/pdf/2404.16645",
        "Upvotes": "7",
        "Date": "2024-04-26"
    },
    {
        "Title": "List Items One by One: A New Data Source and Learning Paradigm for\n  Multimodal LLMs",
        "Abstract": "Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of\nGPT-4V, by enabling the model to associate visual objects with tags inserted on\nthe image. These tags, marked with alphanumerics, can be indexed via text\ntokens for easy reference. Despite the extraordinary performance from GPT-4V,\nwe observe that other Multimodal Large Language Models (MLLMs) struggle to\nunderstand these visual tags. To promote the learning of SoM prompting for\nopen-source models, we propose a new learning paradigm: \"list items one by\none,\" which asks the model to enumerate and describe all visual tags placed on\nthe image following the alphanumeric orders of tags. By integrating our curated\ndataset with other visual instruction tuning datasets, we are able to equip\nexisting MLLMs with the SoM prompting ability. Furthermore, we evaluate our\nfinetuned SoM models on five MLLM benchmarks. We find that this new dataset,\neven in a relatively small size (10k-30k images with tags), significantly\nenhances visual reasoning capabilities and reduces hallucinations for MLLMs.\nPerhaps surprisingly, these improvements persist even when the visual tags are\nomitted from input images during inference. This suggests the potential of\n\"list items one by one\" as a new paradigm for training MLLMs, which strengthens\nthe object-text alignment through the use of visual tags in the training stage.\nFinally, we conduct analyses by probing trained models to understand the\nworking mechanism of SoM. Our code and data are available at\nhttps://github.com/zzxslp/SoM-LLaVA.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16375",
        "PDF Link": "https://arxiv.org/pdf/2404.16375",
        "Upvotes": "4",
        "Date": "2024-04-26"
    },
    {
        "Title": "ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity\n  Preserving",
        "Abstract": "Diffusion-based technologies have made significant strides, particularly in\npersonalized and customized facialgeneration. However, existing methods face\nchallenges in achieving high-fidelity and detailed identity (ID)consistency,\nprimarily due to insufficient fine-grained control over facial areas and the\nlack of a comprehensive strategy for ID preservation by fully considering\nintricate facial details and the overall face. To address these limitations, we\nintroduce ConsistentID, an innovative method crafted for\ndiverseidentity-preserving portrait generation under fine-grained multimodal\nfacial prompts, utilizing only a single reference image. ConsistentID comprises\ntwo key components: a multimodal facial prompt generator that combines facial\nfeatures, corresponding facial descriptions and the overall facial context to\nenhance precision in facial details, and an ID-preservation network optimized\nthrough the facial attention localization strategy, aimed at preserving ID\nconsistency in facial regions. Together, these components significantly enhance\nthe accuracy of ID preservation by introducing fine-grained multimodal ID\ninformation from facial regions. To facilitate training of ConsistentID, we\npresent a fine-grained portrait dataset, FGID, with over 500,000 facial images,\noffering greater diversity and comprehensiveness than existing public facial\ndatasets. % such as LAION-Face, CelebA, FFHQ, and SFHQ. Experimental results\nsubstantiate that our ConsistentID achieves exceptional precision and diversity\nin personalized facial generation, surpassing existing methods in the MyStyle\ndataset. Furthermore, while ConsistentID introduces more multimodal ID\ninformation, it maintains a fast inference speed during generation.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16771",
        "PDF Link": "https://arxiv.org/pdf/2404.16771",
        "Upvotes": "4",
        "Date": "2024-04-26"
    },
    {
        "Title": "SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with\n  Text-Rich Visual Comprehension",
        "Abstract": "Comprehending text-rich visual content is paramount for the practical\napplication of Multimodal Large Language Models (MLLMs), since text-rich\nscenarios are ubiquitous in the real world, which are characterized by the\npresence of extensive texts embedded within images. Recently, the advent of\nMLLMs with impressive versatility has raised the bar for what we can expect\nfrom MLLMs. However, their proficiency in text-rich scenarios has yet to be\ncomprehensively and objectively assessed, since current MLLM benchmarks\nprimarily focus on evaluating general visual comprehension. In this work, we\nintroduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating\ntext-rich visual comprehension of MLLMs. Our benchmark comprises 2.3K\nmultiple-choice questions with precise human annotations, spanning three broad\ncategories: Charts, Maps, and Webs, each of which covers a wide spectrum of\ntext-rich scenarios in the real world. These categories, due to their inherent\ncomplexity and diversity, effectively simulate real-world text-rich\nenvironments. We further conduct a thorough evaluation involving 34 prominent\nMLLMs (including GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize the\ncurrent limitations of MLLMs in text-rich visual comprehension. We hope that\nour work can serve as a valuable addition to existing MLLM benchmarks,\nproviding insightful observations and inspiring further research in the area of\ntext-rich visual comprehension with MLLMs. The dataset and evaluation code can\nbe accessed at https://github.com/AILab-CVC/SEED-Bench.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16790",
        "PDF Link": "https://arxiv.org/pdf/2404.16790",
        "Upvotes": "4",
        "Date": "2024-04-26"
    },
    {
        "Title": "Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and\n  Human Ratings",
        "Abstract": "While text-to-image (T2I) generative models have become ubiquitous, they do\nnot necessarily generate images that align with a given prompt. While previous\nwork has evaluated T2I alignment by proposing metrics, benchmarks, and\ntemplates for collecting human judgements, the quality of these components is\nnot systematically measured. Human-rated prompt sets are generally small and\nthe reliability of the ratings -- and thereby the prompt set used to compare\nmodels -- is not evaluated. We address this gap by performing an extensive\nstudy evaluating auto-eval metrics and human templates. We provide three main\ncontributions: (1) We introduce a comprehensive skills-based benchmark that can\ndiscriminate models across different human templates. This skills-based\nbenchmark categorises prompts into sub-skills, allowing a practitioner to\npinpoint not only which skills are challenging, but at what level of complexity\na skill becomes challenging. (2) We gather human ratings across four templates\nand four T2I models for a total of >100K annotations. This allows us to\nunderstand where differences arise due to inherent ambiguity in the prompt and\nwhere they arise due to differences in metric and model quality. (3) Finally,\nwe introduce a new QA-based auto-eval metric that is better correlated with\nhuman ratings than existing metrics for our new dataset, across different human\ntemplates, and on TIFA160.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16820",
        "PDF Link": "https://arxiv.org/pdf/2404.16820",
        "Upvotes": "3",
        "Date": "2024-04-26"
    },
    {
        "Title": "PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video\n  Dense Captioning",
        "Abstract": "Vision-language pre-training has significantly elevated performance across a\nwide range of image-language applications. Yet, the pre-training process for\nvideo-related tasks demands exceptionally large computational and data\nresources, which hinders the progress of video-language models. This paper\ninvestigates a straightforward, highly efficient, and resource-light approach\nto adapting an existing image-language pre-trained model for dense video\nunderstanding. Our preliminary experiments reveal that directly fine-tuning\npre-trained image-language models with multiple frames as inputs on video\ndatasets leads to performance saturation or even a drop. Our further\ninvestigation reveals that it is largely attributed to the bias of learned\nhigh-norm visual features. Motivated by this finding, we propose a simple but\neffective pooling strategy to smooth the feature distribution along the\ntemporal dimension and thus reduce the dominant impacts from the extreme\nfeatures. The new model is termed Pooling LLaVA, or  in short.\n achieves new state-of-the-art performance on modern benchmark\ndatasets for both video question-answer and captioning tasks. Notably, on the\nrecent popular Video ChatGPT benchmark, PLLaVA achieves a score of 3.48 out of\n5 on average of five evaluated dimensions, exceeding the previous SOTA results\nfrom GPT4V (IG-VLM) by 9\\%. On the latest multi-choice benchmark MVBench,\nPLLaVA achieves 58.1\\% accuracy on average across 20 sub-tasks, 14.5\\% higher\nthan GPT4V (IG-VLM). Code is available at\nhttps://github.com/magic-research/PLLaVA.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16994",
        "PDF Link": "https://arxiv.org/pdf/2404.16994",
        "Upvotes": "17",
        "Date": "2024-04-29"
    },
    {
        "Title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
        "Abstract": "While recently Large Language Models (LLMs) have achieved remarkable\nsuccesses, they are vulnerable to certain jailbreaking attacks that lead to\ngeneration of inappropriate or harmful content. Manual red-teaming requires\nfinding adversarial prompts that cause such jailbreaking, e.g. by appending a\nsuffix to a given instruction, which is inefficient and time-consuming. On the\nother hand, automatic adversarial prompt generation often leads to semantically\nmeaningless attacks that can easily be detected by perplexity-based filters,\nmay require gradient information from the TargetLLM, or do not scale well due\nto time-consuming discrete optimization processes over the token space. In this\npaper, we present a novel method that uses another LLM, called the AdvPrompter,\nto generate human-readable adversarial prompts in seconds, sim800times\nfaster than existing optimization-based approaches. We train the AdvPrompter\nusing a novel algorithm that does not require access to the gradients of the\nTargetLLM. This process alternates between two steps: (1) generating\nhigh-quality target adversarial suffixes by optimizing the AdvPrompter\npredictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated\nadversarial suffixes. The trained AdvPrompter generates suffixes that veil the\ninput instruction without changing its meaning, such that the TargetLLM is\nlured to give a harmful response. Experimental results on popular open source\nTargetLLMs show state-of-the-art results on the AdvBench dataset, that also\ntransfer to closed-source black-box LLM APIs. Further, we demonstrate that by\nfine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made\nmore robust against jailbreaking attacks while maintaining performance, i.e.\nhigh MMLU scores.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16873",
        "PDF Link": "https://arxiv.org/pdf/2404.16873",
        "Upvotes": "12",
        "Date": "2024-04-29"
    },
    {
        "Title": "MaPa: Text-driven Photorealistic Material Painting for 3D Shapes",
        "Abstract": "This paper aims to generate materials for 3D meshes from text descriptions.\nUnlike existing methods that synthesize texture maps, we propose to generate\nsegment-wise procedural material graphs as the appearance representation, which\nsupports high-quality rendering and provides substantial flexibility in\nediting. Instead of relying on extensive paired data, i.e., 3D meshes with\nmaterial graphs and corresponding text descriptions, to train a material graph\ngenerative model, we propose to leverage the pre-trained 2D diffusion model as\na bridge to connect the text and material graphs. Specifically, our approach\ndecomposes a shape into a set of segments and designs a segment-controlled\ndiffusion model to synthesize 2D images that are aligned with mesh parts. Based\non generated images, we initialize parameters of material graphs and fine-tune\nthem through the differentiable rendering module to produce materials in\naccordance with the textual description. Extensive experiments demonstrate the\nsuperior performance of our framework in photorealism, resolution, and\neditability over existing methods. Project page:\nhttps://zhanghe3z.github.io/MaPa/",
        "ArXiv Link": "https://arxiv.org/abs/2404.17569",
        "PDF Link": "https://arxiv.org/pdf/2404.17569",
        "Upvotes": "6",
        "Date": "2024-04-29"
    },
    {
        "Title": "HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring\n  Unconstrained Photo Collections",
        "Abstract": "Internet image collections containing photos captured by crowds of\nphotographers show promise for enabling digital exploration of large-scale\ntourist landmarks. However, prior works focus primarily on geometric\nreconstruction and visualization, neglecting the key role of language in\nproviding a semantic interface for navigation and fine-grained understanding.\nIn constrained 3D domains, recent methods have leveraged vision-and-language\nmodels as a strong prior of 2D visual semantics. While these models display an\nexcellent understanding of broad visual semantics, they struggle with\nunconstrained photo collections depicting such tourist landmarks, as they lack\nexpert knowledge of the architectural domain. In this work, we present a\nlocalization system that connects neural representations of scenes depicting\nlarge-scale landmarks with text describing a semantic region within the scene,\nby harnessing the power of SOTA vision-and-language models with adaptations for\nunderstanding landmark scene semantics. To bolster such models with\nfine-grained knowledge, we leverage large-scale Internet data containing images\nof similar landmarks along with weakly-related textual information. Our\napproach is built upon the premise that images physically grounded in space can\nprovide a powerful supervision signal for localizing new concepts, whose\nsemantics may be unlocked from Internet textual metadata with large language\nmodels. We use correspondences between views of scenes to bootstrap spatial\nunderstanding of these semantics, providing guidance for 3D-compatible\nsegmentation that ultimately lifts to a volumetric scene representation. Our\nresults show that HaLo-NeRF can accurately localize a variety of semantic\nconcepts related to architectural landmarks, surpassing the results of other 3D\nmodels as well as strong 2D segmentation baselines. Our project page is at\nhttps://tau-vailab.github.io/HaLo-NeRF/.",
        "ArXiv Link": "https://arxiv.org/abs/2404.16845",
        "PDF Link": "https://arxiv.org/pdf/2404.16845",
        "Upvotes": "4",
        "Date": "2024-04-29"
    },
    {
        "Title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of\n  Diverse Models",
        "Abstract": "As Large Language Models (LLMs) have become more advanced, they have outpaced\nour abilities to accurately evaluate their quality. Not only is finding data to\nadequately probe particular model properties difficult, but evaluating the\ncorrectness of a model's freeform generation alone is a challenge. To address\nthis, many evaluations now rely on using LLMs themselves as judges to score the\nquality of outputs from other LLMs. Evaluations most commonly use a single\nlarge model like GPT4. While this method has grown in popularity, it is costly,\nhas been shown to introduce intramodel bias, and in this work, we find that\nvery large models are often unnecessary. We propose instead to evaluate models\nusing a Panel of LLm evaluators (PoLL). Across three distinct judge settings\nand spanning six different datasets, we find that using a PoLL composed of a\nlarger number of smaller models outperforms a single large judge, exhibits less\nintra-model bias due to its composition of disjoint model families, and does so\nwhile being over seven times less expensive.",
        "ArXiv Link": "https://arxiv.org/abs/2404.18796",
        "PDF Link": "https://arxiv.org/pdf/2404.18796",
        "Upvotes": "7",
        "Date": "2024-04-30"
    },
    {
        "Title": "LEGENT: Open Platform for Embodied Agents",
        "Abstract": "Despite advancements in Large Language Models (LLMs) and Large Multimodal\nModels (LMMs), their integration into language-grounded, human-like embodied\nagents remains incomplete, hindering complex real-life task performance in\nphysical environments. Existing integrations often feature limited open\nsourcing, challenging collective progress in this field. We introduce LEGENT,\nan open, scalable platform for developing embodied agents using LLMs and LMMs.\nLEGENT offers a dual approach: a rich, interactive 3D environment with\ncommunicable and actionable agents, paired with a user-friendly interface, and\na sophisticated data generation pipeline utilizing advanced algorithms to\nexploit supervision from simulated worlds at scale. In our experiments, an\nembryonic vision-language-action model trained on LEGENT-generated data\nsurpasses GPT-4V in embodied tasks, showcasing promising generalization\ncapabilities.",
        "ArXiv Link": "https://arxiv.org/abs/2404.18243",
        "PDF Link": "https://arxiv.org/pdf/2404.18243",
        "Upvotes": "4",
        "Date": "2024-04-30"
    },
    {
        "Title": "Ag2Manip: Learning Novel Manipulation Skills with Agent-Agnostic Visual\n  and Action Representations",
        "Abstract": "Autonomous robotic systems capable of learning novel manipulation tasks are\npoised to transform industries from manufacturing to service automation.\nHowever, modern methods (e.g., VIP and R3M) still face significant hurdles,\nnotably the domain gap among robotic embodiments and the sparsity of successful\ntask executions within specific action spaces, resulting in misaligned and\nambiguous task representations. We introduce Ag2Manip (Agent-Agnostic\nrepresentations for Manipulation), a framework aimed at surmounting these\nchallenges through two key innovations: a novel agent-agnostic visual\nrepresentation derived from human manipulation videos, with the specifics of\nembodiments obscured to enhance generalizability; and an agent-agnostic action\nrepresentation abstracting a robot's kinematics to a universal agent proxy,\nemphasizing crucial interactions between end-effector and object. Ag2Manip's\nempirical validation across simulated benchmarks like FrankaKitchen, ManiSkill,\nand PartManip shows a 325% increase in performance, achieved without\ndomain-specific demonstrations. Ablation studies underline the essential\ncontributions of the visual and action representations to this success.\nExtending our evaluations to the real world, Ag2Manip significantly improves\nimitation learning success rates from 50% to 77.5%, demonstrating its\neffectiveness and generalizability across both simulated and physical\nenvironments.",
        "ArXiv Link": "https://arxiv.org/abs/2404.17521",
        "PDF Link": "https://arxiv.org/pdf/2404.17521",
        "Upvotes": "3",
        "Date": "2024-04-30"
    },
    {
        "Title": "Capabilities of Gemini Models in Medicine",
        "Abstract": "Excellence in a wide variety of medical applications poses considerable\nchallenges for AI, requiring advanced reasoning, access to up-to-date medical\nknowledge and understanding of complex multimodal data. Gemini models, with\nstrong general capabilities in multimodal and long-context reasoning, offer\nexciting possibilities in medicine. Building on these core strengths of Gemini,\nwe introduce Med-Gemini, a family of highly capable multimodal models that are\nspecialized in medicine with the ability to seamlessly use web search, and that\ncan be efficiently tailored to novel modalities using custom encoders. We\nevaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art\n(SoTA) performance on 10 of them, and surpass the GPT-4 model family on every\nbenchmark where a direct comparison is viable, often by a wide margin. On the\npopular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves\nSoTA performance of 91.1% accuracy, using a novel uncertainty-guided search\nstrategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU\n(health & medicine), Med-Gemini improves over GPT-4V by an average relative\nmargin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context\ncapabilities through SoTA performance on a needle-in-a-haystack retrieval task\nfrom long de-identified health records and medical video question answering,\nsurpassing prior bespoke methods using only in-context learning. Finally,\nMed-Gemini's performance suggests real-world utility by surpassing human\nexperts on tasks such as medical text summarization, alongside demonstrations\nof promising potential for multimodal medical dialogue, medical research and\neducation. Taken together, our results offer compelling evidence for\nMed-Gemini's potential, although further rigorous evaluation will be crucial\nbefore real-world deployment in this safety-critical domain.",
        "ArXiv Link": "https://arxiv.org/abs/2404.18416",
        "PDF Link": "https://arxiv.org/pdf/2404.18416",
        "Upvotes": "3",
        "Date": "2024-04-30"
    },
    {
        "Title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance",
        "Abstract": "Apparel's significant role in human appearance underscores the importance of\ngarment digitalization for digital human creation. Recent advances in 3D\ncontent creation are pivotal for digital human creation. Nonetheless, garment\ngeneration from text guidance is still nascent. We introduce a text-driven 3D\ngarment generation framework, DressCode, which aims to democratize design for\nnovices and offer immense potential in fashion design, virtual try-on, and\ndigital human creation. For our framework, we first introduce SewingGPT, a\nGPT-based architecture integrating cross-attention with text-conditioned\nembedding to generate sewing patterns with text guidance. We also tailored a\npre-trained Stable Diffusion for high-quality, tile-based PBR texture\ngeneration. By leveraging a large language model, our framework generates\nCG-friendly garments through natural language interaction. Our method also\nfacilitates pattern completion and texture editing, simplifying the process for\ndesigners by user-friendly interaction. With comprehensive evaluations and\ncomparisons with other state-of-the-art methods, our method showcases the best\nquality and alignment with input prompts. User studies further validate our\nhigh-quality rendering results, highlighting its practical utility and\npotential in production settings.",
        "ArXiv Link": "https://arxiv.org/abs/2401.16465",
        "PDF Link": "https://arxiv.org/pdf/2401.16465",
        "Upvotes": "2",
        "Date": "2024-04-30"
    },
    {
        "Title": "Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting",
        "Abstract": "Speculative decoding has demonstrated its effectiveness in accelerating the\ninference of large language models while maintaining a consistent sampling\ndistribution. However, the conventional approach of training a separate draft\nmodel to achieve a satisfactory token acceptance rate can be costly. Drawing\ninspiration from early exiting, we propose a novel self-speculative decoding\nframework Kangaroo, which uses a fixed shallow sub-network as a\nself-draft model, with the remaining layers serving as the larger target model.\nWe train a lightweight and efficient adapter module on top of the sub-network\nto bridge the gap between the sub-network and the full model's representation\nability. It is noteworthy that the inference latency of the self-draft model\nmay no longer be negligible compared to the large model, necessitating\nstrategies to increase the token acceptance rate while minimizing the drafting\nsteps of the small model. To address this challenge, we introduce an additional\nearly exiting mechanism for generating draft tokens. Specifically, we halt the\nsmall model's subsequent prediction during the drafting phase once the\nconfidence level for the current token falls below a certain threshold.\nExtensive experiments on the Spec-Bench demonstrate the effectiveness of\nKangaroo. Under single-sequence verification, Kangaroo achieves speedups up to\n1.68times on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional\nparameters (67M compared to 591M). The code for Kangaroo is available at\nhttps://github.com/Equationliu/Kangaroo.",
        "ArXiv Link": "https://arxiv.org/abs/2404.18911",
        "PDF Link": "https://arxiv.org/pdf/2404.18911",
        "Upvotes": "2",
        "Date": "2024-04-30"
    },
    {
        "Title": "BlenderAlchemy: Editing 3D Graphics with Vision-Language Models",
        "Abstract": "Graphics design is important for various applications, including movie\nproduction and game design. To create a high-quality scene, designers usually\nneed to spend hours in software like Blender, in which they might need to\ninterleave and repeat operations, such as connecting material nodes, hundreds\nof times. Moreover, slightly different design goals may require completely\ndifferent sequences, making automation difficult. In this paper, we propose a\nsystem that leverages Vision-Language Models (VLMs), like GPT-4V, to\nintelligently search the design action space to arrive at an answer that can\nsatisfy a user's intent. Specifically, we design a vision-based edit generator\nand state evaluator to work together to find the correct sequence of actions to\nachieve the goal. Inspired by the role of visual imagination in the human\ndesign process, we supplement the visual reasoning capabilities of VLMs with\n\"imagined\" reference images from image-generation models, providing visual\ngrounding of abstract language descriptions. In this paper, we provide\nempirical evidence suggesting our system can produce simple but tedious Blender\nediting sequences for tasks such as editing procedural materials from text\nand/or reference images, as well as adjusting lighting configurations for\nproduct renderings in complex scenes.",
        "ArXiv Link": "https://arxiv.org/abs/2404.17672",
        "PDF Link": "https://arxiv.org/pdf/2404.17672",
        "Upvotes": "2",
        "Date": "2024-04-30"
    },
    {
        "Title": "InstantFamily: Masked Attention for Zero-shot Multi-ID Image Generation",
        "Abstract": "In the field of personalized image generation, the ability to create images\npreserving concepts has significantly improved. Creating an image that\nnaturally integrates multiple concepts in a cohesive and visually appealing\ncomposition can indeed be challenging. This paper introduces \"InstantFamily,\"\nan approach that employs a novel masked cross-attention mechanism and a\nmultimodal embedding stack to achieve zero-shot multi-ID image generation. Our\nmethod effectively preserves ID as it utilizes global and local features from a\npre-trained face recognition model integrated with text conditions.\nAdditionally, our masked cross-attention mechanism enables the precise control\nof multi-ID and composition in the generated images. We demonstrate the\neffectiveness of InstantFamily through experiments showing its dominance in\ngenerating images with multi-ID, while resolving well-known multi-ID generation\nproblems. Additionally, our model achieves state-of-the-art performance in both\nsingle-ID and multi-ID preservation. Furthermore, our model exhibits remarkable\nscalability with a greater number of ID preservation than it was originally\ntrained with.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19427",
        "PDF Link": "https://arxiv.org/pdf/2404.19427",
        "Upvotes": "27",
        "Date": "2024-05-01"
    },
    {
        "Title": "Octopus v4: Graph of language models",
        "Abstract": "Language models have been effective in a wide range of applications, yet the\nmost sophisticated models are often proprietary. For example, GPT-4 by OpenAI\nand various models by Anthropic are expensive and consume substantial energy.\nIn contrast, the open-source community has produced competitive models, like\nLlama3. Furthermore, niche-specific smaller language models, such as those\ntailored for legal, medical or financial tasks, have outperformed their\nproprietary counterparts. This paper introduces a novel approach that employs\nfunctional tokens to integrate multiple open-source models,\neach optimized for particular tasks. Our newly developed Octopus v4 model\nleverages functional tokens to intelligently direct user queries to\nthe most appropriate vertical model and reformat the query to achieve the best\nperformance. Octopus v4, an evolution of the Octopus v1, v2, and v3 models,\nexcels in selection and parameter understanding and reformatting. Additionally,\nwe explore the use of graph as a versatile data structure that effectively\ncoordinates multiple open-source models by harnessing the capabilities of the\nOctopus model and functional tokens. Use our open-sourced GitHub\n(https://www.nexa4ai.com/) to try Octopus v4 models\n(https://huggingface.co/NexaAIDev/Octopus-v4), and contrite to a larger\ngraph of language models. By activating models less than 10B parameters, we\nachieved SOTA MMLU score of 74.8 among the same level models.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19296",
        "PDF Link": "https://arxiv.org/pdf/2404.19296",
        "Upvotes": "20",
        "Date": "2024-05-01"
    },
    {
        "Title": "Better & Faster Large Language Models via Multi-token Prediction",
        "Abstract": "Large language models such as GPT and Llama are trained with a next-token\nprediction loss. In this work, we suggest that training language models to\npredict multiple future tokens at once results in higher sample efficiency.\nMore specifically, at each position in the training corpus, we ask the model to\npredict the following n tokens using n independent output heads, operating on\ntop of a shared model trunk. Considering multi-token prediction as an auxiliary\ntraining task, we measure improved downstream capabilities with no overhead in\ntraining time for both code and natural language models. The method is\nincreasingly useful for larger model sizes, and keeps its appeal when training\nfor multiple epochs. Gains are especially pronounced on generative benchmarks\nlike coding, where our models consistently outperform strong baselines by\nseveral percentage points. Our 13B parameter models solves 12 % more problems\non HumanEval and 17 % more on MBPP than comparable next-token models.\nExperiments on small algorithmic tasks demonstrate that multi-token prediction\nis favorable for the development of induction heads and algorithmic reasoning\ncapabilities. As an additional benefit, models trained with 4-token prediction\nare up to 3 times faster at inference, even with large batch sizes.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19737",
        "PDF Link": "https://arxiv.org/pdf/2404.19737",
        "Upvotes": "20",
        "Date": "2024-05-01"
    },
    {
        "Title": "Iterative Reasoning Preference Optimization",
        "Abstract": "Iterative preference optimization methods have recently been shown to perform\nwell for general instruction tuning tasks, but typically make little\nimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this\nwork we develop an iterative approach that optimizes the preference between\ncompeting generated Chain-of-Thought (CoT) candidates by optimizing for winning\nvs. losing reasoning steps that lead to the correct answer. We train using a\nmodified DPO loss (Rafailov et al., 2023) with an additional negative\nlog-likelihood term, which we find to be crucial. We show reasoning improves\nacross repeated iterations of this scheme. While only relying on examples in\nthe training set, our approach results in increasing accuracy for\nLlama-2-70B-Chat from 55.6% to 81.6% on GSM8K (and 88.7% with majority voting\nout of 32 samples), from 12.5% to 20.8% on MATH, and from 77.8% to 86.7% on\nARC-Challenge, which outperforms other Llama-2-based models not relying on\nadditionally sourced datasets.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19733",
        "PDF Link": "https://arxiv.org/pdf/2404.19733",
        "Upvotes": "16",
        "Date": "2024-05-01"
    },
    {
        "Title": "MotionLCM: Real-time Controllable Motion Generation via Latent\n  Consistency Model",
        "Abstract": "This work introduces MotionLCM, extending controllable motion generation to a\nreal-time level. Existing methods for spatial control in text-conditioned\nmotion generation suffer from significant runtime inefficiency. To address this\nissue, we first propose the motion latent consistency model (MotionLCM) for\nmotion generation, building upon the latent diffusion model (MLD). By employing\none-step (or few-step) inference, we further improve the runtime efficiency of\nthe motion latent diffusion model for motion generation. To ensure effective\ncontrollability, we incorporate a motion ControlNet within the latent space of\nMotionLCM and enable explicit control signals (e.g., pelvis trajectory) in the\nvanilla motion space to control the generation process directly, similar to\ncontrolling other latent-free diffusion models for motion generation. By\nemploying these techniques, our approach can generate human motions with text\nand control signals in real-time. Experimental results demonstrate the\nremarkable generation and controlling capabilities of MotionLCM while\nmaintaining real-time runtime efficiency.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19759",
        "PDF Link": "https://arxiv.org/pdf/2404.19759",
        "Upvotes": "11",
        "Date": "2024-05-01"
    },
    {
        "Title": "Extending Llama-3's Context Ten-Fold Overnight",
        "Abstract": "We extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\nfine-tuning. The entire training cycle is super efficient, which takes 8 hours\non one 8xA800 (80G) GPU machine. The resulted model exhibits superior\nperformances across a broad range of evaluation tasks, such as NIHS, topic\nretrieval, and long-context language understanding; meanwhile, it also well\npreserves the original capability over short contexts. The dramatic context\nextension is mainly attributed to merely 3.5K synthetic training samples\ngenerated by GPT-4 , which indicates the LLMs' inherent (yet largely\nunderestimated) potential to extend its original context length. In fact, the\ncontext length could be extended far beyond 80K with more computation\nresources. Therefore, the team will publicly release the entire resources\n(including data, model, data generation pipeline, training code) so as to\nfacilitate the future research from the community:\nhttps://github.com/FlagOpen/FlagEmbedding.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19553",
        "PDF Link": "https://arxiv.org/pdf/2404.19553",
        "Upvotes": "10",
        "Date": "2024-05-01"
    },
    {
        "Title": "KAN: Kolmogorov-Arnold Networks",
        "Abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19756",
        "PDF Link": "https://arxiv.org/pdf/2404.19756",
        "Upvotes": "9",
        "Date": "2024-05-01"
    },
    {
        "Title": "Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation",
        "Abstract": "Existing automatic captioning methods for visual content face challenges such\nas lack of detail, content hallucination, and poor instruction following. In\nthis work, we propose VisualFactChecker (VFC), a flexible training-free\npipeline that generates high-fidelity and detailed captions for both 2D images\nand 3D objects. VFC consists of three steps: 1) proposal, where image-to-text\ncaptioning models propose multiple initial captions; 2) verification, where a\nlarge language model (LLM) utilizes tools such as object detection and VQA\nmodels to fact-check proposed captions; 3) captioning, where an LLM generates\nthe final caption by summarizing caption proposals and the fact check\nverification results. In this step, VFC can flexibly generate captions in\nvarious styles following complex instructions. We conduct comprehensive\ncaptioning evaluations using four metrics: 1) CLIP-Score for image-text\nsimilarity; 2) CLIP-Image-Score for measuring the image-image similarity\nbetween the original and the reconstructed image generated by a text-to-image\nmodel using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4V\nfor fine-grained evaluation. Evaluation results show that VFC outperforms\nstate-of-the-art open-sourced captioning methods for 2D images on the COCO\ndataset and 3D assets on the Objaverse dataset. Our study demonstrates that by\ncombining open-source models into a pipeline, we can attain captioning\ncapability comparable to proprietary models such as GPT-4V, despite being over\n10x smaller in model size.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19752",
        "PDF Link": "https://arxiv.org/pdf/2404.19752",
        "Upvotes": "7",
        "Date": "2024-05-01"
    },
    {
        "Title": "GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting",
        "Abstract": "We propose GS-LRM, a scalable large reconstruction model that can predict\nhigh-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23\nseconds on single A100 GPU. Our model features a very simple transformer-based\narchitecture; we patchify input posed images, pass the concatenated multi-view\nimage tokens through a sequence of transformer blocks, and decode final\nper-pixel Gaussian parameters directly from these tokens for differentiable\nrendering. In contrast to previous LRMs that can only reconstruct objects, by\npredicting per-pixel Gaussians, GS-LRM naturally handles scenes with large\nvariations in scale and complexity. We show that our model can work on both\nobject and scene captures by training it on Objaverse and RealEstate10K\nrespectively. In both scenarios, the models outperform state-of-the-art\nbaselines by a wide margin. We also demonstrate applications of our model in\ndownstream 3D generation tasks. Our project webpage is available at:\nhttps://sai-bi.github.io/project/gs-lrm/ .",
        "ArXiv Link": "https://arxiv.org/abs/2404.19702",
        "PDF Link": "https://arxiv.org/pdf/2404.19702",
        "Upvotes": "6",
        "Date": "2024-05-01"
    },
    {
        "Title": "Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting",
        "Abstract": "3D scene generation has quickly become a challenging new research direction,\nfueled by consistent improvements of 2D generative diffusion models. Most prior\nwork in this area generates scenes by iteratively stitching newly generated\nframes with existing geometry. These works often depend on pre-trained\nmonocular depth estimators to lift the generated images into 3D, fusing them\nwith the existing scene representation. These approaches are then often\nevaluated via a text metric, measuring the similarity between the generated\nimages and a given text prompt. In this work, we make two fundamental\ncontributions to the field of 3D scene generation. First, we note that lifting\nimages to 3D with a monocular depth estimation model is suboptimal as it\nignores the geometry of the existing scene. We thus introduce a novel depth\ncompletion model, trained via teacher distillation and self-training to learn\nthe 3D fusion process, resulting in improved geometric coherence of the scene.\nSecond, we introduce a new benchmarking scheme for scene generation methods\nthat is based on ground truth geometry, and thus measures the quality of the\nstructure of the scene.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19758",
        "PDF Link": "https://arxiv.org/pdf/2404.19758",
        "Upvotes": "4",
        "Date": "2024-05-01"
    },
    {
        "Title": "SAGS: Structure-Aware 3D Gaussian Splatting",
        "Abstract": "Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the\nway to real-time neural rendering overcoming the computational burden of\nvolumetric methods. Following the pioneering work of 3D-GS, several methods\nhave attempted to achieve compressible and high-fidelity performance\nalternatives. However, by employing a geometry-agnostic optimization scheme,\nthese methods neglect the inherent 3D structure of the scene, thereby\nrestricting the expressivity and the quality of the representation, resulting\nin various floating points and artifacts. In this work, we propose a\nstructure-aware Gaussian Splatting method (SAGS) that implicitly encodes the\ngeometry of the scene, which reflects to state-of-the-art rendering performance\nand reduced storage requirements on benchmark novel-view synthesis datasets.\nSAGS is founded on a local-global graph representation that facilitates the\nlearning of complex scenes and enforces meaningful point displacements that\npreserve the scene's geometry. Additionally, we introduce a lightweight version\nof SAGS, using a simple yet effective mid-point interpolation scheme, which\nshowcases a compact representation of the scene with up to 24times size\nreduction without the reliance on any compression strategies. Extensive\nexperiments across multiple benchmark datasets demonstrate the superiority of\nSAGS compared to state-of-the-art 3D-GS methods under both rendering quality\nand model size. Besides, we demonstrate that our structure-aware method can\neffectively mitigate floating artifacts and irregular distortions of previous\nmethods while obtaining precise depth maps. Project page\nhttps://eververas.github.io/SAGS/.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19149",
        "PDF Link": "https://arxiv.org/pdf/2404.19149",
        "Upvotes": "4",
        "Date": "2024-05-01"
    },
    {
        "Title": "MicroDreamer: Zero-shot 3D Generation in sim20 Seconds by Score-based\n  Iterative Reconstruction",
        "Abstract": "Optimization-based approaches, such as score distillation sampling (SDS),\nshow promise in zero-shot 3D generation but suffer from low efficiency,\nprimarily due to the high number of function evaluations (NFEs) required for\neach sample. In this paper, we introduce score-based iterative reconstruction\n(SIR), an efficient and general algorithm for 3D generation with a multi-view\nscore-based diffusion model. Given the images produced by the diffusion model,\nSIR reduces NFEs by repeatedly optimizing 3D parameters, unlike the single\noptimization in SDS, mimicking the 3D reconstruction process. With other\nimprovements including optimization in the pixel space, we present an efficient\napproach called MicroDreamer that generally applies to various 3D\nrepresentations and 3D generation tasks. In particular, retaining a comparable\nperformance, MicroDreamer is 5-20 times faster than SDS in generating neural\nradiance field and takes about 20 seconds to generate meshes from 3D Gaussian\nsplitting on a single A100 GPU, halving the time of the fastest zero-shot\nbaseline, DreamGaussian. Our code is available at\nhttps://github.com/ML-GSAI/MicroDreamer.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19525",
        "PDF Link": "https://arxiv.org/pdf/2404.19525",
        "Upvotes": "3",
        "Date": "2024-05-01"
    },
    {
        "Title": "DOCCI: Descriptions of Connected and Contrasting Images",
        "Abstract": "Vision-language datasets are vital for both text-to-image (T2I) and\nimage-to-text (I2T) research. However, current datasets lack descriptions with\nfine-grained detail that would allow for richer associations to be learned by\nmodels. To fill the gap, we introduce Descriptions of Connected and Contrasting\nImages (DOCCI), a dataset with long, human-annotated English descriptions for\n15k images that were taken, curated and donated by a single researcher intent\non capturing key challenges such as spatial relations, counting, text\nrendering, world knowledge, and more. We instruct human annotators to create\ncomprehensive descriptions for each image; these average 136 words in length\nand are crafted to clearly distinguish each image from those that are related\nor similar. Each description is highly compositional and typically encompasses\nmultiple challenges. Through both quantitative and qualitative analyses, we\ndemonstrate that DOCCI serves as an effective training resource for\nimage-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or\nsuperior results compared to highly-performant larger models like LLaVA-1.5 7B\nand InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for\ntext-to-image generation, highlighting the limitations of current text-to-image\nmodels in capturing long descriptions and fine details.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19753",
        "PDF Link": "https://arxiv.org/pdf/2404.19753",
        "Upvotes": "3",
        "Date": "2024-05-01"
    },
    {
        "Title": "Lightplane: Highly-Scalable Components for Neural 3D Fields",
        "Abstract": "Contemporary 3D research, particularly in reconstruction and generation,\nheavily relies on 2D images for inputs or supervision. However, current designs\nfor these 2D-3D mapping are memory-intensive, posing a significant bottleneck\nfor existing methods and hindering new applications. In response, we propose a\npair of highly scalable components for 3D neural fields: Lightplane Render and\nSplatter, which significantly reduce memory usage in 2D-3D mapping. These\ninnovations enable the processing of vastly more and higher resolution images\nwith small memory and computational costs. We demonstrate their utility in\nvarious applications, from benefiting single-scene optimization with\nimage-level losses to realizing a versatile pipeline for dramatically scaling\n3D reconstruction and generation. Code:\nhttps://github.com/facebookresearch/lightplane.",
        "ArXiv Link": "https://arxiv.org/abs/2404.19760",
        "PDF Link": "https://arxiv.org/pdf/2404.19760",
        "Upvotes": "2",
        "Date": "2024-05-01"
    },
    {
        "Title": "Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model\n  Editing with Llama-3",
        "Abstract": "This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00664",
        "PDF Link": "https://arxiv.org/pdf/2405.00664",
        "Upvotes": "10",
        "Date": "2024-05-02"
    },
    {
        "Title": "A Careful Examination of Large Language Model Performance on Grade\n  School Arithmetic",
        "Abstract": "Large language models (LLMs) have achieved impressive success on many\nbenchmarks for mathematical reasoning. However, there is growing concern that\nsome of this performance actually reflects dataset contamination, where data\nclosely resembling benchmark questions leaks into the training data, instead of\ntrue reasoning ability. To investigate this claim rigorously, we commission\nGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and\ncomplexity of the established GSM8k benchmark, the gold standard for measuring\nelementary mathematical reasoning. We ensure that the two benchmarks are\ncomparable across important metrics such as human solve rates, number of steps\nin solution, answer magnitude, and more. When evaluating leading open- and\nclosed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with\nseveral families of models (e.g., Phi and Mistral) showing evidence of\nsystematic overfitting across almost all model sizes. At the same time, many\nmodels, especially those on the frontier, (e.g., Gemini/GPT/Claude) show\nminimal signs of overfitting. Further analysis suggests a positive relationship\n(Spearman's r^2=0.32) between a model's probability of generating an example\nfrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting that\nmany models may have partially memorized GSM8k.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00332",
        "PDF Link": "https://arxiv.org/pdf/2405.00332",
        "Upvotes": "10",
        "Date": "2024-05-02"
    },
    {
        "Title": "Paint by Inpaint: Learning to Add Image Objects by Removing Them First",
        "Abstract": "Image editing has advanced significantly with the introduction of\ntext-conditioned diffusion models. Despite this progress, seamlessly adding\nobjects to images based on textual instructions without requiring user-provided\ninput masks remains a challenge. We address this by leveraging the insight that\nremoving objects (Inpaint) is significantly simpler than its inverse process of\nadding them (Paint), attributed to the utilization of segmentation mask\ndatasets alongside inpainting models that inpaint within these masks.\nCapitalizing on this realization, by implementing an automated and extensive\npipeline, we curate a filtered large-scale image dataset containing pairs of\nimages and their corresponding object-removed versions. Using these pairs, we\ntrain a diffusion model to inverse the inpainting process, effectively adding\nobjects into images. Unlike other editing datasets, ours features natural\ntarget images instead of synthetic ones; moreover, it maintains consistency\nbetween source and target by construction. Additionally, we utilize a large\nVision-Language Model to provide detailed descriptions of the removed objects\nand a Large Language Model to convert these descriptions into diverse,\nnatural-language instructions. We show that the trained model surpasses\nexisting ones both qualitatively and quantitatively, and release the\nlarge-scale dataset alongside the trained models for the community.",
        "ArXiv Link": "https://arxiv.org/abs/2404.18212",
        "PDF Link": "https://arxiv.org/pdf/2404.18212",
        "Upvotes": "8",
        "Date": "2024-05-02"
    },
    {
        "Title": "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General\n  Sound",
        "Abstract": "Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modelling techniques to audio data. However,\ntraditional codecs often operate at high bitrates or within narrow domains such\nas speech and lack the semantic clues required for efficient language\nmodelling. Addressing these challenges, we introduce SemantiCodec, a novel\ncodec designed to compress audio into fewer than a hundred tokens per second\nacross diverse audio types, including speech, general audio, and music, without\ncompromising quality. SemantiCodec features a dual-encoder architecture: a\nsemantic encoder using a self-supervised AudioMAE, discretized using k-means\nclustering on extensive audio data, and an acoustic encoder to capture the\nremaining details. The semantic and acoustic encoder outputs are used to\nreconstruct audio via a diffusion-model-based decoder. SemantiCodec is\npresented in three variants with token rates of 25, 50, and 100 per second,\nsupporting a range of ultra-low bit rates between 0.31 kbps and 1.43 kbps.\nExperimental results demonstrate that SemantiCodec significantly outperforms\nthe state-of-the-art Descript codec on reconstruction quality. Our results also\nsuggest that SemantiCodec contains significantly richer semantic information\nthan all evaluated audio codecs, even at significantly lower bitrates. Our code\nand demos are available at https://haoheliu.github.io/SemantiCodec/.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00233",
        "PDF Link": "https://arxiv.org/pdf/2405.00233",
        "Upvotes": "7",
        "Date": "2024-05-02"
    },
    {
        "Title": "Clover: Regressive Lightweight Speculative Decoding with Sequential\n  Knowledge",
        "Abstract": "Large language models (LLMs) suffer from low efficiency as the mismatch\nbetween the requirement of auto-regressive decoding and the design of most\ncontemporary GPUs. Specifically, billions to trillions of parameters must be\nloaded to the GPU cache through its limited memory bandwidth for computation,\nbut only a small batch of tokens is actually computed. Consequently, the GPU\nspends most of its time on memory transfer instead of computation. Recently,\nparallel decoding, a type of speculative decoding algorithms, is becoming more\npopular and has demonstrated impressive efficiency improvement in generation.\nIt introduces extra decoding heads to large models, enabling them to predict\nmultiple subsequent tokens simultaneously and verify these candidate\ncontinuations in a single decoding step. However, this approach deviates from\nthe training objective of next token prediction used during pre-training,\nresulting in a low hit rate for candidate tokens. In this paper, we propose a\nnew speculative decoding algorithm, Clover, which integrates sequential\nknowledge into the parallel decoding process. This enhancement improves the hit\nrate of speculators and thus boosts the overall efficiency. Clover transmits\nthe sequential knowledge from pre-speculated tokens via the Regressive\nConnection, then employs an Attention Decoder to integrate these speculated\ntokens. Additionally, Clover incorporates an Augmenting Block that modifies the\nhidden states to better align with the purpose of speculative generation rather\nthan next token prediction. The experiment results demonstrate that Clover\noutperforms the baseline by up to 91% on Baichuan-Small and 146% on\nBaichuan-Large, respectively, and exceeds the performance of the previously\ntop-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on\nBaichuan-Large, respectively.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00263",
        "PDF Link": "https://arxiv.org/pdf/2405.00263",
        "Upvotes": "6",
        "Date": "2024-05-02"
    },
    {
        "Title": "Spectrally Pruned Gaussian Fields with Neural Compensation",
        "Abstract": "Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered\nattention for its fast rendering speed and high rendering quality. However,\nthis comes with high memory consumption, e.g., a well-trained Gaussian field\nmay utilize three million Gaussian primitives and over 700 MB of memory. We\ncredit this high memory footprint to the lack of consideration for the\nrelationship between primitives. In this paper, we propose a memory-efficient\nGaussian field named SUNDAE with spectral pruning and neural compensation. On\none hand, we construct a graph on the set of Gaussian primitives to model their\nrelationship and design a spectral down-sampling module to prune out primitives\nwhile preserving desired signals. On the other hand, to compensate for the\nquality loss of pruning Gaussians, we exploit a lightweight neural network head\nto mix splatted features, which effectively compensates for quality losses\nwhile capturing the relationship between primitives in its weights. We\ndemonstrate the performance of SUNDAE with extensive results. For example,\nSUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla\nGaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB\nmemory, on the Mip-NeRF360 dataset. Codes are publicly available at\nhttps://runyiyang.github.io/projects/SUNDAE/.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00676",
        "PDF Link": "https://arxiv.org/pdf/2405.00676",
        "Upvotes": "6",
        "Date": "2024-05-02"
    },
    {
        "Title": "Self-Play Preference Optimization for Language Model Alignment",
        "Abstract": "Traditional reinforcement learning from human feedback (RLHF) approaches\nrelying on parametric models like the Bradley-Terry model fall short in\ncapturing the intransitivity and irrationality in human preferences. Recent\nadvancements suggest that directly working with preference probabilities can\nyield a more accurate reflection of human preferences, enabling more flexible\nand accurate language model alignment. In this paper, we propose a\nself-play-based method for language model alignment, which treats the problem\nas a constant-sum two-player game aimed at identifying the Nash equilibrium\npolicy. Our approach, dubbed Self-Play Preference Optimization (SPPO),\napproximates the Nash equilibrium through iterative policy updates and enjoys\ntheoretical convergence guarantee. Our method can effectively increase the\nlog-likelihood of the chosen response and decrease that of the rejected\nresponse, which cannot be trivially achieved by symmetric pairwise loss such as\nDirect Preference Optimization (DPO) and Identity Preference Optimization\n(IPO). In our experiments, using only 60k prompts (without responses) from the\nUltraFeedback dataset and without any prompt augmentation, by leveraging a\npre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain\na model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the\nstate-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on\nAlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and\nthe Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved\nwithout additional external supervision (e.g., responses, preferences, etc.)\nfrom GPT-4 or other stronger language models.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00675",
        "PDF Link": "https://arxiv.org/pdf/2405.00675",
        "Upvotes": "4",
        "Date": "2024-05-02"
    },
    {
        "Title": "Automatic Creative Selection with Cross-Modal Matching",
        "Abstract": "Application developers advertise their Apps by creating product pages with\nApp images, and bidding on search terms. It is then crucial for App images to\nbe highly relevant with the search terms. Solutions to this problem require an\nimage-text matching model to predict the quality of the match between the\nchosen image and the search terms. In this work, we present a novel approach to\nmatching an App image to search terms based on fine-tuning a pre-trained LXMERT\nmodel. We show that compared to the CLIP model and a baseline using a\nTransformer model for search terms, and a ResNet model for images, we\nsignificantly improve the matching accuracy. We evaluate our approach using two\nsets of labels: advertiser associated (image, search term) pairs for a given\napplication, and human ratings for the relevance between (image, search term)\npairs. Our approach achieves 0.96 AUC score for advertiser associated ground\ntruth, outperforming the transformer+ResNet baseline and the fine-tuned CLIP\nmodel by 8% and 14%. For human labeled ground truth, our approach achieves 0.95\nAUC score, outperforming the transformer+ResNet baseline and the fine-tuned\nCLIP model by 16% and 17%.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00029",
        "PDF Link": "https://arxiv.org/pdf/2405.00029",
        "Upvotes": "3",
        "Date": "2024-05-02"
    },
    {
        "Title": "STT: Stateful Tracking with Transformers for Autonomous Driving",
        "Abstract": "Tracking objects in three-dimensional space is critical for autonomous\ndriving. To ensure safety while driving, the tracker must be able to reliably\ntrack objects across frames and accurately estimate their states such as\nvelocity and acceleration in the present. Existing works frequently focus on\nthe association task while either neglecting the model performance on state\nestimation or deploying complex heuristics to predict the states. In this\npaper, we propose STT, a Stateful Tracking model built with Transformers, that\ncan consistently track objects in the scenes while also predicting their states\naccurately. STT consumes rich appearance, geometry, and motion signals through\nlong term history of detections and is jointly optimized for both data\nassociation and state estimation tasks. Since the standard tracking metrics\nlike MOTA and MOTP do not capture the combined performance of the two tasks in\nthe wider spectrum of object states, we extend them with new metrics called\nS-MOTA and MOTPS that address this limitation. STT achieves competitive\nreal-time performance on the Waymo Open Dataset.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00236",
        "PDF Link": "https://arxiv.org/pdf/2405.00236",
        "Upvotes": "2",
        "Date": "2024-05-02"
    },
    {
        "Title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating\n  Other Language Models",
        "Abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of\nresponses from various LMs. However, concerns including transparency,\ncontrollability, and affordability strongly motivate the development of\nopen-source LMs specialized in evaluations. On the other hand, existing open\nevaluator LMs exhibit critical shortcomings: 1) they issue scores that\nsignificantly diverge from those assigned by humans, and 2) they lack the\nflexibility to perform both direct assessment and pairwise ranking, the two\nmost prevalent forms of assessment. Additionally, they do not possess the\nability to evaluate based on custom evaluation criteria, focusing instead on\ngeneral attributes like helpfulness and harmlessness. To address these issues,\nwe introduce Prometheus 2, a more powerful evaluator LM than its predecessor\nthat closely mirrors human and GPT-4 judgements. Moreover, it is capable of\nprocessing both direct assessment and pair-wise ranking formats grouped with a\nuser-defined evaluation criteria. On four direct assessment benchmarks and four\npairwise ranking benchmarks, Prometheus 2 scores the highest correlation and\nagreement with humans and proprietary LM judges among all tested open evaluator\nLMs. Our models, code, and data are all publicly available at\nhttps://github.com/prometheus-eval/prometheus-eval.",
        "ArXiv Link": "https://arxiv.org/abs/2405.01535",
        "PDF Link": "https://arxiv.org/pdf/2405.01535",
        "Upvotes": "27",
        "Date": "2024-05-03"
    },
    {
        "Title": "StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video\n  Generation",
        "Abstract": "For recent diffusion-based generative models, maintaining consistent content\nacross a series of generated images, especially those containing subjects and\ncomplex details, presents a significant challenge. In this paper, we propose a\nnew way of self-attention calculation, termed Consistent Self-Attention, that\nsignificantly boosts the consistency between the generated images and augments\nprevalent pretrained diffusion-based text-to-image models in a zero-shot\nmanner. To extend our method to long-range video generation, we further\nintroduce a novel semantic space temporal motion prediction module, named\nSemantic Motion Predictor. It is trained to estimate the motion conditions\nbetween two provided images in the semantic spaces. This module converts the\ngenerated sequence of images into videos with smooth transitions and consistent\nsubjects that are significantly more stable than the modules based on latent\nspaces only, especially in the context of long video generation. By merging\nthese two novel components, our framework, referred to as StoryDiffusion, can\ndescribe a text-based story with consistent images or videos encompassing a\nrich variety of contents. The proposed StoryDiffusion encompasses pioneering\nexplorations in visual story generation with the presentation of images and\nvideos, which we hope could inspire more research from the aspect of\narchitectural modifications. Our code is made publicly available at\nhttps://github.com/HVision-NKU/StoryDiffusion.",
        "ArXiv Link": "https://arxiv.org/abs/2405.01434",
        "PDF Link": "https://arxiv.org/pdf/2405.01434",
        "Upvotes": "15",
        "Date": "2024-05-03"
    },
    {
        "Title": "LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report",
        "Abstract": "Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted\nmethods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models\n(LLMs). LoRA reduces the number of trainable parameters and memory usage while\nachieving comparable performance to full fine-tuning. We aim to assess the\nviability of training and serving LLMs fine-tuned with LoRA in real-world\napplications. First, we measure the quality of LLMs fine-tuned with quantized\nlow rank adapters across 10 base models and 31 tasks for a total of 310 models.\nWe find that 4-bit LoRA fine-tuned models outperform base models by 34 points\nand GPT-4 by 10 points on average. Second, we investigate the most effective\nbase models for fine-tuning and assess the correlative and predictive\ncapacities of task complexity heuristics in forecasting the outcomes of\nfine-tuning. Finally, we evaluate the latency and concurrency capabilities of\nLoRAX, an open-source Multi-LoRA inference server that facilitates the\ndeployment of multiple LoRA fine-tuned models on a single GPU using shared base\nmodel weights and dynamic adapter loading. LoRAX powers LoRA Land, a web\napplication that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA\nA100 GPU with 80GB memory. LoRA Land highlights the quality and\ncost-effectiveness of employing multiple specialized LLMs over a single,\ngeneral-purpose LLM.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00732",
        "PDF Link": "https://arxiv.org/pdf/2405.00732",
        "Upvotes": "12",
        "Date": "2024-05-03"
    },
    {
        "Title": "WildChat: 1M ChatGPT Interaction Logs in the Wild",
        "Abstract": "Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite\ntheir widespread use, there remains a lack of public datasets showcasing how\nthese tools are used by a population of users in practice. To bridge this gap,\nwe offered free access to ChatGPT for online users in exchange for their\naffirmative, consensual opt-in to anonymously collect their chat transcripts\nand request headers. From this, we compiled WildChat, a corpus of 1 million\nuser-ChatGPT conversations, which consists of over 2.5 million interaction\nturns. We compare WildChat with other popular user-chatbot interaction\ndatasets, and find that our dataset offers the most diverse user prompts,\ncontains the largest number of languages, and presents the richest variety of\npotentially toxic use-cases for researchers to study. In addition to\ntimestamped chat transcripts, we enrich the dataset with demographic data,\nincluding state, country, and hashed IP addresses, alongside request headers.\nThis augmentation allows for more detailed analysis of user behaviors across\ndifferent geographical regions and temporal dimensions. Finally, because it\ncaptures a broad range of use cases, we demonstrate the dataset's potential\nutility in fine-tuning instruction-following models. WildChat is released at\nhttps://wildchat.allen.ai under AI2 ImpACT Licenses.",
        "ArXiv Link": "https://arxiv.org/abs/2405.01470",
        "PDF Link": "https://arxiv.org/pdf/2405.01470",
        "Upvotes": "11",
        "Date": "2024-05-03"
    },
    {
        "Title": "FLAME: Factuality-Aware Alignment for Large Language Models",
        "Abstract": "Alignment is a standard procedure to fine-tune pre-trained large language\nmodels (LLMs) to follow natural language instructions and serve as helpful AI\nassistants. We have observed, however, that the conventional alignment process\nfails to enhance the factual accuracy of LLMs, and often leads to the\ngeneration of more false facts (i.e. hallucination). In this paper, we study\nhow to make the LLM alignment process more factual, by first identifying\nfactors that lead to hallucination in both alignment steps:\\ supervised\nfine-tuning (SFT) and reinforcement learning (RL). In particular, we find that\ntraining the LLM on new knowledge or unfamiliar texts can encourage\nhallucination. This makes SFT less factual as it trains on human labeled data\nthat may be novel to the LLM. Furthermore, reward functions used in standard RL\ncan also encourage hallucination, because it guides the LLM to provide more\nhelpful responses on a diverse set of instructions, often preferring longer and\nmore detailed responses. Based on these observations, we propose\nfactuality-aware alignment, comprised of factuality-aware SFT and\nfactuality-aware RL through direct preference optimization. Experiments show\nthat our proposed factuality-aware alignment guides LLMs to output more factual\nresponses while maintaining instruction-following capability.",
        "ArXiv Link": "https://arxiv.org/abs/2405.01525",
        "PDF Link": "https://arxiv.org/pdf/2405.01525",
        "Upvotes": "7",
        "Date": "2024-05-03"
    },
    {
        "Title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment",
        "Abstract": "Aligning Large Language Models (LLMs) with human values and preferences is\nessential for making them helpful and safe. However, building efficient tools\nto perform alignment can be challenging, especially for the largest and most\ncompetent LLMs which often contain tens or hundreds of billions of parameters.\nWe create NeMo-Aligner, a toolkit for model alignment that can efficiently\nscale to using hundreds of GPUs for training. NeMo-Aligner comes with highly\noptimized and scalable implementations for major paradigms of model alignment\nsuch as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference\nOptimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,\nour toolkit supports running most of the alignment techniques in a Parameter\nEfficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for\nextensibility, allowing support for other alignment techniques with minimal\neffort. It is open-sourced with Apache 2.0 License and we invite community\ncontributions at https://github.com/NVIDIA/NeMo-Aligner",
        "ArXiv Link": "https://arxiv.org/abs/2405.01481",
        "PDF Link": "https://arxiv.org/pdf/2405.01481",
        "Upvotes": "6",
        "Date": "2024-05-03"
    },
    {
        "Title": "Customizing Text-to-Image Models with a Single Image Pair",
        "Abstract": "Art reinterpretation is the practice of creating a variation of a reference\nwork, making a paired artwork that exhibits a distinct artistic style. We ask\nif such an image pair can be used to customize a generative model to capture\nthe demonstrated stylistic difference. We propose Pair Customization, a new\ncustomization method that learns stylistic difference from a single image pair\nand then applies the acquired style to the generation process. Unlike existing\nmethods that learn to mimic a single concept from a collection of images, our\nmethod captures the stylistic difference between paired images. This allows us\nto apply a stylistic change without overfitting to the specific image content\nin the examples. To address this new task, we employ a joint optimization\nmethod that explicitly separates the style and content into distinct LoRA\nweight spaces. We optimize these style and content weights to reproduce the\nstyle and content images while encouraging their orthogonality. During\ninference, we modify the diffusion process via a new style guidance based on\nour learned weights. Both qualitative and quantitative experiments show that\nour method can effectively learn style while avoiding overfitting to image\ncontent, highlighting the potential of modeling such stylistic differences from\na single image pair.",
        "ArXiv Link": "https://arxiv.org/abs/2405.01536",
        "PDF Link": "https://arxiv.org/pdf/2405.01536",
        "Upvotes": "5",
        "Date": "2024-05-03"
    },
    {
        "Title": "LLM-AD: Large Language Model based Audio Description System",
        "Abstract": "The development of Audio Description (AD) has been a pivotal step forward in\nmaking video content more accessible and inclusive. Traditionally, AD\nproduction has demanded a considerable amount of skilled labor, while existing\nautomated approaches still necessitate extensive training to integrate\nmultimodal inputs and tailor the output from a captioning style to an AD style.\nIn this paper, we introduce an automated AD generation pipeline that harnesses\nthe potent multimodal and instruction-following capacities of GPT-4V(ision).\nNotably, our methodology employs readily available components, eliminating the\nneed for additional training. It produces ADs that not only comply with\nestablished natural language AD production standards but also maintain\ncontextually consistent character information across frames, courtesy of a\ntracking-based character recognition module. A thorough analysis on the MAD\ndataset reveals that our approach achieves a performance on par with\nlearning-based methods in automated AD production, as substantiated by a CIDEr\nscore of 20.5.",
        "ArXiv Link": "https://arxiv.org/abs/2405.00983",
        "PDF Link": "https://arxiv.org/pdf/2405.00983",
        "Upvotes": "5",
        "Date": "2024-05-03"
    },
    {
        "Title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding",
        "Abstract": "Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.",
        "ArXiv Link": "https://arxiv.org/abs/2406.19389",
        "PDF Link": "https://arxiv.org/pdf/2406.19389",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of\n  LLMs",
        "Abstract": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs) due to the extensive and precise chain of reasoning required for\naccuracy. Ensuring the correctness of each reasoning step is critical. To\naddress this, we aim to enhance the robustness and factuality of LLMs by\nlearning from human feedback. However, Direct Preference Optimization (DPO) has\nshown limited benefits for long-chain mathematical reasoning, as models\nemploying DPO struggle to identify detailed errors in incorrect answers. This\nlimitation stems from a lack of fine-grained process supervision. We propose a\nsimple, effective, and data-efficient method called Step-DPO, which treats\nindividual reasoning steps as units for preference optimization rather than\nevaluating answers holistically. Additionally, we have developed a data\nconstruction pipeline for Step-DPO, enabling the creation of a high-quality\ndataset containing 10K step-wise preference pairs. We also observe that in DPO,\nself-generated data is more effective than data generated by humans or GPT-4,\ndue to the latter's out-of-distribution nature. Our findings demonstrate that\nas few as 10K preference data pairs and fewer than 500 Step-DPO training steps\ncan yield a nearly 3% gain in accuracy on MATH for models with over 70B\nparameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves\nscores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively,\nsurpassing a series of closed-source models, including GPT-4-1106,\nClaude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at\nhttps://github.com/dvlab-research/Step-DPO.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18629",
        "PDF Link": "https://arxiv.org/pdf/2406.18629",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data",
        "Abstract": "We train a model to generate images from multimodal prompts of interleaved\ntext and images such as \"a <picture of a man> man and his <picture of a dog>\ndog in an <picture of a cartoon> animated style.\" We bootstrap a multimodal\ndataset by extracting semantically meaningful image crops corresponding to\nwords in the image captions of synthetically generated and publicly available\ntext-image data. Our model, MUMU, is composed of a vision-language model\nencoder with a diffusion decoder and is trained on a single 8xH100 GPU node.\nDespite being only trained on crops from the same image, MUMU learns to compose\ninputs from different images into a coherent output. For example, an input of a\nrealistic person and a cartoon will output the same person in the cartoon\nstyle, and an input of a standing subject and a scooter will output the subject\nriding the scooter. As a result, our model generalizes to tasks such as style\ntransfer and character consistency. Our results show the promise of using\nmultimodal models as general purpose controllers for image generation.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18790",
        "PDF Link": "https://arxiv.org/pdf/2406.18790",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Simulating Classroom Education with LLM-Empowered Agents",
        "Abstract": "Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.",
        "ArXiv Link": "https://arxiv.org/abs/2406.19226",
        "PDF Link": "https://arxiv.org/pdf/2406.19226",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented\n  Generation",
        "Abstract": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty\nto preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release\nour code at https://github.com/THU-KEG/SeaKR.",
        "ArXiv Link": "https://arxiv.org/abs/2406.19215",
        "PDF Link": "https://arxiv.org/pdf/2406.19215",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Aligning Teacher with Student Preferences for Tailored Training Data\n  Generation",
        "Abstract": "Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.",
        "ArXiv Link": "https://arxiv.org/abs/2406.19227",
        "PDF Link": "https://arxiv.org/pdf/2406.19227",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Can LLMs Learn by Teaching? A Preliminary Study",
        "Abstract": "Teaching to improve student models (e.g., knowledge distillation) is an\nextensively studied methodology in LLMs. However, for humans, teaching not only\nimproves students but also improves teachers. We ask: Can LLMs also learn by\nteaching (LbT)? If yes, we can potentially unlock the possibility of\ncontinuously advancing the models without solely relying on human-produced data\nor stronger models. In this paper, we provide a preliminary exploration of this\nambitious agenda. We show that LbT ideas can be incorporated into existing LLM\ntraining/prompting pipelines and provide noticeable improvements. Specifically,\nwe design three methods, each mimicking one of the three levels of LbT in\nhumans: observing students' feedback, learning from the feedback, and learning\niteratively, with the goals of improving answer accuracy without training and\nimproving models' inherent capability with fine-tuning. The findings are\nencouraging. For example, similar to LbT in human, we see that: (1) LbT can\ninduce weak-to-strong generalization: strong models can improve themselves by\nteaching other weak models; (2) Diversity in students might help: teaching\nmultiple students could be better than teaching one student or the teacher\nitself. We hope that this early promise can inspire future research on LbT and\nmore broadly adopting the advanced techniques in education to improve LLMs. The\ncode is available at https://github.com/imagination-research/lbt.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14629",
        "PDF Link": "https://arxiv.org/pdf/2406.14629",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression",
        "Abstract": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by 3.9times with the same\naverage attention span, boosting retrieval accuracy by 1.5-7.1times over the\nuniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from 9%-36% to within 5%\nacross two long-context understanding benchmarks. MoA achieves a\n1.2-1.4times GPU memory reduction and boosts decode throughput by 5.5-6.7\ntimes for 7B and 13B dense models on a single GPU, with minimal impact on\nperformance.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14909",
        "PDF Link": "https://arxiv.org/pdf/2406.14909",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Is Programming by Example solved by LLMs?",
        "Abstract": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n`solved' PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.",
        "ArXiv Link": "https://arxiv.org/abs/2406.08316",
        "PDF Link": "https://arxiv.org/pdf/2406.08316",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Dataset Size Recovery from LoRA Weights",
        "Abstract": "Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.",
        "ArXiv Link": "https://arxiv.org/abs/2406.19395",
        "PDF Link": "https://arxiv.org/pdf/2406.19395",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
        "Abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.",
        "ArXiv Link": "https://arxiv.org/abs/2406.19314",
        "PDF Link": "https://arxiv.org/pdf/2406.19314",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding",
        "Abstract": "Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io",
        "ArXiv Link": "https://arxiv.org/abs/2406.19263",
        "PDF Link": "https://arxiv.org/pdf/2406.19263",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for\n  Memory-Efficient Embeddings",
        "Abstract": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
        "ArXiv Link": "https://arxiv.org/abs/2406.19223",
        "PDF Link": "https://arxiv.org/pdf/2406.19223",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Understand What LLM Needs: Dual Preference Alignment for\n  Retrieval-Augmented Generation",
        "Abstract": "Retrieval-augmented generation (RAG) has demonstrated effectiveness in\nmitigating the hallucination problem of large language models (LLMs). However,\nthe difficulty of aligning the retriever with the diverse LLMs' knowledge\npreferences inevitably poses an inevitable challenge in developing a reliable\nRAG system. To address this issue, we propose DPA-RAG, a universal framework\ndesigned to align diverse knowledge preferences within RAG systems.\nSpecifically, we initially introduce a preference knowledge construction\npipline and incorporate five novel query augmentation strategies to alleviate\npreference data scarcity. Based on preference data, DPA-RAG accomplishes both\nexternal and internal preference alignment: 1) It jointly integrate pair-wise,\npoint-wise, and contrastive preference alignment abilities into the reranker,\nachieving external preference alignment among RAG components. 2) It further\nintroduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),\nenabling LLMs to implicitly capture knowledge aligned with their reasoning\npreferences, achieving LLMs' internal alignment. Experimental results across\nfour knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all\nbaselines and seamlessly integrates both black-box and open-sourced LLM\nreaders. Further qualitative analysis and discussions also provide empirical\nguidance for achieving reliable RAG systems. Our code is publicly available at\nhttps://github.com/dongguanting/DPA-RAG.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18676",
        "PDF Link": "https://arxiv.org/pdf/2406.18676",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for\n  Vision-Language Models",
        "Abstract": "Large vision-language models (LVLMs) hallucinate: certain context cues in an\nimage may trigger the language module's overconfident and incorrect reasoning\non abnormal or hypothetical objects. Though a few benchmarks have been\ndeveloped to investigate LVLM hallucinations, they mainly rely on hand-crafted\ncorner cases whose fail patterns may hardly generalize, and finetuning on them\ncould undermine their validity. These motivate us to develop the first\nautomatic benchmark generation approach, AUTOHALLUSION, that harnesses a few\nprincipal strategies to create diverse hallucination examples. It probes the\nlanguage modules in LVLMs for context cues and uses them to synthesize images\nby: (1) adding objects abnormal to the context cues; (2) for two co-occurring\nobjects, keeping one and excluding the other; or (3) removing objects closely\ntied to the context cues. It then generates image-based questions whose\nground-truth answers contradict the language module's prior. A model has to\novercome contextual biases and distractions to reach correct answers, while\nincorrect or inconsistent answers indicate hallucinations. AUTOHALLUSION\nenables us to create new benchmarks at the minimum cost and thus overcomes the\nfragility of hand-crafted benchmarks. It also reveals common failure patterns\nand reasons, providing key insights to detect, avoid, or control\nhallucinations. Comprehensive evaluations of top-tier LVLMs, e.g.,\nGPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, show a 97.7% and\n98.7% success rate of hallucination induction on synthetic and real-world\ndatasets of AUTOHALLUSION, paving the way for a long battle against\nhallucinations.",
        "ArXiv Link": "https://arxiv.org/abs/2406.10900",
        "PDF Link": "https://arxiv.org/pdf/2406.10900",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech\n  Recognition Using LLMs",
        "Abstract": "Motivated by the widespread increase in the phenomenon of code-switching\nbetween Egyptian Arabic and English in recent times, this paper explores the\nintricacies of machine translation (MT) and automatic speech recognition (ASR)\nsystems, focusing on translating code-switched Egyptian Arabic-English to\neither English or Egyptian Arabic. Our goal is to present the methodologies\nemployed in developing these systems, utilizing large language models such as\nLLama and Gemma. In the field of ASR, we explore the utilization of the Whisper\nmodel for code-switched Egyptian Arabic recognition, detailing our experimental\nprocedures including data preprocessing and training techniques. Through the\nimplementation of a consecutive speech-to-text translation system that\nintegrates ASR with MT, we aim to overcome challenges posed by limited\nresources and the unique characteristics of the Egyptian Arabic dialect.\nEvaluation against established metrics showcases promising results, with our\nmethodologies yielding a significant improvement of 56% in English\ntranslation over the state-of-the-art and 9.3% in Arabic translation. Since\ncode-switching is deeply inherent in spoken languages, it is crucial that ASR\nsystems can effectively handle this phenomenon. This capability is crucial for\nenabling seamless interaction in various domains, including business\nnegotiations, cultural exchanges, and academic discourse. Our models and code\nare available as open-source resources. Code:\nhttp://github.com/ahmedheakl/arazn-llm}, Models:\nhttp://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18120",
        "PDF Link": "https://arxiv.org/pdf/2406.18120",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Benchmarking Mental State Representations in Language Models",
        "Abstract": "While numerous works have assessed the generative performance of language\nmodels (LMs) on tasks requiring Theory of Mind reasoning, research into the\nmodels' internal representation of mental states remains limited. Recent work\nhas used probing to demonstrate that LMs can represent beliefs of themselves\nand others. However, these claims are accompanied by limited evaluation, making\nit difficult to assess how mental state representations are affected by model\ndesign and training choices. We report an extensive benchmark with various LM\ntypes with different model sizes, fine-tuning approaches, and prompt designs to\nstudy the robustness of mental state representations and memorisation issues\nwithin the probes. Our results show that the quality of models' internal\nrepresentations of the beliefs of others increases with model size and, more\ncrucially, with fine-tuning. We are the first to study how prompt variations\nimpact probing performance on theory of mind tasks. We demonstrate that models'\nrepresentations are sensitive to prompt variations, even when such variations\nshould be beneficial. Finally, we complement previous activation editing\nexperiments on Theory of Mind tasks and show that it is possible to improve\nmodels' reasoning performance by steering their activations without the need to\ntrain any probe.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17513",
        "PDF Link": "https://arxiv.org/pdf/2406.17513",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets\n  and Large Language Models",
        "Abstract": "The increasing reliance on online recruitment platforms coupled with the\nadoption of AI technologies has highlighted the critical need for efficient\nresume classification methods. However, challenges such as small datasets, lack\nof standardized resume templates, and privacy concerns hinder the accuracy and\neffectiveness of existing classification models. In this work, we address these\nchallenges by presenting a comprehensive approach to resume classification. We\ncurated a large-scale dataset of 13,389 resumes from diverse sources and\nemployed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for\nclassification. Our results demonstrate significant improvements over\ntraditional machine learning approaches, with our best model achieving a top-1\naccuracy of 92\\% and a top-5 accuracy of 97.5\\%. These findings underscore the\nimportance of dataset quality and advanced model architectures in enhancing the\naccuracy and robustness of resume classification systems, thus advancing the\nfield of online recruitment practices.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18125",
        "PDF Link": "https://arxiv.org/pdf/2406.18125",
        "Upvotes": "44",
        "Date": "2024-07-07"
    },
    {
        "Title": "Adam-mini: Use Fewer Learning Rates To Gain More",
        "Abstract": "We propose Adam-mini, an optimizer that achieves on-par or better performance\nthan AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory by\ncutting down the learning rate resources in Adam (i.e., 1/v). We find\nthat geq 90% of these learning rates in v could be harmlessly removed if\nwe (1) carefully partition the parameters into blocks following our proposed\nprinciple on Hessian structure; (2) assign a single but good learning rate to\neach parameter block. We further find that, for each of these parameter blocks,\nthere exists a single high-quality learning rate that can outperform Adam,\nprovided that sufficient resources are available to search it out. We then\nprovide one cost-effective way to find good learning rates and propose\nAdam-mini. Empirically, we verify that Adam-mini performs on par or better than\nAdamW on various language models sized from 125M to 7B for pre-training,\nsupervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini\nalso alleviates communication overheads among GPUs and CPUs, thereby increasing\nthroughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW\nwhen pre-training Llama2-7B on 2times A800-80GB GPUs, which saves 33%\nwall-clock time for pre-training.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16793",
        "PDF Link": "https://arxiv.org/pdf/2406.16793",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Octo-planner: On-device Language Model for Planner-Action Agents",
        "Abstract": "AI agents have become increasingly significant in various domains, enabling\nautonomous decision-making and problem-solving. To function effectively, these\nagents require a planning process that determines the best course of action and\nthen executes the planned actions. In this paper, we present an efficient\non-device Planner-Action framework that separates planning and action execution\ninto two distinct components: a planner agent based on Phi-3 Mini, a 3.8\nbillion parameter LLM optimized for edge devices, and an action agent using the\nOctopus model for function execution. The planner agent first responds to user\nqueries by decomposing tasks into a sequence of sub-steps, which are then\nexecuted by the action agent. To optimize performance on resource-constrained\ndevices, we employ model fine-tuning instead of in-context learning, reducing\ncomputational costs and energy consumption while improving response times. Our\napproach involves using GPT-4 to generate diverse planning queries and\nresponses based on available functions, with subsequent validations to ensure\ndata quality. We fine-tune the Phi-3 Mini model on this curated dataset,\nachieving a 97\\% success rate in our in-domain test environment. To address\nmulti-domain planning challenges, we developed a multi-LoRA training method\nthat merges weights from LoRAs trained on distinct function subsets. This\napproach enables flexible handling of complex, multi-domain queries while\nmaintaining computational efficiency on resource-constrained devices. To\nsupport further research, we have open-sourced our model weights at\nhttps://huggingface.co/NexaAIDev/octopus-planning. For the demo, please\nrefer to https://www.nexa4ai.com/octo-planner.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18082",
        "PDF Link": "https://arxiv.org/pdf/2406.18082",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of\n  Text-to-Time-lapse Video Generation",
        "Abstract": "We propose a novel text-to-video (T2V) generation benchmark,\nChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the\nT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast\nto existing benchmarks that focus on the visual quality and textual relevance\nof generated videos, ChronoMagic-Bench focuses on the model's ability to\ngenerate time-lapse videos with significant metamorphic amplitude and temporal\ncoherence. The benchmark probes T2V models for their physics, biology, and\nchemistry capabilities, in a free-form text query. For these purposes,\nChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,\ncategorized into four major types of time-lapse videos: biological,\nhuman-created, meteorological, and physical phenomena, which are further\ndivided into 75 subcategories. This categorization comprehensively evaluates\nthe model's capacity to handle diverse and complex transformations. To\naccurately align human preference with the benchmark, we introduce two new\nautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic\nattributes and temporal coherence. MTScore measures the metamorphic amplitude,\nreflecting the degree of change over time, while CHScore assesses the temporal\ncoherence, ensuring the generated videos maintain logical progression and\ncontinuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual\nevaluations of ten representative T2V models, revealing their strengths and\nweaknesses across different categories of prompts, and providing a thorough\nevaluation framework that addresses current gaps in video generation research.\nMoreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k\nhigh-quality pairs of 720p time-lapse videos and detailed captions ensuring\nhigh physical pertinence and large metamorphic amplitude.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18522",
        "PDF Link": "https://arxiv.org/pdf/2406.18522",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal\n  LLMs",
        "Abstract": "Chart understanding plays a pivotal role when applying Multimodal Large\nLanguage Models (MLLMs) to real-world tasks such as analyzing scientific papers\nor financial reports. However, existing datasets often focus on oversimplified\nand homogeneous charts with template-based questions, leading to an\nover-optimistic measure of progress. We demonstrate that although open-source\nmodels can appear to outperform strong proprietary models on these benchmarks,\na simple stress test with slightly different charts or questions can\ndeteriorate performance by up to 34.5%. In this work, we propose CharXiv, a\ncomprehensive evaluation suite involving 2,323 natural, challenging, and\ndiverse charts from arXiv papers. CharXiv includes two types of questions: 1)\ndescriptive questions about examining basic chart elements and 2) reasoning\nquestions that require synthesizing information across complex visual elements\nin the chart. To ensure quality, all charts and questions are handpicked,\ncurated, and verified by human experts. Our results reveal a substantial,\npreviously underestimated gap between the reasoning skills of the strongest\nproprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the\nstrongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.\nAll models lag far behind human performance of 80.5%, underscoring weaknesses\nin the chart understanding capabilities of existing MLLMs. We hope CharXiv\nfacilitates future research on MLLM chart understanding by providing a more\nrealistic and faithful measure of progress. Project page and leaderboard:\nhttps://charxiv.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2406.18521",
        "PDF Link": "https://arxiv.org/pdf/2406.18521",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "A Closer Look into Mixture-of-Experts in Large Language Models",
        "Abstract": "Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree recent MoE-based models and reveal some intriguing observations,\nincluding (1) Neurons act like fine-grained experts. (2) The router of MoE\nusually selects experts with larger output norms. (3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier. Based on\nthe observations, we also provide suggestions for a broad spectrum of MoE\npractitioners, such as router design and expert allocation. We hope this work\ncould shed light on future research on the MoE framework and other modular\narchitectures. Code is available at\nhttps://github.com/kamanphoebe/Look-into-MoEs.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18219",
        "PDF Link": "https://arxiv.org/pdf/2406.18219",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks,\n  and Refusals of LLMs",
        "Abstract": "We introduce WildGuard -- an open, light-weight moderation tool for LLM\nsafety that achieves three goals: (1) identifying malicious intent in user\nprompts, (2) detecting safety risks of model responses, and (3) determining\nmodel refusal rate. Together, WildGuard serves the increasing needs for\nautomatic safety moderation and evaluation of LLM interactions, providing a\none-stop tool with enhanced accuracy and broad coverage across 13 risk\ncategories. While existing open moderation tools such as Llama-Guard2 score\nreasonably well in classifying straightforward model interactions, they lag far\nbehind a prompted GPT-4, especially in identifying adversarial jailbreaks and\nin evaluating models' refusals, a key measure for evaluating safety behaviors\nin model responses.\n  To address these challenges, we construct WildGuardMix, a large-scale and\ncarefully balanced multi-task safety moderation dataset with 92K labeled\nexamples that cover vanilla (direct) prompts and adversarial jailbreaks, paired\nwith various refusal and compliance responses. WildGuardMix is a combination of\nWildGuardTrain, the training data of WildGuard, and WildGuardTest, a\nhigh-quality human-annotated moderation test set with 5K labeled items covering\nbroad risk scenarios. Through extensive evaluations on WildGuardTest and ten\nexisting public benchmarks, we show that WildGuard establishes state-of-the-art\nperformance in open-source safety moderation across all the three tasks\ncompared to ten strong existing open-source moderation models (e.g., up to\n26.4% improvement on refusal detection). Importantly, WildGuard matches and\nsometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt\nharmfulness identification). WildGuard serves as a highly effective safety\nmoderator in an LLM interface, reducing the success rate of jailbreak attacks\nfrom 79.8% to 2.4%.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18495",
        "PDF Link": "https://arxiv.org/pdf/2406.18495",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Symbolic Learning Enables Self-Evolving Agents",
        "Abstract": "The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".",
        "ArXiv Link": "https://arxiv.org/abs/2406.18532",
        "PDF Link": "https://arxiv.org/pdf/2406.18532",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "EHRCon: Dataset for Checking Consistency between Unstructured Notes and\n  Structured Tables in Electronic Health Records",
        "Abstract": "Electronic Health Records (EHRs) are integral for storing comprehensive\npatient medical records, combining structured data (e.g., medications) with\ndetailed clinical notes (e.g., physician notes). These elements are essential\nfor straightforward data retrieval and provide deep, contextual insights into\npatient care. However, they often suffer from discrepancies due to unintuitive\nEHR system designs and human errors, posing serious risks to patient safety. To\naddress this, we developed EHRCon, a new dataset and task specifically designed\nto ensure data consistency between structured tables and unstructured notes in\nEHRs. EHRCon was crafted in collaboration with healthcare professionals using\nthe MIMIC-III EHR dataset, and includes manual annotations of 3,943 entities\nacross 105 clinical notes checked against database entries for consistency.\nEHRCon has two versions, one using the original MIMIC-III schema, and another\nusing the OMOP CDM schema, in order to increase its applicability and\ngeneralizability. Furthermore, leveraging the capabilities of large language\nmodels, we introduce CheckEHR, a novel framework for verifying the consistency\nbetween clinical notes and database tables. CheckEHR utilizes an eight-stage\nprocess and shows promising results in both few-shot and zero-shot settings.\nThe code is available at https://github.com/dustn1259/EHRCon.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16341",
        "PDF Link": "https://arxiv.org/pdf/2406.16341",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning",
        "Abstract": "The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV)--compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15334",
        "PDF Link": "https://arxiv.org/pdf/2406.15334",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Understanding and Diagnosing Deep Reinforcement Learning",
        "Abstract": "Deep neural policies have recently been installed in a diverse range of\nsettings, from biotechnology to automated financial systems. However, the\nutilization of deep neural networks to approximate the value function leads to\nconcerns on the decision boundary stability, in particular, with regard to the\nsensitivity of policy decision making to indiscernible, non-robust features due\nto highly non-convex and complex deep neural manifolds. These concerns\nconstitute an obstruction to understanding the reasoning made by deep neural\npolicies, and their foundational limitations. Hence, it is crucial to develop\ntechniques that aim to understand the sensitivities in the learnt\nrepresentations of neural network policies. To achieve this we introduce a\ntheoretically founded method that provides a systematic analysis of the\nunstable directions in the deep neural policy decision boundary across both\ntime and space. Through experiments in the Arcade Learning Environment (ALE),\nwe demonstrate the effectiveness of our technique for identifying correlated\ndirections of instability, and for measuring how sample shifts remold the set\nof sensitive directions in the neural policy landscape. Most importantly, we\ndemonstrate that state-of-the-art robust training techniques yield learning of\ndisjoint unstable directions, with dramatically larger oscillations over time,\nwhen compared to standard training. We believe our results reveal the\nfundamental properties of the decision process made by reinforcement learning\npolicies, and can help in constructing reliable and robust deep neural\npolicies.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16979",
        "PDF Link": "https://arxiv.org/pdf/2406.16979",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large\n  Language Models",
        "Abstract": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, particularly in textual mathematical problem-solving. However,\nexisting open-source image instruction fine-tuning datasets, containing limited\nquestion-answer pairs per image, do not fully exploit visual information to\nenhance the multimodal mathematical reasoning capabilities of Multimodal LLMs\n(MLLMs). To bridge this gap, we address the lack of high-quality, diverse\nmultimodal mathematical datasets by collecting 40K high-quality images with\nquestion-answer pairs from 24 existing datasets and synthesizing 320K new\npairs, creating the MathV360K dataset, which enhances both the breadth and\ndepth of multimodal mathematical questions. We introduce Math-LLaVA, a\nLLaVA-1.5-based model fine-tuned with MathV360K. This novel approach\nsignificantly improves the multimodal mathematical reasoning capabilities of\nLLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V\non MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced\ngeneralizability, showing substantial improvements on the MMMU benchmark. Our\nresearch highlights the importance of dataset diversity and synthesis in\nadvancing MLLMs' mathematical reasoning abilities. The code and data are\navailable at: https://github.com/HZQ950419/Math-LLaVA.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17294",
        "PDF Link": "https://arxiv.org/pdf/2406.17294",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "MatchTime: Towards Automatic Soccer Game Commentary Generation",
        "Abstract": "Soccer is a globally popular sport with a vast audience, in this paper, we\nconsider constructing an automatic soccer game commentary model to improve the\naudiences' viewing experience. In general, we make the following contributions:\nFirst, observing the prevalent video-text misalignment in existing datasets, we\nmanually annotate timestamps for 49 matches, establishing a more robust\nbenchmark for soccer game commentary generation, termed as\nSN-Caption-test-align; Second, we propose a multi-modal temporal alignment\npipeline to automatically correct and filter the existing dataset at scale,\ncreating a higher-quality soccer game commentary dataset for training, denoted\nas MatchTime; Third, based on our curated dataset, we train an automatic\ncommentary generation model, named MatchVoice. Extensive experiments and\nablation studies have demonstrated the effectiveness of our alignment pipeline,\nand training model on the curated datasets achieves state-of-the-art\nperformance for commentary generation, showcasing that better alignment can\nlead to significant performance improvements in downstream tasks.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18530",
        "PDF Link": "https://arxiv.org/pdf/2406.18530",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)\n  Safer Language Models",
        "Abstract": "We introduce WildTeaming, an automatic LLM safety red-teaming framework that\nmines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of\nnovel jailbreak tactics, and then composes multiple tactics for systematic\nexploration of novel jailbreaks. Compared to prior work that performed\nred-teaming via recruited human workers, gradient-based optimization, or\niterative revision with LLMs, our work investigates jailbreaks from chatbot\nusers who were not specifically instructed to break the system. WildTeaming\nreveals previously unidentified vulnerabilities of frontier LLMs, resulting in\nup to 4.6x more diverse and successful adversarial attacks compared to\nstate-of-the-art jailbreak methods.\n  While many datasets exist for jailbreak evaluation, very few open-source\ndatasets exist for jailbreak training, as safety training data has been closed\neven when model weights are open. With WildTeaming we create WildJailbreak, a\nlarge-scale open-source synthetic safety dataset with 262K vanilla (direct\nrequest) and adversarial (complex jailbreak) prompt-response pairs. To mitigate\nexaggerated safety behaviors, WildJailbreak provides two contrastive types of\nqueries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that\nresemble harmful queries in form but contain no harm. As WildJailbreak\nconsiderably upgrades the quality and scale of existing safety resources, it\nuniquely enables us to examine the scaling effects of data and the interplay of\ndata properties and model capabilities during safety training. Through\nextensive experiments, we identify the training properties that enable an ideal\nbalance of safety behaviors: appropriate safeguarding without over-refusal,\neffective handling of vanilla and adversarial queries, and minimal, if any,\ndecrease in general capabilities. All components of WildJailbeak contribute to\nachieving balanced safety behaviors of models.",
        "ArXiv Link": "https://arxiv.org/abs/2406.18510",
        "PDF Link": "https://arxiv.org/pdf/2406.18510",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
        "Abstract": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17565",
        "PDF Link": "https://arxiv.org/pdf/2406.17565",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at\n  Scale",
        "Abstract": "The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17557",
        "PDF Link": "https://arxiv.org/pdf/2406.17557",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "YouDream: Generating Anatomically Controllable Consistent Text-to-3D\n  Animals",
        "Abstract": "3D generation guided by text-to-image diffusion models enables the creation\nof visually compelling assets. However previous methods explore generation\nbased on image or text. The boundaries of creativity are limited by what can be\nexpressed through words or the images that can be sourced. We present YouDream,\na method to generate high-quality anatomically controllable animals. YouDream\nis guided using a text-to-image diffusion model controlled by 2D views of a 3D\npose prior. Our method generates 3D animals that are not possible to create\nusing previous text-to-3D generative methods. Additionally, our method is\ncapable of preserving anatomic consistency in the generated animals, an area\nwhere prior text-to-3D approaches often struggle. Moreover, we design a fully\nautomated pipeline for generating commonly found animals. To circumvent the\nneed for human intervention to create a 3D pose, we propose a multi-agent LLM\nthat adapts poses from a limited library of animal 3D poses to represent the\ndesired animal. A user study conducted on the outcomes of YouDream demonstrates\nthe preference of the animal models generated by our method over others.\nTurntable results and code are released at https://youdream3d.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2406.16273",
        "PDF Link": "https://arxiv.org/pdf/2406.16273",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Unlocking Continual Learning Abilities in Language Models",
        "Abstract": "Language models (LMs) exhibit impressive performance and generalization\ncapabilities. However, LMs struggle with the persistent challenge of\ncatastrophic forgetting, which undermines their long-term sustainability in\ncontinual learning (CL). Existing approaches usually address the issue by\nincorporating old task data or task-wise inductive bias into LMs. However, old\ndata and accurate task information are often unavailable or costly to collect,\nhindering the availability of current CL approaches for LMs. To address this\nlimitation, we introduce MIGU (MagnItude-based\nGradient Updating for continual learning), a\nrehearsal-free and task-label-free method that only updates the model\nparameters with large magnitudes of output in LMs' linear layers. MIGU is based\non our observation that the L1-normalized magnitude distribution of the output\nin LMs' linear layers is different when the LM models deal with different task\ndata. By imposing this simple constraint on the gradient update process, we can\nleverage the inherent behaviors of LMs, thereby unlocking their innate CL\nabilities. Our experiments demonstrate that MIGU is universally applicable to\nall three LM architectures (T5, RoBERTa, and Llama2), delivering\nstate-of-the-art or on-par performance across continual finetuning and\ncontinual pre-training settings on four CL benchmarks. For example, MIGU brings\na 15.2% average accuracy improvement over conventional parameter-efficient\nfinetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly\nintegrate with all three existing CL types to further enhance performance. Code\nis available at https://github.com/wenyudu/MIGU{this https URL}.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17245",
        "PDF Link": "https://arxiv.org/pdf/2406.17245",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Aligning Diffusion Models with Noise-Conditioned Perception",
        "Abstract": "Recent advancements in human preference optimization, initially developed for\nLanguage Models (LMs), have shown promise for text-to-image Diffusion Models,\nenhancing prompt alignment, visual appeal, and user preference. Unlike LMs,\nDiffusion Models typically optimize in pixel or VAE space, which does not align\nwell with human perception, leading to slower and less efficient training\nduring the preference alignment stage. We propose using a perceptual objective\nin the U-Net embedding space of the diffusion model to address these issues.\nOur approach involves fine-tuning Stable Diffusion 1.5 and XL using Direct\nPreference Optimization (DPO), Contrastive Preference Optimization (CPO), and\nsupervised fine-tuning (SFT) within this embedding space. This method\nsignificantly outperforms standard latent-space implementations across various\nmetrics, including quality and computational cost. For SDXL, our approach\nprovides 60.8\\% general preference, 62.2\\% visual appeal, and 52.1\\% prompt\nfollowing against original open-sourced SDXL-DPO on the PartiPrompts dataset,\nwhile significantly reducing compute. Our approach not only improves the\nefficiency and quality of human preference alignment for diffusion models but\nis also easily integrable with other optimization techniques. The training code\nand LoRA weights will be available here:\nhttps://huggingface.co/alexgambashidze/SDXL\\_NCP-DPO\\_v0.1",
        "ArXiv Link": "https://arxiv.org/abs/2406.17636",
        "PDF Link": "https://arxiv.org/pdf/2406.17636",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "DiffusionPDE: Generative PDE-Solving Under Partial Observation",
        "Abstract": "We introduce a general framework for solving partial differential equations\n(PDEs) using generative diffusion models. In particular, we focus on the\nscenarios where we do not have the full knowledge of the scene necessary to\napply classical solvers. Most existing forward or inverse PDE approaches\nperform poorly when the observations on the data or the underlying coefficients\nare incomplete, which is a common assumption for real-world measurements. In\nthis work, we propose DiffusionPDE that can simultaneously fill in the missing\ninformation and solve a PDE by modeling the joint distribution of the solution\nand coefficient spaces. We show that the learned generative priors lead to a\nversatile framework for accurately solving a wide range of PDEs under partial\nobservation, significantly outperforming the state-of-the-art methods for both\nforward and inverse directions.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17763",
        "PDF Link": "https://arxiv.org/pdf/2406.17763",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
        "Abstract": "The long-context capabilities of large language models (LLMs) have been a hot\ntopic in recent years. To evaluate the performance of LLMs in different\nscenarios, various assessment benchmarks have emerged. However, as most of\nthese benchmarks focus on identifying key information to answer questions,\nwhich mainly requires the retrieval ability of LLMs, these benchmarks can\npartially represent the reasoning performance of LLMs from large amounts of\ninformation. Meanwhile, although LLMs often claim to have context windows of\n32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual\nsupported length of these LLMs. To address these issues, we propose the LongIns\nbenchmark dataset, a challenging long-context instruction-based exam for LLMs,\nwhich is built based on the existing instruction datasets. Specifically, in our\nLongIns, we introduce three evaluation settings: Global Instruction & Single\nTask (GIST), Local Instruction & Single Task (LIST), and Local Instruction &\nMultiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations\non existing LLMs and have the following important findings: (1). The\ntop-performing GPT-4 with 128k context length performs poorly on the evaluation\ncontext window of 16k in our LongIns. (2). For the multi-hop reasoning ability\nof many existing LLMs, significant efforts are still needed under short context\nwindows (less than 4k).",
        "ArXiv Link": "https://arxiv.org/abs/2406.17588",
        "PDF Link": "https://arxiv.org/pdf/2406.17588",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning",
        "Abstract": "Multi-modal large language models (MLLMs) have made significant strides in\nvarious visual understanding tasks. However, the majority of these models are\nconstrained to process low-resolution images, which limits their effectiveness\nin perception tasks that necessitate detailed visual information. In our study,\nwe present MG-LLaVA, an innovative MLLM that enhances the model's visual\nprocessing capabilities by incorporating a multi-granularity vision flow, which\nincludes low-resolution, high-resolution, and object-centric features. We\npropose the integration of an additional high-resolution visual encoder to\ncapture fine-grained details, which are then fused with base visual features\nthrough a Conv-Gate fusion network. To further refine the model's object\nrecognition abilities, we incorporate object-level features derived from\nbounding boxes identified by offline detectors. Being trained solely on\npublicly available multimodal data through instruction tuning, MG-LLaVA\ndemonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide\nvariety of language encoders, ranging from 3.8B to 34B, to evaluate the model's\nperformance comprehensively. Extensive evaluations across multiple benchmarks\ndemonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code will be available at\nhttps://github.com/PhoenixZ810/MG-LLaVA.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17770",
        "PDF Link": "https://arxiv.org/pdf/2406.17770",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "APIGen: Automated Pipeline for Generating Verifiable and Diverse\n  Function-Calling Datasets",
        "Abstract": "The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2406.18518",
        "PDF Link": "https://arxiv.org/pdf/2406.18518",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation",
        "Abstract": "In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth",
        "ArXiv Link": "https://arxiv.org/abs/2406.17758",
        "PDF Link": "https://arxiv.org/pdf/2406.17758",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended\n  Multi-Doc QA",
        "Abstract": "Long-context modeling capabilities have garnered widespread attention,\nleading to the emergence of Large Language Models (LLMs) with ultra-context\nwindows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually\ncatching up. However, existing benchmarks employ irrelevant noise texts to\nartificially extend the length of test cases, diverging from the real-world\nscenarios of long-context applications. To bridge this gap, we propose a novel\nlong-context benchmark, Loong, aligning with realistic scenarios through\nextended multi-document question answering (QA). Unlike typical document QA, in\nLoong's test cases, each document is relevant to the final answer, ignoring any\ndocument will lead to the failure of the answer. Furthermore, Loong introduces\nfour types of tasks with a range of context lengths: Spotlight Locating,\nComparison, Clustering, and Chain of Reasoning, to facilitate a more realistic\nand comprehensive evaluation of long-context understanding. Extensive\nexperiments indicate that existing long-context language models still exhibit\nconsiderable potential for enhancement. Retrieval augmented generation (RAG)\nachieves poor performance, demonstrating that Loong can reliably assess the\nmodel's long-context modeling capabilities.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17419",
        "PDF Link": "https://arxiv.org/pdf/2406.17419",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Segment Any Text: A Universal Approach for Robust, Efficient and\n  Adaptable Sentence Segmentation",
        "Abstract": "Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16678",
        "PDF Link": "https://arxiv.org/pdf/2406.16678",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "On the Transformations across Reward Model, Parameter Update, and\n  In-Context Prompt",
        "Abstract": "Despite the general capabilities of pre-trained large language models (LLMs),\nthey still need further adaptation to better serve practical applications. In\nthis paper, we demonstrate the interchangeability of three popular and distinct\nadaptation tools: parameter updating, reward modeling, and in-context\nprompting. This interchangeability establishes a triangular framework with six\ntransformation directions, each of which facilitates a variety of applications.\nOur work offers a holistic view that unifies numerous existing studies and\nsuggests potential research directions. We envision our work as a useful\nroadmap for future research on LLMs.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16377",
        "PDF Link": "https://arxiv.org/pdf/2406.16377",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models",
        "Abstract": "Diffusion model has demonstrated remarkable capability in video generation,\nwhich further sparks interest in introducing trajectory control into the\ngeneration process. While existing works mainly focus on training-based methods\n(e.g., conditional adapter), we argue that diffusion model itself allows decent\ncontrol over the generated content without requiring any training. In this\nstudy, we introduce a tuning-free framework to achieve trajectory-controllable\nvideo generation, by imposing guidance on both noise construction and attention\ncomputation. Specifically, 1) we first show several instructive phenomenons and\nanalyze how initial noises influence the motion trajectory of generated\ncontent. 2) Subsequently, we propose FreeTraj, a tuning-free approach that\nenables trajectory control by modifying noise sampling and attention\nmechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger\nvideo generation with controllable trajectories. Equipped with these designs,\nusers have the flexibility to provide trajectories manually or opt for\ntrajectories automatically generated by the LLM trajectory planner. Extensive\nexperiments validate the efficacy of our approach in enhancing the trajectory\ncontrollability of video diffusion models.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16863",
        "PDF Link": "https://arxiv.org/pdf/2406.16863",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "DialSim: A Real-Time Simulator for Evaluating Long-Term Dialogue\n  Understanding of Conversational Agents",
        "Abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversational agents, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nagents often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, an agent is assigned the role of a character from\npopular TV shows, requiring it to respond to spontaneous questions using past\ndialogue information and to distinguish between known and unknown information.\nKey features of DialSim include evaluating the agent's ability to respond\nwithin a reasonable time limit, handling long-term multi-party dialogues, and\nmanaging adversarial settings (e.g., swap character names) to challenge the\nagent's reliance on pre-trained knowledge. We utilized this simulator to\nevaluate the latest conversational agents and analyze their limitations. Our\nexperiments highlight both the strengths and weaknesses of these agents,\nproviding valuable insights for future improvements in the field of\nconversational AI. DialSim is available at\nhttps://github.com/jiho283/Simulator.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13144",
        "PDF Link": "https://arxiv.org/pdf/2406.13144",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Image Conductor: Precision Control for Interactive Video Synthesis",
        "Abstract": "Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/",
        "ArXiv Link": "https://arxiv.org/abs/2406.15339",
        "PDF Link": "https://arxiv.org/pdf/2406.15339",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse\n  Gradients",
        "Abstract": "Large language model (LLM) training and finetuning are often bottlenecked by\nlimited GPU memory. While existing projection-based optimization methods\naddress this by projecting gradients into a lower-dimensional subspace to\nreduce optimizer state memory, they typically rely on dense projection\nmatrices, which can introduce computational and memory overheads. In this work,\nwe propose Grass (GRAdient Stuctured Sparsification), a novel approach that\nleverages sparse projections to transform gradients into structured sparse\nupdates. This design not only significantly reduces memory usage for optimizer\nstates but also minimizes gradient memory footprint, computation, and\ncommunication costs, leading to substantial throughput improvements. Extensive\nexperiments on pretraining and finetuning tasks demonstrate that Grass achieves\ncompetitive performance to full-rank training and existing projection-based\nmethods. Notably, Grass enables half-precision pretraining of a 13B parameter\nLLaMA model on a single 40GB A100 GPU--a feat infeasible for previous\nmethods--and yields up to a 2times throughput improvement on an 8-GPU\nsystem. Code can be found at https://github.com/aashiqmuhamed/GRASS .",
        "ArXiv Link": "https://arxiv.org/abs/2406.17660",
        "PDF Link": "https://arxiv.org/pdf/2406.17660",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Large Language Models Assume People are More Rational than We Really are",
        "Abstract": "In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17055",
        "PDF Link": "https://arxiv.org/pdf/2406.17055",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Multi-property Steering of Large Language Models with Dynamic Activation\n  Composition",
        "Abstract": "Activation steering methods were shown to be effective in conditioning\nlanguage model generation by additively intervening over models' intermediate\nrepresentations. However, the evaluation of these techniques has so far been\nlimited to single conditioning properties and synthetic settings. In this work,\nwe conduct a comprehensive evaluation of various activation steering\nstrategies, highlighting the property-dependent nature of optimal parameters to\nensure a robust effect throughout generation. To address this issue, we propose\nDynamic Activation Composition, an information-theoretic approach to modulate\nthe steering intensity of one or more properties throughout generation. Our\nexperiments on multi-property steering show that our method successfully\nmaintains high conditioning while minimizing the impact of conditioning on\ngeneration fluency.",
        "ArXiv Link": "https://arxiv.org/abs/2406.17563",
        "PDF Link": "https://arxiv.org/pdf/2406.17563",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "Cross-Modality Safety Alignment",
        "Abstract": "As Artificial General Intelligence (AGI) becomes increasingly integrated into\nvarious facets of human life, ensuring the safety and ethical alignment of such\nsystems is paramount. Previous studies primarily focus on single-modality\nthreats, which may not suffice given the integrated and complex nature of\ncross-modality interactions. We introduce a novel safety alignment challenge\ncalled Safe Inputs but Unsafe Output (SIUO) to evaluate cross-modality safety\nalignment. Specifically, it considers cases where single modalities are safe\nindependently but could potentially lead to unsafe or unethical outputs when\ncombined. To empirically investigate this problem, we developed the SIUO, a\ncross-modality benchmark encompassing 9 critical safety domains, such as\nself-harm, illegal activities, and privacy violations. Our findings reveal\nsubstantial safety vulnerabilities in both closed- and open-source LVLMs, such\nas GPT-4V and LLaVA, underscoring the inadequacy of current models to reliably\ninterpret and respond to complex, real-world scenarios.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15279",
        "PDF Link": "https://arxiv.org/pdf/2406.15279",
        "Upvotes": "69",
        "Date": "2024-07-07"
    },
    {
        "Title": "DreamBench++: A Human-Aligned Benchmark for Personalized Image\n  Generation",
        "Abstract": "Personalized image generation holds great promise in assisting humans in\neveryday work and life due to its impressive function in creatively generating\npersonalized content. However, current evaluations either are automated but\nmisalign with humans or require human evaluations that are time-consuming and\nexpensive. In this work, we present DreamBench++, a human-aligned benchmark\nautomated by advanced multimodal GPT models. Specifically, we systematically\ndesign the prompts to let GPT be both human-aligned and self-aligned, empowered\nwith task reinforcement. Further, we construct a comprehensive dataset\ncomprising diverse images and prompts. By benchmarking 7 modern generative\nmodels, we demonstrate that DreamBench++ results in significantly more\nhuman-aligned evaluation, helping boost the community with innovative findings.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16855",
        "PDF Link": "https://arxiv.org/pdf/2406.16855",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
        "Abstract": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16860",
        "PDF Link": "https://arxiv.org/pdf/2406.16860",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions",
        "Abstract": "Automated software engineering has been greatly empowered by the recent\nadvances in Large Language Models (LLMs) for programming. While current\nbenchmarks have shown that LLMs can perform various software engineering tasks\nlike human developers, the majority of their evaluations are limited to short\nand self-contained algorithmic tasks. Solving challenging and practical\nprogramming tasks requires the capability of utilizing diverse function calls\nas tools to efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.\nTo assess how well LLMs can solve challenging and practical programming tasks,\nwe introduce Bench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\nprogramming tasks. To evaluate LLMs rigorously, each programming task\nencompasses 5.6 test cases with an average branch coverage of 99%. In addition,\nwe propose a natural-language-oriented variant of Bench, Benchi, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15877",
        "PDF Link": "https://arxiv.org/pdf/2406.15877",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Evaluating D-MERIT of Partial-annotation on Information Retrieval",
        "Abstract": "Retrieval models are often evaluated on partially-annotated datasets. Each\nquery is mapped to a few relevant texts and the remaining corpus is assumed to\nbe irrelevant. As a result, models that successfully retrieve false negatives\nare punished in evaluation. Unfortunately, completely annotating all texts for\nevery query is not resource efficient. In this work, we show that using\npartially-annotated datasets in evaluation can paint a distorted picture. We\ncurate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to\ncontain all relevant passages for each query. Queries describe a group (e.g.,\n``journals about linguistics'') and relevant passages are evidence that\nentities belong to the group (e.g., a passage indicating that Language is a\njournal about linguistics). We show that evaluating on a dataset containing\nannotations for only a subset of the relevant passages might result in\nmisleading ranking of the retrieval systems and that as more relevant texts are\nincluded in the evaluation set, the rankings converge. We propose our dataset\nas a resource for evaluation and our study as a recommendation for balance\nbetween resource-efficiency and reliable evaluation when annotating evaluation\nsets for text retrieval.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16048",
        "PDF Link": "https://arxiv.org/pdf/2406.16048",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Long Context Transfer from Language to Vision",
        "Abstract": "Video sequences offer valuable temporal information, but existing large\nmultimodal models (LMMs) fall short in understanding extremely long videos.\nMany works address this by reducing the number of visual tokens using visual\nresamplers. Alternatively, in this paper, we approach this problem from the\nperspective of the language model. By simply extrapolating the context length\nof the language backbone, we enable LMMs to comprehend orders of magnitude more\nvisual tokens without any video training. We call this phenomenon long context\ntransfer and carefully ablate its properties. To effectively measure LMMs'\nability to generalize to long contexts in the vision modality, we develop\nV-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark\ninspired by the language model's NIAH test. Our proposed Long Video Assistant\n(LongVA) can process 2000 frames or over 200K visual tokens without additional\ncomplexities. With its extended context length, LongVA achieves\nstate-of-the-art performance on Video-MME among 7B-scale models by densely\nsampling more input frames. Our work is open-sourced at\nhttps://github.com/EvolvingLMMs-Lab/LongVA.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16852",
        "PDF Link": "https://arxiv.org/pdf/2406.16852",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Video-Infinity: Distributed Long Video Generation",
        "Abstract": "Diffusion models have recently achieved remarkable results for video\ngeneration. Despite the encouraging performances, the generated videos are\ntypically constrained to a small number of frames, resulting in clips lasting\nmerely a few seconds. The primary challenges in producing longer videos include\nthe substantial memory requirements and the extended processing time required\non a single GPU. A straightforward solution would be to split the workload\nacross multiple GPUs, which, however, leads to two issues: (1) ensuring all\nGPUs communicate effectively to share timing and context information, and (2)\nmodifying existing video diffusion models, which are usually trained on short\nsequences, to create longer videos without additional training. To tackle\nthese, in this paper we introduce Video-Infinity, a distributed inference\npipeline that enables parallel processing across multiple GPUs for long-form\nvideo generation. Specifically, we propose two coherent mechanisms: Clip\nparallelism and Dual-scope attention. Clip parallelism optimizes the gathering\nand sharing of context information across GPUs which minimizes communication\noverhead, while Dual-scope attention modulates the temporal self-attention to\nbalance local and global contexts efficiently across the devices. Together, the\ntwo mechanisms join forces to distribute the workload and enable the fast\ngeneration of long videos. Under an 8 x Nvidia 6000 Ada GPU (48G) setup, our\nmethod generates videos up to 2,300 frames in approximately 5 minutes, enabling\nlong video generation at a speed 100 times faster than the prior methods.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16260",
        "PDF Link": "https://arxiv.org/pdf/2406.16260",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in\n  Large Video-Language Models",
        "Abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have extended\ntheir capabilities to video understanding. Yet, these models are often plagued\nby \"hallucinations\", where irrelevant or nonsensical content is generated,\ndeviating from the actual video context. This work introduces VideoHallucer,\nthe first comprehensive benchmark for hallucination detection in large\nvideo-language models (LVLMs). VideoHallucer categorizes hallucinations into\ntwo main types: intrinsic and extrinsic, offering further subcategories for\ndetailed analysis, including object-relation, temporal, semantic detail,\nextrinsic factual, and extrinsic non-factual hallucinations. We adopt an\nadversarial binary VideoQA method for comprehensive evaluation, where pairs of\nbasic and hallucinated questions are crafted strategically. By evaluating\neleven LVLMs on VideoHallucer, we reveal that i) the majority of current models\nexhibit significant issues with hallucinations; ii) while scaling datasets and\nparameters improves models' ability to detect basic visual cues and\ncounterfactuals, it provides limited benefit for detecting extrinsic factual\nhallucinations; iii) existing models are more adept at detecting facts than\nidentifying hallucinations. As a byproduct, these analyses further instruct the\ndevelopment of our self-PEP framework, achieving an average of 5.38%\nimprovement in hallucination resistance across all model architectures.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16338",
        "PDF Link": "https://arxiv.org/pdf/2406.16338",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Scaling Laws for Linear Complexity Language Models",
        "Abstract": "The interest in linear complexity models for large language models is on the\nrise, although their scaling capacity remains uncertain. In this study, we\npresent the scaling laws for linear complexity language models to establish a\nfoundation for their scalability. Specifically, we examine the scaling\nbehaviors of three efficient linear architectures. These include TNL, a linear\nattention model with data-independent decay; HGRN2, a linear RNN with\ndata-dependent decay; and cosFormer2, a linear attention model without decay.\nWe also include LLaMA as a baseline architecture for softmax attention for\ncomparison. These models were trained with six variants, ranging from 70M to 7B\nparameters on a 300B-token corpus, and evaluated with a total of 1,376\nintermediate checkpoints on various downstream tasks. These tasks include\nvalidation loss, commonsense reasoning, and information retrieval and\ngeneration. The study reveals that existing linear complexity language models\nexhibit similar scaling capabilities as conventional transformer-based models\nwhile also demonstrating superior linguistic proficiency and knowledge\nretention.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16690",
        "PDF Link": "https://arxiv.org/pdf/2406.16690",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "WARP: On the Benefits of Weight Averaged Rewarded Policies",
        "Abstract": "Reinforcement learning from human feedback (RLHF) aligns large language\nmodels (LLMs) by encouraging their generations to have high rewards, using a\nreward model trained on human preferences. To prevent the forgetting of\npre-trained knowledge, RLHF usually incorporates a KL regularization; this\nforces the policy to remain close to its supervised fine-tuned initialization,\nthough it hinders the reward optimization. To tackle the trade-off between KL\nand reward, in this paper we introduce a novel alignment strategy named Weight\nAveraged Rewarded Policies (WARP). WARP merges policies in the weight space at\nthree distinct stages. First, it uses the exponential moving average of the\npolicy as a dynamic anchor in the KL regularization. Second, it applies\nspherical interpolation to merge independently fine-tuned policies into a new\nenhanced one. Third, it linearly interpolates between this merged model and the\ninitialization, to recover features from pre-training. This procedure is then\napplied iteratively, with each iteration's final model used as an advanced\ninitialization for the next, progressively refining the KL-reward Pareto front,\nachieving superior rewards at fixed KL. Experiments with GEMMA policies\nvalidate that WARP improves their quality and alignment, outperforming other\nopen-source LLMs.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16768",
        "PDF Link": "https://arxiv.org/pdf/2406.16768",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Efficient Continual Pre-training by Mitigating the Stability Gap",
        "Abstract": "Continual pre-training has increasingly become the predominant approach for\nadapting Large Language Models (LLMs) to new domains. This process involves\nupdating the pre-trained LLM with a corpus from a new domain, resulting in a\nshift in the training distribution. To study the behavior of LLMs during this\nshift, we measured the model's performance throughout the continual\npre-training process. we observed a temporary performance drop at the\nbeginning, followed by a recovery phase, a phenomenon known as the \"stability\ngap,\" previously noted in vision models classifying new classes. To address\nthis issue and enhance LLM performance within a fixed compute budget, we\npropose three effective strategies: (1) Continually pre-training the LLM on a\nsubset with a proper size for multiple epochs, resulting in faster performance\nrecovery than pre-training the LLM on a large corpus in a single epoch; (2)\nPre-training the LLM only on high-quality sub-corpus, which rapidly boosts\ndomain performance; and (3) Using a data mixture similar to the pre-training\ndata to reduce distribution gap. We conduct various experiments on Llama-family\nmodels to validate the effectiveness of our strategies in both medical\ncontinual pre-training and instruction tuning. For example, our strategies\nimprove the average medical task performance of the OpenLlama-3B model from\n36.2% to 40.7% with only 40% of the original training budget and enhance the\naverage general task performance without causing forgetting. Furthermore, we\napply our strategies to the Llama-3-8B model. The resulting model,\nLlama-3-Physician, achieves the best medical performance among current\nopen-source models, and performs comparably to or even better than GPT-4 on\nseveral medical benchmarks. We release our models at\nhttps://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14833",
        "PDF Link": "https://arxiv.org/pdf/2406.14833",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and\n  Specialized Drafters",
        "Abstract": "Large language models (LLMs) have revolutionized natural language processing\nand broadened their applicability across diverse commercial applications.\nHowever, the deployment of these models is constrained by high inference time\nin multilingual settings. To mitigate this challenge, this paper explores a\ntraining recipe of an assistant model in speculative decoding, which are\nleveraged to draft and-then its future tokens are verified by the target LLM.\nWe show that language-specific draft models, optimized through a targeted\npretrain-and-finetune strategy, substantially brings a speedup of inference\ntime compared to the previous methods. We validate these models across various\nlanguages in inference time, out-of-domain speedup, and GPT-4o evaluation.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16758",
        "PDF Link": "https://arxiv.org/pdf/2406.16758",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers",
        "Abstract": "Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16747",
        "PDF Link": "https://arxiv.org/pdf/2406.16747",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex\n  Models",
        "Abstract": "As large language models (LLMs) increasingly permeate daily lives, there is a\ngrowing demand for real-time interactions that mirror human conversations.\nTraditional turn-based chat systems driven by LLMs prevent users from verbally\ninteracting with the system while it is generating responses. To overcome these\nlimitations, we adapt existing LLMs to duplex models so that these\nLLMs can listen for users while generating output and dynamically adjust\nthemselves to provide users with instant feedback. % such as in response to\ninterruptions. Specifically, we divide the queries and responses of\nconversations into several time slices and then adopt a\ntime-division-multiplexing (TDM) encoding-decoding strategy to\npseudo-simultaneously process these slices. Furthermore, to make LLMs\nproficient enough to handle real-time conversations, we build a fine-tuning\ndataset consisting of alternating time slices of queries and responses as well\nas covering typical feedback types in instantaneous interactions. Our\nexperiments show that although the queries and responses of conversations are\nsegmented into incomplete slices for processing, LLMs can preserve their\noriginal performance on standard benchmarks with a few fine-tuning steps on our\ndataset. Automatic and human evaluation indicate that duplex models make\nuser-AI interactions more natural and human-like, and greatly improve user\nsatisfaction compared to vanilla LLMs. Our duplex model and dataset will be\nreleased.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15718",
        "PDF Link": "https://arxiv.org/pdf/2406.15718",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in\n  LLMs",
        "Abstract": "We propose semantic entropy probes (SEPs), a cheap and reliable method for\nuncertainty quantification in Large Language Models (LLMs). Hallucinations,\nwhich are plausible-sounding but factually incorrect and arbitrary model\ngenerations, present a major challenge to the practical adoption of LLMs.\nRecent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can\ndetect hallucinations by estimating uncertainty in the space semantic meaning\nfor a set of model generations. However, the 5-to-10-fold increase in\ncomputation cost associated with SE computation hinders practical adoption. To\naddress this, we propose SEPs, which directly approximate SE from the hidden\nstates of a single generation. SEPs are simple to train and do not require\nsampling multiple model generations at test time, reducing the overhead of\nsemantic uncertainty quantification to almost zero. We show that SEPs retain\nhigh performance for hallucination detection and generalize better to\nout-of-distribution data than previous probing methods that directly predict\nmodel accuracy. Our results across models and tasks suggest that model hidden\nstates capture SE, and our ablation studies give further insights into the\ntoken positions and model layers for which this is the case.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15927",
        "PDF Link": "https://arxiv.org/pdf/2406.15927",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
        "Abstract": "Detoxifying multilingual Large Language Models (LLMs) has become crucial due\nto their increasing global use. In this work, we explore zero-shot\ncross-lingual generalization of preference tuning in detoxifying LLMs. Unlike\nprevious studies that show limited cross-lingual generalization for other\nsafety tasks, we demonstrate that Direct Preference Optimization (DPO) training\nwith only English data can significantly reduce toxicity in multilingual\nopen-ended generations. For example, the probability of mGPT-1.3B generating\ntoxic continuations drops from 46.8% to 3.9% across 17 different languages\nafter training. Our results also extend to other multilingual LLMs, such as\nBLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal\nintervention and activation analysis, we identified the dual multilinguality\nproperty of MLP layers in LLMs, which explains the cross-lingual generalization\nof DPO. Finally, we show that bilingual sentence retrieval can predict the\ncross-lingual transferability of DPO preference tuning.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16235",
        "PDF Link": "https://arxiv.org/pdf/2406.16235",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Confidence Regulation Neurons in Language Models",
        "Abstract": "Despite their widespread use, the mechanisms by which large language models\n(LLMs) represent and regulate uncertainty in next-token predictions remain\nlargely unexplored. This study investigates two critical components believed to\ninfluence this uncertainty: the recently discovered entropy neurons and a new\nset of components that we term token frequency neurons. Entropy neurons are\ncharacterized by an unusually high weight norm and influence the final layer\nnormalization (LayerNorm) scale to effectively scale down the logits. Our work\nshows that entropy neurons operate by writing onto an unembedding null space,\nallowing them to impact the residual stream norm with minimal direct effect on\nthe logits themselves. We observe the presence of entropy neurons across a\nrange of models, up to 7 billion parameters. On the other hand, token frequency\nneurons, which we discover and describe here for the first time, boost or\nsuppress each token's logit proportionally to its log frequency, thereby\nshifting the output distribution towards or away from the unigram distribution.\nFinally, we present a detailed case study where entropy neurons actively manage\nconfidence in the setting of induction, i.e. detecting and continuing repeated\nsubsequences.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16254",
        "PDF Link": "https://arxiv.org/pdf/2406.16254",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection\n  in Large Language Models",
        "Abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful,\nthey still exhibit significant but subtle weaknesses, such as mistakes in\ninstruction-following or coding tasks. As these unexpected errors could lead to\nsevere consequences in practical deployments, it is crucial to investigate the\nlimitations within LLMs systematically. Traditional benchmarking approaches\ncannot thoroughly pinpoint specific model deficiencies, while manual\ninspections are costly and not scalable. In this paper, we introduce a unified\nframework, AutoDetect, to automatically expose weaknesses in LLMs across\nvarious tasks. Inspired by the educational assessment process that measures\nstudents' learning outcomes, AutoDetect consists of three LLM-powered agents:\nExaminer, Questioner, and Assessor. The collaboration among these three agents\nis designed to realize comprehensive and in-depth weakness identification. Our\nframework demonstrates significant success in uncovering flaws, with an\nidentification success rate exceeding 30% in prominent models such as ChatGPT\nand Claude. More importantly, these identified weaknesses can guide specific\nmodel improvements, proving more effective than untargeted data augmentation\nmethods like Self-Instruct. Our approach has led to substantial enhancements in\npopular LLMs, including the Llama series and Mistral-7b, boosting their\nperformance by over 10% across several benchmarks. Code and data are publicly\navailable at https://github.com/thu-coai/AutoDetect.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16714",
        "PDF Link": "https://arxiv.org/pdf/2406.16714",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "How Many Parameters Does it Take to Change a Light Bulb? Evaluating\n  Performance in Self-Play of Conversational Games as a Function of Model\n  Characteristics",
        "Abstract": "What makes a good Large Language Model (LLM)? That it performs well on the\nrelevant benchmarks -- which hopefully measure, with some validity, the\npresence of capabilities that are also challenged in real application. But what\nmakes the model perform well? What gives a model its abilities? We take a\nrecently introduced type of benchmark that is meant to challenge capabilities\nin a goal-directed, agentive context through self-play of conversational games,\nand analyse how performance develops as a function of model characteristics\nlike number of parameters, or type of training. We find that while there is a\nclear relationship between number of parameters and performance, there is still\na wide spread of performance points within a given size bracket, which is to be\naccounted for by training parameters such as fine-tuning data quality and\nmethod. From a more practical angle, we also find a certain degree of\nunpredictability about performance across access methods, possible due to\nunexposed sampling parameters, and a, very welcome, performance stability\nagainst at least moderate weight quantisation during inference.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14051",
        "PDF Link": "https://arxiv.org/pdf/2406.14051",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians",
        "Abstract": "High-fidelity 3D garment synthesis from text is desirable yet challenging for\ndigital avatar creation. Recent diffusion-based approaches via Score\nDistillation Sampling (SDS) have enabled new possibilities but either\nintricately couple with human body or struggle to reuse. We introduce\nClotheDreamer, a 3D Gaussian-based method for generating wearable,\nproduction-ready 3D garment assets from text prompts. We propose a novel\nrepresentation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate\noptimization. DCGS represents clothed avatar as one Gaussian model but freezes\nbody Gaussian splats. To enhance quality and completeness, we incorporate\nbidirectional SDS to supervise clothed avatar and garment RGBD renderings\nrespectively with pose conditions and propose a new pruning strategy for loose\nclothing. Our approach can also support custom clothing templates as input.\nBenefiting from our design, the synthetic 3D garment can be easily applied to\nvirtual try-on and support physically accurate animation. Extensive experiments\nshowcase our method's superior and competitive performance. Our project page is\nat https://ggxxii.github.io/clothedreamer.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16815",
        "PDF Link": "https://arxiv.org/pdf/2406.16815",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Found in the Middle: Calibrating Positional Attention Bias Improves Long\n  Context Utilization",
        "Abstract": "Large language models (LLMs), even when specifically trained to process long\ninput contexts, struggle to capture relevant information located in the middle\nof their input. This phenomenon has been known as the lost-in-the-middle\nproblem. In this work, we make three contributions. First, we set out to\nunderstand the factors that cause this phenomenon. In doing so, we establish a\nconnection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs\nexhibit a U-shaped attention bias where the tokens at the beginning and at the\nend of its input receive higher attention, regardless of their relevance.\nSecond, we mitigate this positional bias through a calibration mechanism,\nfound-in-the-middle, that allows the model to attend to contexts faithfully\naccording to their relevance, even though when they are in the middle. Third,\nwe show found-in-the-middle not only achieves better performance in locating\nrelevant information within a long context, but also eventually leads to\nimproved retrieval-augmented generation (RAG) performance across various tasks,\noutperforming existing methods by up to 15 percentage points. These findings\nopen up future directions in understanding LLM attention bias and its potential\nconsequences.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16008",
        "PDF Link": "https://arxiv.org/pdf/2406.16008",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "IRASim: Learning Interactive Real-Robot Action Simulators",
        "Abstract": "Scalable robot learning in the real world is limited by the cost and safety\nissues of real robots. In addition, rolling out robot trajectories in the real\nworld can be time-consuming and labor-intensive. In this paper, we propose to\nlearn an interactive real-robot action simulator as an alternative. We\nintroduce a novel method, IRASim, which leverages the power of generative\nmodels to generate extremely realistic videos of a robot arm that executes a\ngiven action trajectory, starting from an initial given frame. To validate the\neffectiveness of our method, we create a new benchmark, IRASim Benchmark, based\non three real-robot datasets and perform extensive experiments on the\nbenchmark. Results show that IRASim outperforms all the baseline methods and is\nmore preferable in human evaluations. We hope that IRASim can serve as an\neffective and scalable approach to enhance robot learning in the real world. To\npromote research for generative real-robot action simulators, we open-source\ncode, benchmark, and checkpoints at https: //gen-irasim.github.io.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14540",
        "PDF Link": "https://arxiv.org/pdf/2406.14540",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations",
        "Abstract": "Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, naively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+23\\% on average across models) on various QA datasets with long\ncontext, especially when the answer lies within the middle of the context.\nSurprisingly, despite introducing only single-hop ICL examples, LLMs also\nsuccessfully generalize to multi-hop long-context QA using our approach.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13632",
        "PDF Link": "https://arxiv.org/pdf/2406.13632",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models",
        "Abstract": "Speech understanding as an element of the more generic video understanding\nusing audio-visual large language models (av-LLMs) is a crucial yet\nunderstudied aspect. This paper proposes video-SALMONN, a single end-to-end\nav-LLM for video processing, which can understand not only visual frame\nsequences, audio events and music, but speech as well. To obtain fine-grained\ntemporal information required by speech understanding, while keeping efficient\nfor other video elements, this paper proposes a novel multi-resolution causal\nQ-Former (MRC Q-Former) structure to connect pre-trained audio-visual encoders\nand the backbone large language model. Moreover, dedicated training approaches\nincluding the diversity loss and the unpaired audio-visual mixed training\nscheme are proposed to avoid frames or modality dominance. On the introduced\nspeech-audio-visual evaluation benchmark, video-SALMONN achieves more than 25\\%\nabsolute accuracy improvements on the video-QA task and over 30\\% absolute\naccuracy improvements on audio-visual QA tasks with human speech. In addition,\nvideo-SALMONN demonstrates remarkable video comprehension and reasoning\nabilities on tasks that are unprecedented by other av-LLMs. Our training code\nand model checkpoints are available at\n\\url{https://github.com/bytedance/SALMONN/}.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15704",
        "PDF Link": "https://arxiv.org/pdf/2406.15704",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "Repulsive Score Distillation for Diverse Sampling of Diffusion Models",
        "Abstract": "Score distillation sampling has been pivotal for integrating diffusion models\ninto generation of complex visuals. Despite impressive results it suffers from\nmode collapse and lack of diversity. To cope with this challenge, we leverage\nthe gradient flow interpretation of score distillation to propose Repulsive\nScore Distillation (RSD). In particular, we propose a variational framework\nbased on repulsion of an ensemble of particles that promotes diversity. Using a\nvariational approximation that incorporates a coupling among particles, the\nrepulsion appears as a simple regularization that allows interaction of\nparticles based on their relative pairwise similarity, measured e.g., via\nradial basis kernels. We design RSD for both unconstrained and constrained\nsampling scenarios. For constrained sampling we focus on inverse problems in\nthe latent space that leads to an augmented variational formulation, that\nstrikes a good balance between compute, quality and diversity. Our extensive\nexperiments for text-to-image generation, and inverse problems demonstrate that\nRSD achieves a superior trade-off between diversity and quality compared with\nstate-of-the-art alternatives.",
        "ArXiv Link": "https://arxiv.org/abs/2406.16683",
        "PDF Link": "https://arxiv.org/pdf/2406.16683",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?",
        "Abstract": "In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).",
        "ArXiv Link": "https://arxiv.org/abs/2406.16772",
        "PDF Link": "https://arxiv.org/pdf/2406.16772",
        "Upvotes": "52",
        "Date": "2024-07-07"
    },
    {
        "Title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
        "Abstract": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to extract answers\nfrom the short retrieved units. Such an imbalanced `heavy' retriever and\n`light' reader design can lead to sub-optimal performance. In order to\nalleviate the imbalance, we propose a new framework LongRAG, consisting of a\n`long retriever' and a `long reader'. LongRAG processes the entire Wikipedia\ninto 4K-token units, which is 30x longer than before. By increasing the unit\nsize, we significantly reduce the total units from 22M to 700K. This\nsignificantly lowers the burden of retriever, which leads to a remarkable\nretrieval score: answer recall@1=71% on NQ (previously 52%) and answer\nrecall@2=72% (previously 47%) on HotpotQA (full-wiki). Then we feed the top-k\nretrieved units (approx 30K tokens) to an existing long-context LLM to\nperform zero-shot answer extraction. Without requiring any training, LongRAG\nachieves an EM of 62.7% on NQ, which is the best known result. LongRAG also\nachieves 64.3% on HotpotQA (full-wiki), which is on par of the SoTA model. Our\nstudy offers insights into the future roadmap for combining RAG with\nlong-context LLMs.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15319",
        "PDF Link": "https://arxiv.org/pdf/2406.15319",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges",
        "Abstract": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges. We leverage TriviaQA\nas a benchmark for assessing objective knowledge reasoning of LLMs and evaluate\nthem alongside human annotations which we found to have a high inter-annotator\nagreement. Our study includes 9 judge models and 9 exam taker models -- both\nbase and instruction-tuned. We assess the judge model's alignment across\ndifferent model sizes, families, and judge prompts. Among other results, our\nresearch rediscovers the importance of using Cohen's kappa as a metric of\nalignment as opposed to simple percent agreement, showing that judges with high\npercent agreement can still assign vastly different scores. We find that both\nLlama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but in\nterms of ranking exam taker models, they are outperformed by both JudgeLM-7B\nand the lexical judge Contains, which have up to 34 points lower human\nalignment. Through error analysis and various other studies, including the\neffects of instruction length and leniency bias, we hope to provide valuable\nlessons for using LLMs as judges in the future.",
        "ArXiv Link": "https://arxiv.org/abs/2406.12624",
        "PDF Link": "https://arxiv.org/pdf/2406.12624",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Complexity of Symbolic Representation in Working Memory of Transformer\n  Correlates with the Complexity of a Task",
        "Abstract": "Even though Transformers are extensively used for Natural Language Processing\ntasks, especially for machine translation, they lack an explicit memory to\nstore key concepts of processed texts. This paper explores the properties of\nthe content of symbolic working memory added to the Transformer model decoder.\nSuch working memory enhances the quality of model predictions in machine\ntranslation task and works as a neural-symbolic representation of information\nthat is important for the model to make correct translations. The study of\nmemory content revealed that translated text keywords are stored in the working\nmemory, pointing to the relevance of memory content to the processed text.\nAlso, the diversity of tokens and parts of speech stored in memory correlates\nwith the complexity of the corpora for machine translation task.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14213",
        "PDF Link": "https://arxiv.org/pdf/2406.14213",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Towards Retrieval Augmented Generation over Large Video Libraries",
        "Abstract": "Video content creators need efficient tools to repurpose content, a task that\noften requires complex manual or automated searches. Crafting a new video from\nlarge video libraries remains a challenge. In this paper we introduce the task\nof Video Library Question Answering (VLQA) through an interoperable\narchitecture that applies Retrieval Augmented Generation (RAG) to video\nlibraries. We propose a system that uses large language models (LLMs) to\ngenerate search queries, retrieving relevant video moments indexed by speech\nand visual metadata. An answer generation module then integrates user queries\nwith this metadata to produce responses with specific video timestamps. This\napproach shows promise in multimedia content retrieval, and AI-assisted video\ncontent creation.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14938",
        "PDF Link": "https://arxiv.org/pdf/2406.14938",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Stylebreeder: Exploring and Democratizing Artistic Styles through\n  Text-to-Image Models",
        "Abstract": "Text-to-image models are becoming increasingly popular, revolutionizing the\nlandscape of digital art creation by enabling highly detailed and creative\nvisual content generation. These models have been widely employed across\nvarious domains, particularly in art generation, where they facilitate a broad\nspectrum of creative expression and democratize access to artistic creation. In\nthis paper, we introduce STYLEBREEDER, a comprehensive dataset of 6.8M\nimages and 1.8M prompts generated by 95K users on Artbreeder, a platform that\nhas emerged as a significant hub for creative exploration with over 13M users.\nWe introduce a series of tasks with this dataset aimed at identifying diverse\nartistic styles, generating personalized content, and recommending styles based\non user interests. By documenting unique, user-generated styles that transcend\nconventional categories like 'cyberpunk' or 'Picasso,' we explore the potential\nfor unique, crowd-sourced styles that could provide deep insights into the\ncollective creative psyche of users worldwide. We also evaluate different\npersonalization methods to enhance artistic expression and introduce a style\natlas, making these models available in LoRA format for public use. Our\nresearch demonstrates the potential of text-to-image diffusion models to\nuncover and promote unique artistic expressions, further democratizing AI in\nart and fostering a more diverse and inclusive artistic community. The dataset,\ncode and models are available at https://stylebreeder.github.io under a Public\nDomain (CC0) license.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14599",
        "PDF Link": "https://arxiv.org/pdf/2406.14599",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework",
        "Abstract": "Challenges in the automated evaluation of Retrieval-Augmented Generation\n(RAG) Question-Answering (QA) systems include hallucination problems in\ndomain-specific knowledge and the lack of gold standard benchmarks for company\ninternal tasks. This results in difficulties in evaluating RAG variations, like\nRAG-Fusion (RAGF), in the context of a product QA task at Infineon\nTechnologies. To solve these problems, we propose a comprehensive evaluation\nframework, which leverages Large Language Models (LLMs) to generate large\ndatasets of synthetic queries based on real user queries and in-domain\ndocuments, uses LLM-as-a-judge to rate retrieved documents and answers,\nevaluates the quality of answers, and ranks different variants of\nRetrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based\ncompetition. LLM-as-a-judge rating of a random sample of synthetic queries\nshows a moderate, positive correlation with domain expert scoring in relevance,\naccuracy, completeness, and precision. While RAGF outperformed RAG in Elo\nscore, a significance analysis against expert annotations also shows that RAGF\nsignificantly outperforms RAG in completeness, but underperforms in precision.\nIn addition, Infineon's RAGF assistant demonstrated slightly higher performance\nin document relevance based on MRR@5 scores. We find that RAGElo positively\naligns with the preferences of human annotators, though due caution is still\nrequired. Finally, RAGF's approach leads to more complete answers based on\nexpert annotations and better answers overall based on RAGElo's evaluation\ncriteria.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14783",
        "PDF Link": "https://arxiv.org/pdf/2406.14783",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "MantisScore: Building Automatic Metrics to Simulate Fine-grained Human\n  Feedback for Video Generation",
        "Abstract": "The recent years have witnessed great advances in video generation. However,\nthe development of automatic video metrics is lagging significantly behind.\nNone of the existing metric is able to provide reliable scores over generated\nvideos. The main barrier is the lack of large-scale human-annotated dataset. In\nthis paper, we release VideoFeedback, the first large-scale dataset containing\nhuman-provided multi-aspect score over 37.6K synthesized videos from 11\nexisting video generative models. We train MantisScore (initialized from\nMantis) based on VideoFeedback to enable automatic video quality assessment.\nExperiments show that the Spearman correlation between MantisScore and humans\ncan reach 77.1 on VideoFeedback-test, beating the prior best metrics by about\n50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and\nVBench show that MantisScore has consistently much higher correlation with\nhuman judges than other metrics. Due to these results, we believe MantisScore\ncan serve as a great proxy for human raters to (1) rate different video models\nto track progress (2) simulate fine-grained human feedback in Reinforcement\nLearning with Human Feedback (RLHF) to improve current video generation models.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15252",
        "PDF Link": "https://arxiv.org/pdf/2406.15252",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "EvTexture: Event-driven Texture Enhancement for Video Super-Resolution",
        "Abstract": "Event-based vision has drawn increasing attention due to its unique\ncharacteristics, such as high temporal resolution and high dynamic range. It\nhas been used in video super-resolution (VSR) recently to enhance the flow\nestimation and temporal alignment. Rather than for motion learning, we propose\nin this paper the first VSR method that utilizes event signals for texture\nenhancement. Our method, called EvTexture, leverages high-frequency details of\nevents to better recover texture regions in VSR. In our EvTexture, a new\ntexture enhancement branch is presented. We further introduce an iterative\ntexture enhancement module to progressively explore the\nhigh-temporal-resolution event information for texture restoration. This allows\nfor gradual refinement of texture regions across multiple iterations, leading\nto more accurate and rich high-resolution details. Experimental results show\nthat our EvTexture achieves state-of-the-art performance on four datasets. For\nthe Vid4 dataset with rich textures, our method can get up to 4.67dB gain\ncompared with recent event-based methods. Code:\nhttps://github.com/DachunKai/EvTexture.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13457",
        "PDF Link": "https://arxiv.org/pdf/2406.13457",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Reward Steering with Evolutionary Heuristics for Decoding-time Alignment",
        "Abstract": "The widespread applicability and increasing omnipresence of LLMs have\ninstigated a need to align LLM responses to user and stakeholder preferences.\nMany preference optimization approaches have been proposed that fine-tune LLM\nparameters to achieve good alignment. However, such parameter tuning is known\nto interfere with model performance on many tasks. Moreover, keeping up with\nshifting user preferences is tricky in such a situation. Decoding-time\nalignment with reward model guidance solves these issues at the cost of\nincreased inference time. However, most of such methods fail to strike the\nright balance between exploration and exploitation of reward -- often due to\nthe conflated formulation of these two aspects - to give well-aligned\nresponses. To remedy this we decouple these two aspects and implement them in\nan evolutionary fashion: exploration is enforced by decoding from mutated\ninstructions and exploitation is represented as the periodic replacement of\npoorly-rewarded generations with well-rewarded ones. Empirical evidences\nindicate that this strategy outperforms many preference optimization and\ndecode-time alignment approaches on two widely accepted alignment benchmarks\nAlpacaEval 2 and MT-Bench. Our implementation will be available at:\nhttps://darwin-alignment.github.io.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15193",
        "PDF Link": "https://arxiv.org/pdf/2406.15193",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Jailbreaking as a Reward Misspecification Problem",
        "Abstract": "The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. We introduce a metric ReGap to quantify the extent of reward\nmisspecification and demonstrate its effectiveness and robustness in detecting\nharmful backdoor prompts. Building upon these insights, we present ReMiss, a\nsystem for automated red teaming that generates adversarial prompts against\nvarious target aligned LLMs. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark while preserving the human readability of the\ngenerated prompts. Detailed analysis highlights the unique advantages brought\nby the proposed reward misspecification objective compared to previous methods.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14393",
        "PDF Link": "https://arxiv.org/pdf/2406.14393",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Cognitive Map for Language Models: Optimal Planning via Verbally\n  Representing the World Model",
        "Abstract": "Language models have demonstrated impressive capabilities across various\nnatural language processing tasks, yet they struggle with planning tasks\nrequiring multi-step simulations. Inspired by human cognitive processes, this\npaper investigates the optimal planning power of language models that can\nconstruct a cognitive map of a given environment. Our experiments demonstrate\nthat cognitive map significantly enhances the performance of both optimal and\nreachable planning generation ability in the Gridworld path planning task. We\nobserve that our method showcases two key characteristics similar to human\ncognition: generalization of its planning ability to extrapolated\nenvironments and rapid adaptation with limited training data. We hope our\nfindings in the Gridworld task provide insights into modeling human cognitive\nprocesses in language models, potentially leading to the development of more\nadvanced and robust systems that better resemble human cognition.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15275",
        "PDF Link": "https://arxiv.org/pdf/2406.15275",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Two Giraffes in a Dirt Field: Using Game Play to Investigate Situation\n  Modelling in Large Multimodal Models",
        "Abstract": "While the situation has improved for text-only models, it again seems to be\nthe case currently that multimodal (text and image) models develop faster than\nways to evaluate them. In this paper, we bring a recently developed evaluation\nparadigm from text models to multimodal models, namely evaluation through the\ngoal-oriented game (self) play, complementing reference-based and\npreference-based evaluation. Specifically, we define games that challenge a\nmodel's capability to represent a situation from visual information and align\nsuch representations through dialogue. We find that the largest closed models\nperform rather well on the games that we define, while even the best\nopen-weight models struggle with them. On further analysis, we find that the\nexceptional deep captioning capabilities of the largest models drive some of\nthe performance. There is still room to grow for both kinds of models, ensuring\nthe continued relevance of the benchmark.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14035",
        "PDF Link": "https://arxiv.org/pdf/2406.14035",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Data Contamination Can Cross Language Barriers",
        "Abstract": "The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be not even wrong, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from https://github.com/ShangDataLab/Deep-Contam.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13236",
        "PDF Link": "https://arxiv.org/pdf/2406.13236",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "DELLA-Merging: Reducing Interference in Model Merging through\n  Magnitude-Based Sampling",
        "Abstract": "With the proliferation of domain-specific models, model merging has emerged\nas a set of techniques that combine the capabilities of multiple models into\none that can multitask without the cost of additional training. In this paper,\nwe propose a new model merging technique, Drop and rEscaLe via sampLing with\nmAgnitude (DELLA-Merging), that employs a novel pruning technique, MAGPRUNE,\nwhich shows significant advantages over DARE and TIES. MAGPRUNE first ranks the\nparameters in order of their magnitude and assigns higher dropout probabilities\n(p) to parameters with lower ranks corresponding to lower magnitudes. To\napproximate the original embeddings, MAGPRUNE employs a rescaling operation on\nthe parameters that survive the random dropping by 1/(1 - p). On three\ndifferent expert models considered for merging (LM, Math, Code) and\ncorresponding benchmark datasets (AlpacaEval, GSM8K, MBPP), DELLA shows an\naverage improvement of 2.4 points over baseline methods employing delta\nparameter pruning (an improvement of 3.6 points over TIES, 1.2 points over\nDARE), and 11.1 points over the no-pruning baseline (TA). We release the source\ncode at: https://github.com/declare-lab/della.",
        "ArXiv Link": "https://arxiv.org/abs/2406.11617",
        "PDF Link": "https://arxiv.org/pdf/2406.11617",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Ruby Teaming: Improving Quality Diversity Search with Memory for\n  Automated Red Teaming",
        "Abstract": "We propose Ruby Teaming, a method that improves on Rainbow Teaming by\nincluding a memory cache as its third dimension. The memory dimension provides\ncues to the mutator to yield better-quality prompts, both in terms of attack\nsuccess rate (ASR) and quality diversity. The prompt archive generated by Ruby\nTeaming has an ASR of 74%, which is 20% higher than the baseline. In terms of\nquality diversity, Ruby Teaming outperforms Rainbow Teaming by 6% and 3% on\nShannon's Evenness Index (SEI) and Simpson's Diversity Index (SDI),\nrespectively.",
        "ArXiv Link": "https://arxiv.org/abs/2406.11654",
        "PDF Link": "https://arxiv.org/pdf/2406.11654",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "4K4DGen: Panoramic 4D Generation at 4K Resolution",
        "Abstract": "The blooming of virtual reality and augmented reality (VR/AR) technologies\nhas driven an increasing demand for the creation of high-quality, immersive,\nand dynamic environments. However, existing generative techniques either focus\nsolely on dynamic objects or perform outpainting from a single perspective\nimage, failing to meet the needs of VR/AR applications. In this work, we tackle\nthe challenging task of elevating a single panorama to an immersive 4D\nexperience. For the first time, we demonstrate the capability to generate\nomnidirectional dynamic scenes with 360-degree views at 4K resolution, thereby\nproviding an immersive user experience. Our method introduces a pipeline that\nfacilitates natural scene animations and optimizes a set of 4D Gaussians using\nefficient splatting techniques for real-time exploration. To overcome the lack\nof scene-scale annotated 4D data and models, especially in panoramic formats,\nwe propose a novel Panoramic Denoiser that adapts generic 2D diffusion priors\nto animate consistently in 360-degree images, transforming them into panoramic\nvideos with dynamic scenes at targeted regions. Subsequently, we elevate the\npanoramic video into a 4D immersive environment while preserving spatial and\ntemporal consistency. By transferring prior knowledge from 2D models in the\nperspective domain to the panoramic domain and the 4D lifting with spatial\nappearance and geometry regularization, we achieve high-quality Panorama-to-4D\ngeneration at a resolution of (4096 times 2048) for the first time. See the\nproject website at https://4k4dgen.github.io.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13527",
        "PDF Link": "https://arxiv.org/pdf/2406.13527",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
        "Abstract": "Retrieval Augmented Generation (RAG) represents a significant advancement in\nartificial intelligence combining a retrieval phase with a generative phase,\nwith the latter typically being powered by large language models (LLMs). The\ncurrent common practices in RAG involve using \"instructed\" LLMs, which are\nfine-tuned with supervised training to enhance their ability to follow\ninstructions and are aligned with human preferences using state-of-the-art\ntechniques. Contrary to popular belief, our study demonstrates that base models\noutperform their instructed counterparts in RAG tasks by 20% on average under\nour experimental settings. This finding challenges the prevailing assumptions\nabout the superiority of instructed LLMs in RAG applications. Further\ninvestigations reveal a more nuanced situation, questioning fundamental aspects\nof RAG and suggesting the need for broader discussions on the topic; or, as\nFromm would have it, \"Seldom is a glance at the statistics enough to understand\nthe meaning of the figures\".",
        "ArXiv Link": "https://arxiv.org/abs/2406.14972",
        "PDF Link": "https://arxiv.org/pdf/2406.14972",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Learning Molecular Representation in a Cell",
        "Abstract": "Predicting drug efficacy and safety in vivo requires information on\nbiological responses (e.g., cell morphology and gene expression) to small\nmolecule perturbations. However, current molecular representation learning\nmethods do not provide a comprehensive view of cell states under these\nperturbations and struggle to remove noise, hindering model generalization. We\nintroduce the Information Alignment (InfoAlign) approach to learn molecular\nrepresentations through the information bottleneck method in cells. We\nintegrate molecules and cellular response data as nodes into a context graph,\nconnecting them with weighted edges based on chemical, biological, and\ncomputational criteria. For each molecule in a training batch, InfoAlign\noptimizes the encoder's latent representation with a minimality objective to\ndiscard redundant structural information. A sufficiency objective decodes the\nrepresentation to align with different feature spaces from the molecule's\nneighborhood in the context graph. We demonstrate that the proposed sufficiency\nobjective for alignment is tighter than existing encoder-based contrastive\nmethods. Empirically, we validate representations from InfoAlign in two\ndownstream tasks: molecular property prediction against up to 19 baseline\nmethods across four datasets, plus zero-shot molecule-morphology matching.",
        "ArXiv Link": "https://arxiv.org/abs/2406.12056",
        "PDF Link": "https://arxiv.org/pdf/2406.12056",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Style-NeRF2NeRF: 3D Style Transfer From Style-Aligned Multi-View Images",
        "Abstract": "We propose a simple yet effective pipeline for stylizing a 3D scene,\nharnessing the power of 2D image diffusion models. Given a NeRF model\nreconstructed from a set of multi-view images, we perform 3D style transfer by\nrefining the source NeRF model using stylized images generated by a\nstyle-aligned image-to-image diffusion model. Given a target style prompt, we\nfirst generate perceptually similar multi-view images by leveraging a\ndepth-conditioned diffusion model with an attention-sharing mechanism. Next,\nbased on the stylized multi-view images, we propose to guide the style transfer\nprocess with the sliced Wasserstein loss based on the feature maps extracted\nfrom a pre-trained CNN model. Our pipeline consists of decoupled steps,\nallowing users to test various prompt ideas and preview the stylized 3D result\nbefore proceeding to the NeRF fine-tuning stage. We demonstrate that our method\ncan transfer diverse artistic styles to real-world 3D scenes with competitive\nquality.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13393",
        "PDF Link": "https://arxiv.org/pdf/2406.13393",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking",
        "Abstract": "Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.",
        "ArXiv Link": "https://arxiv.org/abs/2406.15349",
        "PDF Link": "https://arxiv.org/pdf/2406.15349",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "ICAL: Continual Learning of Multimodal Agents by Transforming\n  Trajectories into Actionable Insights",
        "Abstract": "Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations to be\nincluded in their context window. In this work, we ask: Can LLMs and VLMs\ngenerate their own prompt examples from generic, sub-optimal demonstrations? We\npropose In-Context Abstraction Learning (ICAL), a method that builds a memory\nof multimodal experience insights from sub-optimal demonstrations and human\nfeedback. Given a noisy demonstration in a new domain, VLMs abstract the\ntrajectory into a general program by fixing inefficient actions and annotating\ncognitive abstractions: task relationships, object state changes, temporal\nsubgoals, and task construals. These abstractions are refined and adapted\ninteractively through human feedback while the agent attempts to execute the\ntrajectory in a similar environment. The resulting abstractions, when used as\nexemplars in the prompt, significantly improve decision-making in\nretrieval-augmented LLM and VLM agents. Our ICAL agent surpasses the\nstate-of-the-art in dialogue-based instruction following in TEACh, multimodal\nweb agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we\nachieve a 12.6% improvement in goal-condition success. In VisualWebArena, our\ntask success rate improves over the SOTA from 14.3% to 22.7%. In Ego4D action\nforecasting, we improve over few-shot GPT-4V and remain competitive with\nsupervised models. We show finetuning our retrieval-augmented in-context agent\nyields additional improvements. Our approach significantly reduces reliance on\nexpert-crafted examples and consistently outperforms in-context learning from\naction plans that lack such insights.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14596",
        "PDF Link": "https://arxiv.org/pdf/2406.14596",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "RE-AdaptIR: Improving Information Retrieval through Reverse Engineered\n  Adaptation",
        "Abstract": "Large language models (LLMs) fine-tuned for text-retrieval have demonstrated\nstate-of-the-art results across several information retrieval (IR) benchmarks.\nHowever, supervised training for improving these models requires numerous\nlabeled examples, which are generally unavailable or expensive to acquire. In\nthis work, we explore the effectiveness of extending reverse engineered\nadaptation to the context of information retrieval (RE-AdaptIR). We use\nRE-AdaptIR to improve LLM-based IR models using only unlabeled data. We\ndemonstrate improved performance both in training domains as well as zero-shot\nin domains where the models have seen no queries. We analyze performance\nchanges in various fine-tuning scenarios and offer findings of immediate use to\npractitioners.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14764",
        "PDF Link": "https://arxiv.org/pdf/2406.14764",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical\n  Report",
        "Abstract": "Multimodal Foundation Models (MMFMs) have shown remarkable performance on\nvarious computer vision and natural language processing tasks. However, their\nperformance on particular tasks such as document understanding is still\nlimited. They also require more compute, time, and engineering resources to\nfinetune and deploy compared to traditional, unimodal models. In this report,\nwe present Multimodal Structured Generation, a general framework which\nconstrains the output logits of frozen MMFMs to force them to reason before\nresponding with structured outputs that downstream APIs can parse and use. We\nprovide a detailed account of our approach, including the technical details,\ntheoretical discussions, and final evaluation results in the 2nd Multimodal\nFoundation Models Challenge hosted by the Computer Vision and Pattern\nRecognition (CVPR) conference. Our approach achieved the second highest score\nin the hidden test set for Phase 2 and third highest overall. This shows the\nmethod's ability to generalize to unseen tasks. And that simple engineering can\nbeat expensive & complicated modelling steps as we first discussed in our\npaper, Retrieval Augmented Structured Generation: Business Document Information\nExtraction as Tool Use. All of our scripts, deployment steps, and evaluation\nresults can be accessed in https://github.com/leloykun/MMFM-Challenge",
        "ArXiv Link": "https://arxiv.org/abs/2406.11403",
        "PDF Link": "https://arxiv.org/pdf/2406.11403",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "Low-Resource Machine Translation through the Lens of Personalized\n  Federated Learning",
        "Abstract": "We present a new approach based on the Personalized Federated Learning\nalgorithm MeritFed that can be applied to Natural Language Tasks with\nheterogeneous data. We evaluate it on the Low-Resource Machine Translation\ntask, using the dataset from the Large-Scale Multilingual Machine Translation\nShared Task (Small Track #2) and the subset of Sami languages from the\nmultilingual benchmark for Finno-Ugric languages. In addition to its\neffectiveness, MeritFed is also highly interpretable, as it can be applied to\ntrack the impact of each language used for training. Our analysis reveals that\ntarget dataset size affects weight distribution across auxiliary languages,\nthat unrelated languages do not interfere with the training, and auxiliary\noptimizer parameters have minimal impact. Our approach is easy to apply with a\nfew lines of code, and we provide scripts for reproducing the experiments at\nhttps://github.com/VityaVitalich/MeritFed",
        "ArXiv Link": "https://arxiv.org/abs/2406.12564",
        "PDF Link": "https://arxiv.org/pdf/2406.12564",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "ToVo: Toxicity Taxonomy via Voting",
        "Abstract": "Existing toxic detection models face significant limitations, such as lack of\ntransparency, customization, and reproducibility. These challenges stem from\nthe closed-source nature of their training data and the paucity of explanations\nfor their evaluation mechanism. To address these issues, we propose a dataset\ncreation mechanism that integrates voting and chain-of-thought processes,\nproducing a high-quality open-source dataset for toxic content detection. Our\nmethodology ensures diverse classification metrics for each sample and includes\nboth classification scores and explanatory reasoning for the classifications.\n  We utilize the dataset created through our proposed mechanism to train our\nmodel, which is then compared against existing widely-used detectors. Our\napproach not only enhances transparency and customizability but also\nfacilitates better fine-tuning for specific use cases. This work contributes a\nrobust framework for developing toxic content detection models, emphasizing\nopenness and adaptability, thus paving the way for more effective and\nuser-specific content moderation solutions.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14835",
        "PDF Link": "https://arxiv.org/pdf/2406.14835",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of\n  LLM Responses Based on Hofstede Cultural Dimensions",
        "Abstract": "Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14805",
        "PDF Link": "https://arxiv.org/pdf/2406.14805",
        "Upvotes": "54",
        "Date": "2024-07-07"
    },
    {
        "Title": "nabla^2DFT: A Universal Quantum Chemistry Dataset of Drug-Like\n  Molecules and a Benchmark for Neural Network Potentials",
        "Abstract": "Methods of computational quantum chemistry provide accurate approximations of\nmolecular properties crucial for computer-aided drug discovery and other areas\nof chemical science. However, high computational complexity limits the\nscalability of their applications. Neural network potentials (NNPs) are a\npromising alternative to quantum chemistry methods, but they require large and\ndiverse datasets for training. This work presents a new dataset and benchmark\ncalled nabla^2DFT that is based on the nablaDFT. It contains twice as much\nmolecular structures, three times more conformations, new data types and tasks,\nand state-of-the-art models. The dataset includes energies, forces, 17\nmolecular properties, Hamiltonian and overlap matrices, and a wavefunction\nobject. All calculations were performed at the DFT level\n(omegaB97X-D/def2-SVP) for each conformation. Moreover, nabla^2DFT is the\nfirst dataset that contains relaxation trajectories for a substantial number of\ndrug-like molecules. We also introduce a novel benchmark for evaluating NNPs in\nmolecular property prediction, Hamiltonian prediction, and conformational\noptimization tasks. Finally, we propose an extendable framework for training\nNNPs and implement 10 models within it.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14347",
        "PDF Link": "https://arxiv.org/pdf/2406.14347",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Instruction Pre-Training: Language Models are Supervised Multitask\n  Learners",
        "Abstract": "Unsupervised multitask pre-training has been the critical method behind the\nrecent success of language models (LMs). However, supervised multitask learning\nstill holds significant promise, as scaling it in the post-training stage\ntrends towards better generalization. In this paper, we explore supervised\nmultitask pre-training by proposing Instruction Pre-Training, a framework that\nscalably augments massive raw corpora with instruction-response pairs to\npre-train LMs. The instruction-response pairs are generated by an efficient\ninstruction synthesizer built on open-source models. In our experiments, we\nsynthesize 200M instruction-response pairs covering 40+ task categories to\nverify the effectiveness of Instruction Pre-Training. In pre-training from\nscratch, Instruction Pre-Training not only consistently enhances pre-trained\nbase models but also benefits more from further instruction tuning. In\ncontinual pre-training, Instruction Pre-Training enables Llama3-8B to be\ncomparable to or even outperform Llama3-70B. Our model, code, and data are\navailable at https://github.com/microsoft/LMOps.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14491",
        "PDF Link": "https://arxiv.org/pdf/2406.14491",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN\n  Inversion and High Quality Image Editing",
        "Abstract": "The task of manipulating real image attributes through StyleGAN inversion has\nbeen extensively researched. This process involves searching latent variables\nfrom a well-trained StyleGAN generator that can synthesize a real image,\nmodifying these latent variables, and then synthesizing an image with the\ndesired edits. A balance must be struck between the quality of the\nreconstruction and the ability to edit. Earlier studies utilized the\nlow-dimensional W-space for latent search, which facilitated effective editing\nbut struggled with reconstructing intricate details. More recent research has\nturned to the high-dimensional feature space F, which successfully inverses the\ninput image but loses much of the detail during editing. In this paper, we\nintroduce StyleFeatureEditor -- a novel method that enables editing in both\nw-latents and F-latents. This technique not only allows for the reconstruction\nof finer image details but also ensures their preservation during editing. We\nalso present a new training pipeline specifically designed to train our model\nto accurately edit F-latents. Our method is compared with state-of-the-art\nencoding approaches, demonstrating that our model excels in terms of\nreconstruction quality and is capable of editing even challenging out-of-domain\nexamples. Code is available at\nhttps://github.com/AIRI-Institute/StyleFeatureEditor.",
        "ArXiv Link": "https://arxiv.org/abs/2406.10601",
        "PDF Link": "https://arxiv.org/pdf/2406.10601",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "HARE: HumAn pRiors, a key to small language model Efficiency",
        "Abstract": "Human priors play a crucial role in efficiently utilizing data in deep\nlearning. However, with the development of large language models (LLMs), there\nis an increasing emphasis on scaling both model size and data volume, which\noften diminishes the importance of human priors in data construction.\nInfluenced by these trends, existing Small Language Models (SLMs) mainly rely\non web-scraped large-scale training data, neglecting the proper incorporation\nof human priors. This oversight limits the training efficiency of language\nmodels in resource-constrained settings. In this paper, we propose a principle\nto leverage human priors for data construction. This principle emphasizes\nachieving high-performance SLMs by training on a concise dataset that\naccommodates both semantic diversity and data quality consistency, while\navoiding benchmark data leakage. Following this principle, we train an SLM\nnamed HARE-1.1B. Extensive experiments on large-scale benchmark datasets\ndemonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs,\nvalidating the effectiveness of the proposed principle. Additionally, this\nprovides new insights into efficient language model training in\nresource-constrained environments from the view of human priors.",
        "ArXiv Link": "https://arxiv.org/abs/2406.11410",
        "PDF Link": "https://arxiv.org/pdf/2406.11410",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
        "Abstract": "Vision Language Models (VLMs) demonstrate remarkable proficiency in\naddressing a wide array of visual questions, which requires strong perception\nand reasoning faculties. Assessing these two competencies independently is\ncrucial for model refinement, despite the inherent difficulty due to the\nintertwined nature of seeing and reasoning in existing VLMs. To tackle this\nissue, we present Prism, an innovative framework designed to disentangle the\nperception and reasoning processes involved in visual question solving. Prism\ncomprises two distinct stages: a perception stage that utilizes a VLM to\nextract and articulate visual information in textual form, and a reasoning\nstage that formulates responses based on the extracted visual information using\na Large Language Model (LLM). This modular design enables the systematic\ncomparison and assessment of both proprietary and open-source VLM for their\nperception and reasoning strengths. Our analytical framework provides several\nvaluable insights, underscoring Prism's potential as a cost-effective solution\nfor vision-language tasks. By combining a streamlined VLM focused on perception\nwith a powerful LLM tailored for reasoning, Prism achieves superior results in\ngeneral vision-language tasks while substantially cutting down on training and\noperational expenses. Quantitative evaluations show that Prism, when configured\nwith a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on\npar with VLMs 10 times larger on the rigorous multimodal benchmark MMStar.\nThe project is released at: https://github.com/SparksJoe/Prism.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14544",
        "PDF Link": "https://arxiv.org/pdf/2406.14544",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
        "Abstract": "Merging Large Language Models (LLMs) is a cost-effective technique for\ncombining multiple expert LLMs into a single versatile model, retaining the\nexpertise of the original ones. However, current approaches often overlook the\nimportance of safety alignment during merging, leading to highly misaligned\nmodels. This work investigates the effects of model merging on alignment. We\nevaluate several popular model merging techniques, demonstrating that existing\nmethods do not only transfer domain expertise but also propagate misalignment.\nWe propose a simple two-step approach to address this problem: (i) generating\nsynthetic safety and domain-specific data, and (ii) incorporating these\ngenerated data into the optimization process of existing data-aware model\nmerging techniques. This allows us to treat alignment as a skill that can be\nmaximized in the resulting merged LLM. Our experiments illustrate the\neffectiveness of integrating alignment-related data during merging, resulting\nin models that excel in both domain expertise and alignment.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14563",
        "PDF Link": "https://arxiv.org/pdf/2406.14563",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding",
        "Abstract": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14515",
        "PDF Link": "https://arxiv.org/pdf/2406.14515",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
        "Abstract": "When presented with questions involving visual thinking, humans naturally\nswitch reasoning modalities, often forming mental images or drawing visual\naids. Large language models have shown promising results in arithmetic and\nsymbolic reasoning by expressing intermediate reasoning in text as a chain of\nthought, yet struggle to extend this capability to answer text queries that are\neasily solved by visual reasoning, even with extensive multimodal pretraining.\nWe introduce a simple method, whiteboard-of-thought prompting, to unlock the\nvisual reasoning capabilities of multimodal large language models across\nmodalities. Whiteboard-of-thought prompting provides multimodal large language\nmodels with a metaphorical `whiteboard' to draw out reasoning steps as images,\nthen returns these images back to the model for further processing. We find\nthis can be accomplished with no demonstrations or specialized modules, instead\nleveraging models' existing ability to write code with libraries such as\nMatplotlib and Turtle. This simple approach shows state-of-the-art results on\nfour difficult natural language tasks that involve visual and spatial\nreasoning. We identify multiple settings where GPT-4o using chain-of-thought\nfails dramatically, including more than one where it achieves 0% accuracy,\nwhile whiteboard-of-thought enables up to 92% accuracy in these same\nsettings. We present a detailed exploration of where the technique succeeds as\nwell as its sources of error.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14562",
        "PDF Link": "https://arxiv.org/pdf/2406.14562",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Invertible Consistency Distillation for Text-Guided Image Editing in\n  Around 7 Steps",
        "Abstract": "Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14539",
        "PDF Link": "https://arxiv.org/pdf/2406.14539",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal\n  Documents",
        "Abstract": "Recent advancements in Large Multimodal Models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. Addressing these issues, we\nintroduce a novel dataset format, PIN (Paired and INterleaved multimodal\ndocuments), designed to significantly improve both the depth and breadth of\nmultimodal training. The PIN format is built on three foundational principles:\nknowledge intensity, scalability, and support for diverse training modalities.\nThis innovative format combines markdown files and comprehensive images to\nenrich training data with a dense knowledge structure and versatile training\nstrategies. We present PIN-14M, an open-source dataset comprising 14 million\nsamples derived from a diverse range of Chinese and English sources, tailored\nto include complex web and scientific content. This dataset is constructed\nmeticulously to ensure data quality and ethical integrity, aiming to facilitate\nadvanced training strategies and improve model robustness against common\nmultimodal training pitfalls. Our initial results, forming the basis of this\ntechnical report, suggest significant potential for the PIN format in refining\nLMM performance, with plans for future expansions and detailed evaluations of\nits impact on model capabilities.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13923",
        "PDF Link": "https://arxiv.org/pdf/2406.13923",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks",
        "Abstract": "Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.",
        "ArXiv Link": "https://arxiv.org/abs/2406.12925",
        "PDF Link": "https://arxiv.org/pdf/2406.12925",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous\n  Reinforcement Learning",
        "Abstract": "Training corpuses for vision language models (VLMs) typically lack sufficient\namounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal\nfor decision-making tasks such as in-the-wild device control through graphical\nuser interfaces (GUIs). While training with static demonstrations has shown\nsome promise, we show that such methods fall short for controlling real GUIs\ndue to their failure to deal with real-world stochasticity and non-stationarity\nnot captured in static observational data. This paper introduces a novel\nautonomous RL approach, called DigiRL, for training in-the-wild device control\nagents through fine-tuning a pre-trained VLM in two stages: offline RL to\ninitialize the model, followed by offline-to-online RL. To do this, we build a\nscalable and parallelizable Android learning environment equipped with a\nVLM-based evaluator and develop a simple yet effective RL approach for learning\nin this domain. Our approach runs advantage-weighted RL with advantage\nestimators enhanced to account for stochasticity along with an automatic\ncurriculum for deriving maximal learning signal. We demonstrate the\neffectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our\n1.3B VLM trained with RL achieves a 49.5% absolute improvement -- from 17.7 to\n67.2% success rate -- over supervised fine-tuning with static human\ndemonstration data. These results significantly surpass not only the prior best\nagents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent\ntrained with AitW data (38.5%), but also the prior best autonomous RL approach\nbased on filtered behavior cloning (57.8%), thereby establishing a new\nstate-of-the-art for digital agents for in-the-wild device control.",
        "ArXiv Link": "https://arxiv.org/abs/2406.11896",
        "PDF Link": "https://arxiv.org/pdf/2406.11896",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Self-play with Execution Feedback: Improving Instruction-following\n  Capabilities of Large Language Models",
        "Abstract": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13542",
        "PDF Link": "https://arxiv.org/pdf/2406.13542",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Improving Visual Commonsense in Language Models via Multiple Image\n  Generation",
        "Abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge.\nHowever, existing large language models (LLMs) are primarily trained using\ntextual data only, limiting their ability to incorporate essential visual\ninformation. In contrast, Visual Language Models, which excel at\nvisually-oriented tasks, often fail at non-visual tasks such as basic\ncommonsense reasoning. This divergence highlights a critical challenge - the\nintegration of robust visual understanding with foundational text-based\nlanguage reasoning. To this end, we introduce a method aimed at enhancing LLMs'\nvisual commonsense. Specifically, our method generates multiple images based on\nthe input text prompt and integrates these into the model's decision-making\nprocess by mixing their prediction probabilities. To facilitate multimodal\ngrounded language modeling, we employ a late-fusion layer that combines the\nprojected visual features with the output of a pre-trained LLM conditioned on\ntext only. This late-fusion layer enables predictions based on comprehensive\nimage-text knowledge as well as text only when this is required. We evaluate\nour approach using several visual commonsense reasoning tasks together with\ntraditional NLP tasks, including common sense reasoning and reading\ncomprehension. Our experimental results demonstrate significant superiority\nover existing baselines. When applied to recent state-of-the-art LLMs (e.g.,\nLlama3), we observe improvements not only in visual common sense but also in\ntraditional NLP benchmarks. Code and models are available under\nhttps://github.com/guyyariv/vLMIG.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13621",
        "PDF Link": "https://arxiv.org/pdf/2406.13621",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "LiveMind: Low-latency Large Language Models with Simultaneous Inference",
        "Abstract": "In this paper, we introduce a novel low-latency inference framework for large\nlanguage models (LLMs) inference which enables LLMs to perform inferences with\nincomplete prompts. By reallocating computational processes to prompt input\nphase, we achieve a substantial reduction in latency, thereby significantly\nenhancing the interactive experience for users of LLMs. The framework adeptly\nmanages the visibility of the streaming prompt to the model, allowing it to\ninfer from incomplete prompts or await additional prompts. Compared with\ntraditional inference methods that utilize complete prompts, our approach\ndemonstrates an average reduction of 59% in response latency on the MMLU-Pro\ndataset, while maintaining comparable accuracy. Additionally, our framework\nfacilitates collaborative inference and output across different models. By\nemploying an LLM for inference and a small language model (SLM) for output, we\nachieve an average 68% reduction in response latency, alongside a 5.5%\nimprovement in accuracy on the MMLU-Pro dataset compared with the SLM baseline.\nFor long prompts exceeding 20 sentences, the response latency can be reduced by\nup to 93%.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14319",
        "PDF Link": "https://arxiv.org/pdf/2406.14319",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Iterative Length-Regularized Direct Preference Optimization: A Case\n  Study on Improving 7B Language Models to GPT-4 Level",
        "Abstract": "Direct Preference Optimization (DPO), a standard method for aligning language\nmodels with human preferences, is traditionally applied to offline preferences.\nRecent studies show that DPO benefits from iterative training with online\npreferences labeled by a trained reward model. In this work, we identify a\npitfall of vanilla iterative DPO - improved response quality can lead to\nincreased verbosity. To address this, we introduce iterative length-regularized\nDPO (iLR-DPO) to penalize response length. Our empirical results show that\niLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing\nverbosity. Specifically, our 7B model achieves a 50.5% length-controlled win\nrate against GPT-4 Preview on AlpacaEval 2.0, and excels across\nstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.\nThese results demonstrate the effectiveness of iterative DPO in aligning\nlanguage models with human feedback.",
        "ArXiv Link": "https://arxiv.org/abs/2406.11817",
        "PDF Link": "https://arxiv.org/pdf/2406.11817",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "ExVideo: Extending Video Diffusion Models via Parameter-Efficient\n  Post-Tuning",
        "Abstract": "Recently, advancements in video synthesis have attracted significant\nattention. Video synthesis models such as AnimateDiff and Stable Video\nDiffusion have demonstrated the practical applicability of diffusion models in\ncreating dynamic visual content. The emergence of SORA has further spotlighted\nthe potential of video generation technologies. Nonetheless, the extension of\nvideo lengths has been constrained by the limitations in computational\nresources. Most existing video synthesis models can only generate short video\nclips. In this paper, we propose a novel post-tuning methodology for video\nsynthesis models, called ExVideo. This approach is designed to enhance the\ncapability of current video synthesis models, allowing them to produce content\nover extended temporal durations while incurring lower training expenditures.\nIn particular, we design extension strategies across common temporal model\narchitectures respectively, including 3D convolution, temporal attention, and\npositional embedding. To evaluate the efficacy of our proposed post-tuning\napproach, we conduct extension training on the Stable Video Diffusion model.\nOur approach augments the model's capacity to generate up to 5times its\noriginal number of frames, requiring only 1.5k GPU hours of training on a\ndataset comprising 40k videos. Importantly, the substantial increase in video\nlength doesn't compromise the model's innate generalization capabilities, and\nthe model showcases its advantages in generating videos of diverse styles and\nresolutions. We will release the source code and the enhanced model publicly.",
        "ArXiv Link": "https://arxiv.org/abs/2406.14130",
        "PDF Link": "https://arxiv.org/pdf/2406.14130",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "REPOEXEC: Evaluate Code Generation with a Repository-Level Executable\n  Benchmark",
        "Abstract": "The ability of CodeLLMs to generate executable and functionally correct code\nat the repository-level scale remains largely unexplored. We introduce\nRepoExec, a novel benchmark for evaluating code generation at the\nrepository-level scale. RepoExec focuses on three main aspects: executability,\nfunctional correctness through automated test case generation with high\ncoverage rate, and carefully crafted cross-file contexts to accurately generate\ncode. Our work explores a controlled scenario where developers specify\nnecessary code dependencies, challenging the model to integrate these\naccurately. Experiments show that while pretrained LLMs outperform\ninstruction-tuned models in correctness, the latter excel in utilizing provided\ndependencies and demonstrating debugging capabilities. We also introduce a new\ninstruction-tuned dataset that focuses on code dependencies and demonstrate\nthat CodeLLMs fine-tuned on our dataset have a better capability to leverage\nthese dependencies effectively. RepoExec aims to provide a comprehensive\nevaluation of code functionality and alignment with developer intent, paving\nthe way for more reliable and applicable CodeLLMs in real-world scenarios. The\ndataset and source code can be found\nat~https://github.com/FSoft-AI4Code/RepoExec.",
        "ArXiv Link": "https://arxiv.org/abs/2406.11927",
        "PDF Link": "https://arxiv.org/pdf/2406.11927",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation",
        "Abstract": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13663",
        "PDF Link": "https://arxiv.org/pdf/2406.13663",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "StableSemantics: A Synthetic Language-Vision Dataset of Semantic\n  Representations in Naturalistic Images",
        "Abstract": "Understanding the semantics of visual scenes is a fundamental challenge in\nComputer Vision. A key aspect of this challenge is that objects sharing similar\nsemantic meanings or functions can exhibit striking visual differences, making\naccurate identification and categorization difficult. Recent advancements in\ntext-to-image frameworks have led to models that implicitly capture natural\nscene statistics. These frameworks account for the visual variability of\nobjects, as well as complex object co-occurrences and sources of noise such as\ndiverse lighting conditions. By leveraging large-scale datasets and\ncross-attention conditioning, these models generate detailed and contextually\nrich scene representations. This capability opens new avenues for improving\nobject recognition and scene understanding in varied and challenging\nenvironments. Our work presents StableSemantics, a dataset comprising 224\nthousand human-curated prompts, processed natural language captions, over 2\nmillion synthetic images, and 10 million attention maps corresponding to\nindividual noun chunks. We explicitly leverage human-generated prompts that\ncorrespond to visually interesting stable diffusion generations, provide 10\ngenerations per phrase, and extract cross-attention maps for each image. We\nexplore the semantic distribution of generated images, examine the distribution\nof objects within images, and benchmark captioning and open vocabulary\nsegmentation methods on our data. To the best of our knowledge, we are the\nfirst to release a diffusion dataset with semantic attributions. We expect our\nproposed dataset to catalyze advances in visual semantic understanding and\nprovide a foundation for developing more sophisticated and effective visual\nmodels. Website: https://stablesemantics.github.io/StableSemantics",
        "ArXiv Link": "https://arxiv.org/abs/2406.13735",
        "PDF Link": "https://arxiv.org/pdf/2406.13735",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models",
        "Abstract": "We present a latent diffusion model over 3D scenes, that can be trained using\nonly 2D image data. To achieve this, we first design an autoencoder that maps\nmulti-view images to 3D Gaussian splats, and simultaneously builds a compressed\nlatent representation of these splats. Then, we train a multi-view diffusion\nmodel over the latent space to learn an efficient generative model. This\npipeline does not require object masks nor depths, and is suitable for complex\nscenes with arbitrary camera positions. We conduct careful experiments on two\nlarge-scale datasets of complex real-world scenes -- MVImgNet and\nRealEstate10K. We show that our approach enables generating 3D scenes in as\nlittle as 0.2 seconds, either from scratch, from a single input view, or from\nsparse input views. It produces diverse and high-quality results while running\nan order of magnitude faster than non-latent diffusion models and earlier\nNeRF-based generative models",
        "ArXiv Link": "https://arxiv.org/abs/2406.13099",
        "PDF Link": "https://arxiv.org/pdf/2406.13099",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "A Systematic Survey of Text Summarization: From Statistical Methods to\n  Large Language Models",
        "Abstract": "Text summarization research has undergone several significant transformations\nwith the advent of deep neural networks, pre-trained language models (PLMs),\nand recent large language models (LLMs). This survey thus provides a\ncomprehensive review of the research progress and evolution in text\nsummarization through the lens of these paradigm shifts. It is organized into\ntwo main parts: (1) a detailed overview of datasets, evaluation metrics, and\nsummarization methods before the LLM era, encompassing traditional statistical\nmethods, deep learning approaches, and PLM fine-tuning techniques, and (2) the\nfirst detailed examination of recent advancements in benchmarking, modeling,\nand evaluating summarization in the LLM era. By synthesizing existing\nliterature and presenting a cohesive overview, this survey also discusses\nresearch trends, open challenges, and proposes promising research directions in\nsummarization, aiming to guide researchers through the evolving landscape of\nsummarization research.",
        "ArXiv Link": "https://arxiv.org/abs/2406.11289",
        "PDF Link": "https://arxiv.org/pdf/2406.11289",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "From Insights to Actions: The Impact of Interpretability and Analysis\n  Research on NLP",
        "Abstract": "Interpretability and analysis (IA) research is a growing subfield within NLP\nwith the goal of developing a deeper understanding of the behavior or inner\nworkings of NLP systems and methods. Despite growing interest in the subfield,\na commonly voiced criticism is that it lacks actionable insights and therefore\nhas little impact on NLP. In this paper, we seek to quantify the impact of IA\nresearch on the broader field of NLP. We approach this with a mixed-methods\nanalysis of: (1) a citation graph of 185K+ papers built from all papers\npublished at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of\n138 members of the NLP community. Our quantitative results show that IA work is\nwell-cited outside of IA, and central in the NLP citation graph. Through\nqualitative analysis of survey responses and manual annotation of 556 papers,\nwe find that NLP researchers build on findings from IA work and perceive it is\nimportant for progress in NLP, multiple subfields, and rely on its findings and\nterminology for their own work. Many novel methods are proposed based on IA\nfindings and highly influenced by them, but highly influential non-IA work\ncites IA findings without being driven by them. We end by summarizing what is\nmissing in IA work today and provide a call to action, to pave the way for a\nmore impactful future of IA research.",
        "ArXiv Link": "https://arxiv.org/abs/2406.12618",
        "PDF Link": "https://arxiv.org/pdf/2406.12618",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "\u87ff-bench: A Benchmark for Tool-Agent-User Interaction in Real-World\n  Domains",
        "Abstract": "Existing benchmarks do not test language agents on their interaction with\nhuman users or ability to follow domain-specific rules, both of which are vital\nfor deploying them in real world applications. We propose tau-bench, a\nbenchmark emulating dynamic conversations between a user (simulated by language\nmodels) and a language agent provided with domain-specific API tools and policy\nguidelines. We employ an efficient and faithful evaluation process that\ncompares the database state at the end of a conversation with the annotated\ngoal state. We also propose a new metric (pass^k) to evaluate the reliability\nof agent behavior over multiple trials. Our experiments show that even\nstate-of-the-art function calling agents (like gpt-4o) succeed on <50% of the\ntasks, and are quite inconsistent (pass^8 <25% in retail). Our findings point\nto the need for methods that can improve the ability of agents to act\nconsistently and follow rules reliably.",
        "ArXiv Link": "https://arxiv.org/abs/2406.12045",
        "PDF Link": "https://arxiv.org/pdf/2406.12045",
        "Upvotes": "96",
        "Date": "2024-07-07"
    },
    {
        "Title": "Qwen2 Technical Report",
        "Abstract": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face1 and ModelScope2, and the\nsupplementary materials including example code on GitHub3. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "ArXiv Link": "https://arxiv.org/abs/2407.10671",
        "PDF Link": "https://arxiv.org/pdf/2407.10671",
        "Upvotes": "31",
        "Date": "2024-07-16"
    },
    {
        "Title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore\n  Non-Determinism",
        "Abstract": "Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation.",
        "ArXiv Link": "https://arxiv.org/abs/2407.10457",
        "PDF Link": "https://arxiv.org/pdf/2407.10457",
        "Upvotes": "11",
        "Date": "2024-07-16"
    },
    {
        "Title": "Make-An-Agent: A Generalizable Policy Network Generator with\n  Behavior-Prompted Diffusion",
        "Abstract": "Can we generate a control policy for an agent using just one demonstration of\ndesired behaviors as a prompt, as effortlessly as creating an image from a\ntextual description? In this paper, we present Make-An-Agent, a novel policy\nparameter generator that leverages the power of conditional diffusion models\nfor behavior-to-policy generation. Guided by behavior embeddings that encode\ntrajectory information, our policy generator synthesizes latent parameter\nrepresentations, which can then be decoded into policy networks. Trained on\npolicy network checkpoints and their corresponding trajectories, our generation\nmodel demonstrates remarkable versatility and scalability on multiple tasks and\nhas a strong generalization ability on unseen tasks to output well-performed\npolicies with only few-shot demonstrations as inputs. We showcase its efficacy\nand efficiency on various domains and tasks, including varying objectives,\nbehaviors, and even across different robot manipulators. Beyond simulation, we\ndirectly deploy policies generated by Make-An-Agent onto real-world robots on\nlocomotion tasks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.10973",
        "PDF Link": "https://arxiv.org/pdf/2407.10973",
        "Upvotes": "3",
        "Date": "2024-07-16"
    },
    {
        "Title": "Video Occupancy Models",
        "Abstract": "We introduce a new family of video prediction models designed to support\ndownstream control tasks. We call these models Video Occupancy models (VOCs).\nVOCs operate in a compact latent space, thus avoiding the need to make\npredictions about individual pixels. Unlike prior latent-space world models,\nVOCs directly predict the discounted distribution of future states in a single\nstep, thus avoiding the need for multistep roll-outs. We show that both\nproperties are beneficial when building predictive models of video for use in\ndownstream control. Code is available at\nhttps://github.com/manantomar/video-occupancy-models{github.com/manantomar/video-occupancy-models}.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09533",
        "PDF Link": "https://arxiv.org/pdf/2407.09533",
        "Upvotes": "3",
        "Date": "2024-07-16"
    },
    {
        "Title": "SHERL: Synthesizing High Accuracy and Efficient Memory for\n  Resource-Limited Transfer Learning",
        "Abstract": "Parameter-efficient transfer learning (PETL) has emerged as a flourishing\nresearch field for adapting large pre-trained models to downstream tasks,\ngreatly reducing trainable parameters while grappling with memory challenges\nduring fine-tuning. To address it, memory-efficient series (METL) avoid\nbackpropagating gradients through the large backbone. However, they compromise\nby exclusively relying on frozen intermediate outputs and limiting the\nexhaustive exploration of prior knowledge from pre-trained models. Moreover,\nthe dependency and redundancy between cross-layer features are frequently\noverlooked, thereby submerging more discriminative representations and causing\nan inherent performance gap (vs. conventional PETL methods). Hence, we propose\nan innovative METL strategy called SHERL for resource-limited scenarios to\ndecouple the entire adaptation into two successive and complementary processes.\nIn the early route, intermediate outputs are consolidated via an\nanti-redundancy operation, enhancing their compatibility for subsequent\ninteractions; thereby in the late route, utilizing minimal late pre-trained\nlayers could alleviate the peak demand on memory overhead and regulate these\nfairly flexible features into more adaptive and powerful representations for\nnew domains. Extensive ablations on vision-and-language and language-only tasks\nshow that SHERL combines the strengths of both parameter and memory-efficient\ntechniques, performing on-par or better across diverse architectures with lower\nmemory during fine-tuning. Our code is publicly available at:\nhttps://github.com/Paranioar/SHERL.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07523",
        "PDF Link": "https://arxiv.org/pdf/2407.07523",
        "Upvotes": "2",
        "Date": "2024-07-16"
    },
    {
        "Title": "GRUtopia: Dream General Robots in a City at Scale",
        "Abstract": "Recent works have been exploring the scaling laws in the field of Embodied\nAI. Given the prohibitive costs of collecting real-world data, we believe the\nSimulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the\nlearning of embodied models. This paper introduces project GRUtopia, the first\nsimulated interactive 3D society designed for various robots. It features\nseveral advancements: (a) The scene dataset, GRScenes, includes 100k\ninteractive, finely annotated scenes, which can be freely combined into\ncity-scale environments. In contrast to previous works mainly focusing on home,\nGRScenes covers 89 diverse scene categories, bridging the gap of\nservice-oriented environments where general robots would be initially deployed.\n(b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC)\nsystem that is responsible for social interaction, task generation, and task\nassignment, thus simulating social scenarios for embodied AI applications. (c)\nThe benchmark, GRBench, supports various robots but focuses on legged robots as\nprimary agents and poses moderately challenging tasks involving Object\nLoco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that\nthis work can alleviate the scarcity of high-quality data in this field and\nprovide a more comprehensive assessment of Embodied AI research. The project is\navailable at https://github.com/OpenRobotLab/GRUtopia.",
        "ArXiv Link": "https://arxiv.org/abs/2407.10943",
        "PDF Link": "https://arxiv.org/pdf/2407.10943",
        "Upvotes": "1",
        "Date": "2024-07-16"
    },
    {
        "Title": "Masked Generative Video-to-Audio Transformers with Enhanced\n  Synchronicity",
        "Abstract": "Video-to-audio (V2A) generation leverages visual-only video features to\nrender plausible sounds that match the scene. Importantly, the generated sound\nonsets should match the visual actions that are aligned with them, otherwise\nunnatural synchronization artifacts arise. Recent works have explored the\nprogression of conditioning sound generators on still images and then video\nfeatures, focusing on quality and semantic matching while ignoring\nsynchronization, or by sacrificing some amount of quality to focus on improving\nsynchronization only. In this work, we propose a V2A generative model, named\nMaskVAT, that interconnects a full-band high-quality general audio codec with a\nsequence-to-sequence masked generative model. This combination allows modeling\nboth high audio quality, semantic matching, and temporal synchronicity at the\nsame time. Our results show that, by combining a high-quality codec with the\nproper pre-trained audio-visual features and a sequence-to-sequence parallel\nstructure, we are able to yield highly synchronized results on one hand, whilst\nbeing competitive with the state of the art of non-codec generative audio\nmodels. Sample videos and generated audios are available at\nhttps://maskvat.github.io .",
        "ArXiv Link": "https://arxiv.org/abs/2407.10387",
        "PDF Link": "https://arxiv.org/pdf/2407.10387",
        "Upvotes": "1",
        "Date": "2024-07-16"
    },
    {
        "Title": "LAB-Bench: Measuring Capabilities of Language Models for Biology\n  Research",
        "Abstract": "There is widespread optimism that frontier Large Language Models (LLMs) and\nLLM-augmented systems have the potential to rapidly accelerate scientific\ndiscovery across disciplines. Today, many benchmarks exist to measure LLM\nknowledge and reasoning on textbook-style science questions, but few if any\nbenchmarks are designed to evaluate language model performance on practical\ntasks required for scientific research, such as literature search, protocol\nplanning, and data analysis. As a step toward building such benchmarks, we\nintroduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of\nover 2,400 multiple choice questions for evaluating AI systems on a range of\npractical biology research capabilities, including recall and reasoning over\nliterature, interpretation of figures, access and navigation of databases, and\ncomprehension and manipulation of DNA and protein sequences. Importantly, in\ncontrast to previous scientific benchmarks, we expect that an AI system that\ncan achieve consistently high scores on the more difficult LAB-Bench tasks\nwould serve as a useful assistant for researchers in areas such as literature\nsearch and molecular cloning. As an initial assessment of the emergent\nscientific task capabilities of frontier language models, we measure\nperformance of several against our benchmark and report results compared to\nhuman expert biology researchers. We will continue to update and expand\nLAB-Bench over time, and expect it to serve as a useful tool in the development\nof automated research systems going forward. A public subset of LAB-Bench is\navailable for use at the following URL:\nhttps://huggingface.co/datasets/futurehouse/lab-bench",
        "ArXiv Link": "https://arxiv.org/abs/2407.10362",
        "PDF Link": "https://arxiv.org/pdf/2407.10362",
        "Upvotes": "1",
        "Date": "2024-07-16"
    },
    {
        "Title": "Noise Calibration: Plug-and-play Content-Preserving Video Enhancement\n  using Pre-trained Video Diffusion Models",
        "Abstract": "In order to improve the quality of synthesized videos, currently, one\npredominant method involves retraining an expert diffusion model and then\nimplementing a noising-denoising process for refinement. Despite the\nsignificant training costs, maintaining consistency of content between the\noriginal and enhanced videos remains a major challenge. To tackle this\nchallenge, we propose a novel formulation that considers both visual quality\nand consistency of content. Consistency of content is ensured by a proposed\nloss function that maintains the structure of the input, while visual quality\nis improved by utilizing the denoising process of pretrained diffusion models.\nTo address the formulated optimization problem, we have developed a\nplug-and-play noise optimization strategy, referred to as Noise Calibration. By\nrefining the initial random noise through a few iterations, the content of\noriginal video can be largely preserved, and the enhancement effect\ndemonstrates a notable improvement. Extensive experiments have demonstrated the\neffectiveness of the proposed method.",
        "ArXiv Link": "https://arxiv.org/abs/2407.10285",
        "PDF Link": "https://arxiv.org/pdf/2407.10285",
        "Upvotes": "1",
        "Date": "2024-07-16"
    },
    {
        "Title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models",
        "Abstract": "Spreadsheets, with their extensive two-dimensional grids, various layouts,\nand diverse formatting options, present notable challenges for large language\nmodels (LLMs). In response, we introduce SpreadsheetLLM, pioneering an\nefficient encoding method designed to unleash and optimize LLMs' powerful\nunderstanding and reasoning capability on spreadsheets. Initially, we propose a\nvanilla serialization approach that incorporates cell addresses, values, and\nformats. However, this approach was limited by LLMs' token constraints, making\nit impractical for most applications. To tackle this challenge, we develop\nSheetCompressor, an innovative encoding framework that compresses spreadsheets\neffectively for LLMs. It comprises three modules: structural-anchor-based\ncompression, inverse index translation, and data-format-aware aggregation. It\nsignificantly improves performance in spreadsheet table detection task,\noutperforming the vanilla approach by 25.6% in GPT4's in-context learning\nsetting. Moreover, fine-tuned LLM with SheetCompressor has an average\ncompression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score,\nsurpassing the best existing models by 12.3%. Finally, we propose Chain of\nSpreadsheet for downstream tasks of spreadsheet understanding and validate in a\nnew and demanding spreadsheet QA task. We methodically leverage the inherent\nlayout and structure of spreadsheets, demonstrating that SpreadsheetLLM is\nhighly effective across a variety of spreadsheet tasks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09025",
        "PDF Link": "https://arxiv.org/pdf/2407.09025",
        "Upvotes": "67",
        "Date": "2024-07-16"
    },
    {
        "Title": "Human-like Episodic Memory for Infinite Context LLMs",
        "Abstract": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs,\nenabling them to effectively handle practically infinite context lengths while\nmaintaining computational efficiency. EM-LLM organises sequences of tokens into\ncoherent episodic events using a combination of Bayesian surprise and\ngraph-theoretic boundary refinement in an on-line fashion. When needed, these\nevents are retrieved through a two-stage memory process, combining\nsimilarity-based and temporally contiguous retrieval for efficient and\nhuman-like access to relevant information. Experiments on the LongBench dataset\ndemonstrate EM-LLM's superior performance, outperforming the state-of-the-art\nInfLLM model with an overall relative improvement of 4.3% across various tasks,\nincluding a 33% improvement on the PassageRetrieval task. Furthermore, our\nanalysis reveals strong correlations between EM-LLM's event segmentation and\nhuman-perceived events, suggesting a bridge between this artificial system and\nits biological counterpart. This work not only advances LLM capabilities in\nprocessing extended contexts but also provides a computational framework for\nexploring human memory mechanisms, opening new avenues for interdisciplinary\nresearch in AI and cognitive science.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09450",
        "PDF Link": "https://arxiv.org/pdf/2407.09450",
        "Upvotes": "39",
        "Date": "2024-07-16"
    },
    {
        "Title": "Toto: Time Series Optimized Transformer for Observability",
        "Abstract": "This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07874",
        "PDF Link": "https://arxiv.org/pdf/2407.07874",
        "Upvotes": "23",
        "Date": "2024-07-16"
    },
    {
        "Title": "H2O-Danube3 Technical Report",
        "Abstract": "We present H2O-Danube3, a series of small language models consisting of\nH2O-Danube3-4B, trained on 6T tokens and H2O-Danube3-500M, trained on 4T\ntokens. Our models are pre-trained on high quality Web data consisting of\nprimarily English tokens in three stages with different data mixes before final\nsupervised tuning for chat version. The models exhibit highly competitive\nmetrics across a multitude of academic, chat, and fine-tuning benchmarks.\nThanks to its compact architecture, H2O-Danube3 can be efficiently run on a\nmodern smartphone, enabling local inference and rapid processing capabilities\neven on mobile devices. We make all models openly available under Apache 2.0\nlicense further democratizing LLMs to a wider audience economically.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09276",
        "PDF Link": "https://arxiv.org/pdf/2407.09276",
        "Upvotes": "15",
        "Date": "2024-07-16"
    },
    {
        "Title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution",
        "Abstract": "Large Language Models (LLMs) are frequently updated due to data or\narchitecture changes to improve their performance. When updating models,\ndevelopers often focus on increasing overall performance metrics with less\nemphasis on being compatible with previous model versions. However, users often\nbuild a mental model of the functionality and capabilities of a particular\nmachine learning model they are interacting with. They have to adapt their\nmental model with every update -- a draining task that can lead to user\ndissatisfaction. In practice, fine-tuned downstream task adapters rely on\npretrained LLM base models. When these base models are updated, these\nuser-facing downstream task models experience instance regression or negative\nflips -- previously correct instances are now predicted incorrectly. This\nhappens even when the downstream task training procedures remain identical. Our\nwork aims to provide seamless model updates to a user in two ways. First, we\nprovide evaluation metrics for a notion of compatibility to prior model\nversions, specifically for generative tasks but also applicable for\ndiscriminative tasks. We observe regression and inconsistencies between\ndifferent model versions on a diverse set of tasks and model updates. Second,\nwe propose a training strategy to minimize the number of inconsistencies in\nmodel updates, involving training of a compatibility model that can enhance\ntask fine-tuned language models. We reduce negative flips -- instances where a\nprior model version was correct, but a new model incorrect -- by up to 40% from\nLlama 1 to Llama 2.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09435",
        "PDF Link": "https://arxiv.org/pdf/2407.09435",
        "Upvotes": "12",
        "Date": "2024-07-16"
    },
    {
        "Title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers",
        "Abstract": "Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. To address this\nlimitation, we introduce SPIQA (Scientific Paper Image Question Answering), the\nfirst large-scale QA dataset specifically designed to interpret complex figures\nand tables within the context of scientific research articles across various\ndomains of computer science. Leveraging the breadth of expertise and ability of\nmultimodal large language models (MLLMs) to understand figures, we employ\nautomatic and manual curation to create the dataset. We craft an\ninformation-seeking task involving multiple images that cover a wide variety of\nplots, charts, tables, schematic diagrams, and result visualizations. SPIQA\ncomprises 270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09413",
        "PDF Link": "https://arxiv.org/pdf/2407.09413",
        "Upvotes": "8",
        "Date": "2024-07-16"
    },
    {
        "Title": "Transformer Layers as Painters",
        "Abstract": "Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09298",
        "PDF Link": "https://arxiv.org/pdf/2407.09298",
        "Upvotes": "8",
        "Date": "2024-07-16"
    },
    {
        "Title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing",
        "Abstract": "Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current methods for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computation cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking. Specifically, for a\nbehavior that we aim to avoid, we employ a linear classifier, which we term the\nbehavior probe, to classify binary behavior labels within the hidden state\nspace of the LLM. Using this probe, we introduce an algorithm to identify a\ncritical subset of LLM parameters that significantly influence this targeted\nbehavior. Then we directly edit these selected parameters by shifting them\ntowards the behavior probe. Such a direct parameter editing method necessitates\nonly inference-level computational resources. Experiments demonstrate that in\nthe representative detoxification task, our approach achieves reductions of up\nto 90.0\\% in toxicity on the RealToxicityPrompts dataset and 49.2\\% on ToxiGen,\nwhile maintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics. Our code is available at\nhttps://github.com/lucywang720/model-surgery.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08770",
        "PDF Link": "https://arxiv.org/pdf/2407.08770",
        "Upvotes": "7",
        "Date": "2024-07-16"
    },
    {
        "Title": "Speech Slytherin: Examining the Performance and Efficiency of Mamba for\n  Speech Separation, Recognition, and Synthesis",
        "Abstract": "It is too early to conclude that Mamba is a better alternative to\ntransformers for speech before comparing Mamba with transformers in terms of\nboth performance and efficiency in multiple speech-related tasks. To reach this\nconclusion, we propose and evaluate three models for three tasks: Mamba-TasNet\nfor speech separation, ConMamba for speech recognition, and VALL-M for speech\nsynthesis. We compare them with transformers of similar sizes in performance,\nmemory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable\nor higher performance than their transformer counterparts: Sepformer,\nConformer, and VALL-E. They are more efficient than transformers in memory and\nspeed for speech longer than a threshold duration, inversely related to the\nresolution of a speech token. Mamba for separation is the most efficient, and\nMamba for recognition is the least. Further, we show that Mamba is not more\nefficient than transformer for speech shorter than the threshold duration and\nperforms worse in models that require joint modeling of text and speech, such\nas cross or masked attention of two inputs. Therefore, we argue that the\nsuperiority of Mamba or transformer depends on particular problems and models.\nCode available at https://github.com/xi-j/Mamba-TasNet and\nhttps://github.com/xi-j/Mamba-ASR.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09732",
        "PDF Link": "https://arxiv.org/pdf/2407.09732",
        "Upvotes": "5",
        "Date": "2024-07-16"
    },
    {
        "Title": "New Desiderata for Direct Preference Optimization",
        "Abstract": "Large language models in the past have typically relied on some form of\nreinforcement learning with human feedback (RLHF) to better align model\nresponses with human preferences. However, because of oft-observed\ninstabilities when implementing these RLHF pipelines, various\nreparameterization techniques have recently been introduced to sidestep the\nneed for separately learning an RL reward model. Instead, directly fine-tuning\nfor human preferences is achieved via the minimization of a single closed-form\ntraining objective, a process originally referred to as direct preference\noptimization (DPO) and followed by several notable descendants. Although\neffective in certain real-world settings, we introduce new evaluation criteria\nthat serve to highlight unresolved shortcomings in the ability of existing DPO\nmethods to interpolate between a pre-trained reference model and empirical\nmeasures of human preferences, as well as unavoidable trade-offs in how low-\nand high-quality responses are regularized and constraints are handled. Our\ninsights then motivate an alternative DPO-like loss that provably mitigates\nthese limitations. Empirical results serve to corroborate notable aspects of\nour analyses.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09072",
        "PDF Link": "https://arxiv.org/pdf/2407.09072",
        "Upvotes": "5",
        "Date": "2024-07-16"
    },
    {
        "Title": "GAVEL: Generating Games Via Evolution and Language Models",
        "Abstract": "Automatically generating novel and interesting games is a complex task.\nChallenges include representing game rules in a computationally workable form,\nsearching through the large space of potential games under most such\nrepresentations, and accurately evaluating the originality and quality of\npreviously unseen games. Prior work in automated game generation has largely\nfocused on relatively restricted rule representations and relied on\ndomain-specific heuristics. In this work, we explore the generation of novel\ngames in the comparatively expansive Ludii game description language, which\nencodes the rules of over 1000 board games in a variety of styles and modes of\nplay. We draw inspiration from recent advances in large language models and\nevolutionary computation in order to train a model that intelligently mutates\nand recombines games and mechanics expressed as code. We demonstrate both\nquantitatively and qualitatively that our approach is capable of generating new\nand interesting games, including in regions of the potential rules space not\ncovered by existing games in the Ludii dataset. A sample of the generated games\nare available to play online through the Ludii portal.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09388",
        "PDF Link": "https://arxiv.org/pdf/2407.09388",
        "Upvotes": "5",
        "Date": "2024-07-16"
    },
    {
        "Title": "StyleSplat: 3D Object Style Transfer with Gaussian Splatting",
        "Abstract": "Recent advancements in radiance fields have opened new avenues for creating\nhigh-quality 3D assets and scenes. Style transfer can enhance these 3D assets\nwith diverse artistic styles, transforming creative expression. However,\nexisting techniques are often slow or unable to localize style transfer to\nspecific objects. We introduce StyleSplat, a lightweight method for stylizing\n3D objects in scenes represented by 3D Gaussians from reference style images.\nOur approach first learns a photorealistic representation of the scene using 3D\nGaussian splatting while jointly segmenting individual 3D objects. We then use\na nearest-neighbor feature matching loss to finetune the Gaussians of the\nselected objects, aligning their spherical harmonic coefficients with the style\nimage to ensure consistency and visual appeal. StyleSplat allows for quick,\ncustomizable style transfer and localized stylization of multiple objects\nwithin a scene, each with a different style. We demonstrate its effectiveness\nacross various 3D scenes and styles, showcasing enhanced control and\ncustomization in 3D creation.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09473",
        "PDF Link": "https://arxiv.org/pdf/2407.09473",
        "Upvotes": "5",
        "Date": "2024-07-16"
    },
    {
        "Title": "TCAN: Animating Human Images with Temporally Consistent Pose Guidance\n  using Diffusion Models",
        "Abstract": "Pose-driven human-image animation diffusion models have shown remarkable\ncapabilities in realistic human video synthesis. Despite the promising results\nachieved by previous approaches, challenges persist in achieving temporally\nconsistent animation and ensuring robustness with off-the-shelf pose detectors.\nIn this paper, we present TCAN, a pose-driven human image animation method that\nis robust to erroneous poses and consistent over time. In contrast to previous\nmethods, we utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption pairs. To\nkeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling the\nnetwork to align the latent space between the pose and appearance features.\nAdditionally, by introducing an additional temporal layer to the ControlNet, we\nenhance robustness against outliers of the pose detector. Through the analysis\nof attention maps over the temporal axis, we also designed a novel temperature\nmap leveraging pose information, allowing for a more static background.\nExtensive experiments demonstrate that the proposed method can achieve\npromising results in video synthesis tasks encompassing various poses, like\nchibi. Project Page: https://eccv2024tcan.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2407.09012",
        "PDF Link": "https://arxiv.org/pdf/2407.09012",
        "Upvotes": "5",
        "Date": "2024-07-16"
    },
    {
        "Title": "Characterizing Prompt Compression Methods for Long Context Inference",
        "Abstract": "Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08892",
        "PDF Link": "https://arxiv.org/pdf/2407.08892",
        "Upvotes": "4",
        "Date": "2024-07-16"
    },
    {
        "Title": "Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning",
        "Abstract": "Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.",
        "ArXiv Link": "https://arxiv.org/abs/2406.02265",
        "PDF Link": "https://arxiv.org/pdf/2406.02265",
        "Upvotes": "4",
        "Date": "2024-07-16"
    },
    {
        "Title": "RRM: Relightable assets using Radiance guided Material extraction",
        "Abstract": "Synthesizing NeRFs under arbitrary lighting has become a seminal problem in\nthe last few years. Recent efforts tackle the problem via the extraction of\nphysically-based parameters that can then be rendered under arbitrary lighting,\nbut they are limited in the range of scenes they can handle, usually\nmishandling glossy scenes. We propose RRM, a method that can extract the\nmaterials, geometry, and environment lighting of a scene even in the presence\nof highly reflective objects. Our method consists of a physically-aware\nradiance field representation that informs physically-based parameters, and an\nexpressive environment light structure based on a Laplacian Pyramid. We\ndemonstrate that our contributions outperform the state-of-the-art on parameter\nretrieval tasks, leading to high-fidelity relighting and novel view synthesis\non surfacic scenes.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06397",
        "PDF Link": "https://arxiv.org/pdf/2407.06397",
        "Upvotes": "3",
        "Date": "2024-07-16"
    },
    {
        "Title": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled\n  Refusal Training",
        "Abstract": "This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nwell-known models such as GPT-4 in defending against attacks. Importantly, our\napproach successfully defends recent advanced attack methods (e.g., CodeAttack)\nthat have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be\nfound at https://github.com/RobustNLP/DeRTa.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09121",
        "PDF Link": "https://arxiv.org/pdf/2407.09121",
        "Upvotes": "3",
        "Date": "2024-07-16"
    },
    {
        "Title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large\n  Language Models -- The Story Goes On",
        "Abstract": "In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08348",
        "PDF Link": "https://arxiv.org/pdf/2407.08348",
        "Upvotes": "41",
        "Date": "2024-07-16"
    },
    {
        "Title": "Video Diffusion Alignment via Reward Gradients",
        "Abstract": "We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08737",
        "PDF Link": "https://arxiv.org/pdf/2407.08737",
        "Upvotes": "36",
        "Date": "2024-07-16"
    },
    {
        "Title": "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning\n  Instruction Using Language Model",
        "Abstract": "Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: https://github.com/zwq2018/Multi-modal-Self-instruct.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07053",
        "PDF Link": "https://arxiv.org/pdf/2407.07053",
        "Upvotes": "35",
        "Date": "2024-07-16"
    },
    {
        "Title": "MAVIS: Mathematical Visual Instruction Tuning",
        "Abstract": "Multi-modal Large Language Models (MLLMs) have recently emerged as a\nsignificant focus in academia and industry. Despite their proficiency in\ngeneral multi-modal scenarios, the mathematical problem-solving capabilities in\nvisual contexts remain insufficiently explored. We identify three key areas\nwithin MLLMs that need to be improved: visual encoding of math diagrams,\ndiagram-language alignment, and mathematical reasoning skills. This draws forth\nan urgent demand for large-scale, high-quality data and training pipelines in\nvisual mathematics. In this paper, we propose MAVIS, the first MAthematical\nVISual instruction tuning paradigm for MLLMs, involving a series of\nmathematical visual datasets and specialized MLLMs. Targeting the three issues,\nMAVIS contains three progressive training stages from scratch. First, we curate\nMAVIS-Caption, consisting of 558K diagram-caption pairs, to fine-tune a\nmath-specific vision encoder (CLIP-Math) through contrastive learning, tailored\nfor improved diagram visual encoding. Second, we utilize MAVIS-Caption to align\nthe CLIP-Math with a large language model (LLM) by a projection layer,\nenhancing vision-language alignment in mathematical domains. Third, we\nintroduce MAVIS-Instruct, including 900K meticulously collected and annotated\nvisual math problems, which is adopted to finally instruct-tune the MLLM for\nrobust mathematical reasoning skills. In MAVIS-Instruct, we incorporate\ncomplete chain-of-thought (CoT) rationales for each problem, and minimize\ntextual redundancy, thereby concentrating the model towards the visual\nelements. Data and Models are released at https://github.com/ZrrSkywalker/MAVIS",
        "ArXiv Link": "https://arxiv.org/abs/2407.08739",
        "PDF Link": "https://arxiv.org/pdf/2407.08739",
        "Upvotes": "26",
        "Date": "2024-07-16"
    },
    {
        "Title": "Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive\n  Low-Rank Gradients",
        "Abstract": "Training Large Language Models (LLMs) is memory-intensive due to the large\nnumber of parameters and associated optimization states. GaLore, a recent\nmethod, reduces memory usage by projecting weight gradients into a low-rank\nsubspace without compromising performance. However, GaLore relies on\ntime-consuming Singular Value Decomposition (SVD) operations to identify the\nsubspace, and the frequent subspace updates lead to significant training time\noverhead. Moreover, GaLore offers minimal improvements in accuracy and\nefficiency compared to LoRA in more accessible fine-tuning scenarios. To\naddress these limitations, we introduce Q-Galore, a novel approach that\nsubstantially reduces memory usage by combining quantization and low-rank\nprojection, surpassing the benefits of GaLore. Our method is based on two key\nobservations: (i) the gradient subspace exhibits diverse properties, with some\nlayers converging early in training while others are subject to frequent\nchanges; (ii) the projection matrices are highly resilient to low-bit\nquantization. Leveraging these insights, Q-GaLore adaptively updates the\ngradient subspace based on its convergence statistics, achieving comparable\nperformance while significantly reducing the number of SVD operations. We\nmaintain the projection matrices in INT4 format and weights in INT8 format,\nincorporating stochastic rounding to capture accumulated gradient information.\nThis approach enables a high-precision training trajectory using only\nlow-precision weights. We demonstrate that Q-GaLore achieves highly competitive\nperformance with exceptional memory efficiency. At pre-training, Q-GaLore\nfacilitates training a LLaMA-7B model from scratch on a single NVIDIA RTX 4060\nTi with only 16 GB memory. At fine-tuning, it reduces memory consumption by up\nto 50% compared to LoRA and GaLore, while consistently outperforming QLoRA at\nthe same memory cost.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08296",
        "PDF Link": "https://arxiv.org/pdf/2407.08296",
        "Upvotes": "25",
        "Date": "2024-07-16"
    },
    {
        "Title": "Self-Recognition in Language Models",
        "Abstract": "A rapidly growing number of applications rely on a small set of closed-source\nlanguage models (LMs). This dependency might introduce novel security risks if\nLMs develop self-recognition capabilities. Inspired by human identity\nverification methods, we propose a novel approach for assessing\nself-recognition in LMs using model-generated \"security questions\". Our test\ncan be externally administered to keep track of frontier models as it does not\nrequire access to internal model parameters or output probabilities. We use our\ntest to examine self-recognition in ten of the most capable open- and\nclosed-source LMs currently publicly available. Our extensive experiments found\nno empirical evidence of general or consistent self-recognition in any examined\nLM. Instead, our results suggest that given a set of alternatives, LMs seek to\npick the \"best\" answer, regardless of its origin. Moreover, we find indications\nthat preferences about which models produce the best answers are consistent\nacross LMs. We additionally uncover novel insights on position bias\nconsiderations for LMs in multiple-choice settings.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06946",
        "PDF Link": "https://arxiv.org/pdf/2407.06946",
        "Upvotes": "20",
        "Date": "2024-07-16"
    },
    {
        "Title": "Is Your Model Really A Good Math Reasoner? Evaluating Mathematical\n  Reasoning with Checklist",
        "Abstract": "Exceptional mathematical reasoning ability is one of the key features that\ndemonstrate the power of large language models (LLMs). How to comprehensively\ndefine and evaluate the mathematical abilities of LLMs, and even reflect the\nuser experience in real-world scenarios, has emerged as a critical issue.\nCurrent benchmarks predominantly concentrate on problem-solving capabilities,\nwhich presents a substantial risk of model overfitting and fails to accurately\nrepresent genuine mathematical reasoning abilities. In this paper, we argue\nthat if a model really understands a problem, it should be robustly and readily\napplied across a diverse array of tasks. Motivated by this, we introduce\nMATHCHECK, a well-designed checklist for testing task generalization and\nreasoning robustness, as well as an automatic tool to generate checklists\nefficiently. MATHCHECK includes multiple mathematical reasoning tasks and\nrobustness test types to facilitate a comprehensive evaluation of both\nmathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we\ndevelop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual\nreasoning and multi-modal reasoning capabilities, respectively, serving as\nupgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.\nWe adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,\nassessing their comprehensive mathematical reasoning abilities. Our results\ndemonstrate that while frontier LLMs like GPT-4o continue to excel in various\nabilities on the checklist, many other model families exhibit a significant\ndecline. Further experiments indicate that, compared to traditional math\nbenchmarks, MATHCHECK better reflects true mathematical abilities and\nrepresents mathematical intelligence more linearly, thereby supporting our\ndesign. On our MATHCHECK, we can easily conduct detailed behavior analysis to\ndeeply investigate models.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08733",
        "PDF Link": "https://arxiv.org/pdf/2407.08733",
        "Upvotes": "16",
        "Date": "2024-07-16"
    },
    {
        "Title": "DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal\n  Perception",
        "Abstract": "Existing Multimodal Large Language Models (MLLMs) increasingly emphasize\ncomplex understanding of various visual elements, including multiple objects,\ntext information, and spatial relations. Their development for comprehensive\nvisual perception hinges on the availability of high-quality image-text\ndatasets that offer diverse visual elements and throughout image descriptions.\nHowever, the scarcity of such hyper-detailed datasets currently hinders\nprogress within the MLLM community. The bottleneck stems from the limited\nperceptual capabilities of current caption engines, which fall short in\nproviding complete and accurate annotations. To facilitate the cutting-edge\nresearch of MLLMs on comprehensive vision perception, we thereby propose\nPerceptual Fusion, using a low-budget but highly effective caption engine for\ncomplete and accurate image descriptions. Specifically, Perceptual Fusion\nintegrates diverse perception experts as image priors to provide explicit\ninformation on visual elements and adopts an efficient MLLM as a centric pivot\nto mimic advanced MLLMs' perception abilities. We carefully select 1M highly\nrepresentative images from uncurated LAION dataset and generate dense\ndescriptions using our engine, dubbed DenseFusion-1M. Extensive experiments\nvalidate that our engine outperforms its counterparts, where the resulting\ndataset significantly improves the perception and cognition abilities of\nexisting MLLMs across diverse vision-language benchmarks, especially with\nhigh-resolution images as inputs. The dataset and code are publicly available\nat https://github.com/baaivision/DenseFusion.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08303",
        "PDF Link": "https://arxiv.org/pdf/2407.08303",
        "Upvotes": "16",
        "Date": "2024-07-16"
    },
    {
        "Title": "MambaVision: A Hybrid Mamba-Transformer Vision Backbone",
        "Abstract": "We propose a novel hybrid Mamba-Transformer backbone, denoted as MambaVision,\nwhich is specifically tailored for vision applications. Our core contribution\nincludes redesigning the Mamba formulation to enhance its capability for\nefficient modeling of visual features. In addition, we conduct a comprehensive\nablation study on the feasibility of integrating Vision Transformers (ViT) with\nMamba. Our results demonstrate that equipping the Mamba architecture with\nseveral self-attention blocks at the final layers greatly improves the modeling\ncapacity to capture long-range spatial dependencies. Based on our findings, we\nintroduce a family of MambaVision models with a hierarchical architecture to\nmeet various design criteria. For Image classification on ImageNet-1K dataset,\nMambaVision model variants achieve a new State-of-the-Art (SOTA) performance in\nterms of Top-1 accuracy and image throughput. In downstream tasks such as\nobject detection, instance segmentation and semantic segmentation on MS COCO\nand ADE20K datasets, MambaVision outperforms comparably-sized backbones and\ndemonstrates more favorable performance. Code:\nhttps://github.com/NVlabs/MambaVision.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08083",
        "PDF Link": "https://arxiv.org/pdf/2407.08083",
        "Upvotes": "15",
        "Date": "2024-07-16"
    },
    {
        "Title": "SEED-Story: Multimodal Long Story Generation with Large Language Model",
        "Abstract": "With the remarkable advancements in image generation and open-form text\ngeneration, the creation of interleaved image-text content has become an\nincreasingly intriguing field. Multimodal story generation, characterized by\nproducing narrative texts and vivid images in an interleaved manner, has\nemerged as a valuable and practical task with broad applications. However, this\ntask poses significant challenges, as it necessitates the comprehension of the\ncomplex interplay between texts and images, and the ability to generate long\nsequences of coherent, contextually relevant texts and visuals. In this work,\nwe propose SEED-Story, a novel method that leverages a Multimodal Large\nLanguage Model (MLLM) to generate extended multimodal stories. Our model, built\nupon the powerful comprehension capability of MLLM, predicts text tokens as\nwell as visual tokens, which are subsequently processed with an adapted visual\nde-tokenizer to produce images with consistent characters and styles. We\nfurther propose multimodal attention sink mechanism to enable the generation of\nstories with up to 25 sequences (only 10 for training) in a highly efficient\nautoregressive manner. Additionally, we present a large-scale and\nhigh-resolution dataset named StoryStream for training our model and\nquantitatively evaluating the task of multimodal story generation in various\naspects.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08683",
        "PDF Link": "https://arxiv.org/pdf/2407.08683",
        "Upvotes": "14",
        "Date": "2024-07-16"
    },
    {
        "Title": "Autoregressive Speech Synthesis without Vector Quantization",
        "Abstract": "We present MELLE, a novel continuous-valued tokens based language modeling\napproach for text to speech synthesis (TTS). MELLE autoregressively generates\ncontinuous mel-spectrogram frames directly from text condition, bypassing the\nneed for vector quantization, which are originally designed for audio\ncompression and sacrifice fidelity compared to mel-spectrograms. Specifically,\n(i) instead of cross-entropy loss, we apply regression loss with a proposed\nspectrogram flux loss function to model the probability distribution of the\ncontinuous-valued tokens. (ii) we have incorporated variational inference into\nMELLE to facilitate sampling mechanisms, thereby enhancing the output diversity\nand model robustness. Experiments demonstrate that, compared to the two-stage\ncodec language models VALL-E and its variants, the single-stage MELLE mitigates\nrobustness issues by avoiding the inherent flaws of sampling discrete codes,\nachieves superior performance across multiple metrics, and, most importantly,\noffers a more streamlined paradigm. See https://aka.ms/melle for demos of our\nwork.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08551",
        "PDF Link": "https://arxiv.org/pdf/2407.08551",
        "Upvotes": "12",
        "Date": "2024-07-16"
    },
    {
        "Title": "The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective",
        "Abstract": "The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs, on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stage of MLLMs can\nspecific data-centric approaches be employed to enhance which capabilities, and\n2) by utilizing which capabilities and acting as which roles can models\ncontribute to multi-modal data. To promote the data-model co-development for\nMLLM community, we systematically review existing works related to MLLMs from\nthe data-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08583",
        "PDF Link": "https://arxiv.org/pdf/2407.08583",
        "Upvotes": "10",
        "Date": "2024-07-16"
    },
    {
        "Title": "Gradient Boosting Reinforcement Learning",
        "Abstract": "Neural networks (NN) achieve remarkable results in various tasks, but lack\nkey characteristics: interpretability, support for categorical features, and\nlightweight implementations suitable for edge devices. While ongoing efforts\naim to address these challenges, Gradient Boosting Trees (GBT) inherently meet\nthese requirements. As a result, GBTs have become the go-to method for\nsupervised learning tasks in many real-world applications and competitions.\nHowever, their application in online learning scenarios, notably in\nreinforcement learning (RL), has been limited. In this work, we bridge this gap\nby introducing Gradient-Boosting RL (GBRL), a framework that extends the\nadvantages of GBT to the RL domain. Using the GBRL framework, we implement\nvarious actor-critic algorithms and compare their performance with their NN\ncounterparts. Inspired by shared backbones in NN we introduce a tree-sharing\napproach for policy and value functions with distinct learning rates, enhancing\nlearning efficiency over millions of interactions. GBRL achieves competitive\nperformance across a diverse array of tasks, excelling in domains with\nstructured or categorical features. Additionally, we present a\nhigh-performance, GPU-accelerated implementation that integrates seamlessly\nwith widely-used RL libraries (available at https://github.com/NVlabs/gbrl).\nGBRL expands the toolkit for RL practitioners, demonstrating the viability and\npromise of GBT within the RL paradigm, particularly in domains characterized by\nstructured or categorical features.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08250",
        "PDF Link": "https://arxiv.org/pdf/2407.08250",
        "Upvotes": "10",
        "Date": "2024-07-16"
    },
    {
        "Title": "Map It Anywhere (MIA): Empowering Bird's Eye View Mapping using\n  Large-scale Public Data",
        "Abstract": "Top-down Bird's Eye View (BEV) maps are a popular representation for ground\nrobot navigation due to their richness and flexibility for downstream tasks.\nWhile recent methods have shown promise for predicting BEV maps from\nFirst-Person View (FPV) images, their generalizability is limited to small\nregions captured by current autonomous vehicle-based datasets. In this context,\nwe show that a more scalable approach towards generalizable map prediction can\nbe enabled by using two large-scale crowd-sourced mapping platforms, Mapillary\nfor FPV images and OpenStreetMap for BEV semantic maps. We introduce Map It\nAnywhere (MIA), a data engine that enables seamless curation and modeling of\nlabeled map prediction data from existing open-source map platforms. Using our\nMIA data engine, we display the ease of automatically collecting a dataset of\n1.2 million pairs of FPV images & BEV maps encompassing diverse geographies,\nlandscapes, environmental factors, camera models & capture scenarios. We\nfurther train a simple camera model-agnostic model on this data for BEV map\nprediction. Extensive evaluations using established benchmarks and our dataset\nshow that the data curated by MIA enables effective pretraining for\ngeneralizable BEV map prediction, with zero-shot performance far exceeding\nbaselines trained on existing datasets by 35%. Our analysis highlights the\npromise of using large-scale public maps for developing & testing generalizable\nBEV perception, paving the way for more robust autonomous navigation.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08726",
        "PDF Link": "https://arxiv.org/pdf/2407.08726",
        "Upvotes": "8",
        "Date": "2024-07-16"
    },
    {
        "Title": "GTA: A Benchmark for General Tool Agents",
        "Abstract": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08713",
        "PDF Link": "https://arxiv.org/pdf/2407.08713",
        "Upvotes": "8",
        "Date": "2024-07-16"
    },
    {
        "Title": "Towards Building Specialized Generalist AI with System 1 and System 2\n  Fusion",
        "Abstract": "In this perspective paper, we introduce the concept of Specialized Generalist\nArtificial Intelligence (SGAI or simply SGI) as a crucial milestone toward\nArtificial General Intelligence (AGI). Compared to directly scaling general\nabilities, SGI is defined as AI that specializes in at least one task,\nsurpassing human experts, while also retaining general abilities. This fusion\npath enables SGI to rapidly achieve high-value areas. We categorize SGI into\nthree stages based on the level of mastery over professional skills and\ngenerality performance. Additionally, we discuss the necessity of SGI in\naddressing issues associated with large language models, such as their\ninsufficient generality, specialized capabilities, uncertainty in innovation,\nand practical applications. Furthermore, we propose a conceptual framework for\ndeveloping SGI that integrates the strengths of Systems 1 and 2 cognitive\nprocessing. This framework comprises three layers and four key components,\nwhich focus on enhancing individual abilities and facilitating collaborative\nevolution. We conclude by summarizing the potential challenges and suggesting\nfuture directions. We hope that the proposed SGI will provide insights into\nfurther research and applications towards achieving AGI.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08642",
        "PDF Link": "https://arxiv.org/pdf/2407.08642",
        "Upvotes": "7",
        "Date": "2024-07-16"
    },
    {
        "Title": "Live2Diff: Live Stream Translation via Uni-directional Attention in\n  Video Diffusion Models",
        "Abstract": "Large Language Models have shown remarkable efficacy in generating streaming\ndata such as text and audio, thanks to their temporally uni-directional\nattention mechanism, which models correlations between the current token and\nprevious tokens. However, video streaming remains much less explored, despite a\ngrowing need for live video processing. State-of-the-art video diffusion models\nleverage bi-directional temporal attention to model the correlations between\nthe current frame and all the surrounding (i.e. including future) frames, which\nhinders them from processing streaming videos. To address this problem, we\npresent Live2Diff, the first attempt at designing a video diffusion model with\nuni-directional temporal attention, specifically targeting live streaming video\ntranslation. Compared to previous works, our approach ensures temporal\nconsistency and smoothness by correlating the current frame with its\npredecessors and a few initial warmup frames, without any future frames.\nAdditionally, we use a highly efficient denoising scheme featuring a KV-cache\nmechanism and pipelining, to facilitate streaming video translation at\ninteractive framerates. Extensive experiments demonstrate the effectiveness of\nthe proposed attention mechanism and pipeline, outperforming previous methods\nin terms of temporal smoothness and/or efficiency.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08701",
        "PDF Link": "https://arxiv.org/pdf/2407.08701",
        "Upvotes": "7",
        "Date": "2024-07-16"
    },
    {
        "Title": "Generalizable Implicit Motion Modeling for Video Frame Interpolation",
        "Abstract": "Motion modeling is critical in flow-based Video Frame Interpolation (VFI).\nExisting paradigms either consider linear combinations of bidirectional flows\nor directly predict bilateral flows for given timestamps without exploring\nfavorable motion priors, thus lacking the capability of effectively modeling\nspatiotemporal dynamics in real-world videos. To address this limitation, in\nthis study, we introduce Generalizable Implicit Motion Modeling (GIMM), a novel\nand effective approach to motion modeling for VFI. Specifically, to enable GIMM\nas an effective motion modeling paradigm, we design a motion encoding pipeline\nto model spatiotemporal motion latent from bidirectional flows extracted from\npre-trained flow estimators, effectively representing input-specific motion\npriors. Then, we implicitly predict arbitrary-timestep optical flows within two\nadjacent input frames via an adaptive coordinate-based neural network, with\nspatiotemporal coordinates and motion latent as inputs. Our GIMM can be\nsmoothly integrated with existing flow-based VFI works without further\nmodifications. We show that GIMM performs better than the current state of the\nart on the VFI benchmarks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08680",
        "PDF Link": "https://arxiv.org/pdf/2407.08680",
        "Upvotes": "7",
        "Date": "2024-07-16"
    },
    {
        "Title": "WildGaussians: 3D Gaussian Splatting in the Wild",
        "Abstract": "While the field of 3D scene reconstruction is dominated by NeRFs due to their\nphotorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,\noffering similar quality with real-time rendering speeds. However, both methods\nprimarily excel with well-controlled 3D scenes, while in-the-wild data -\ncharacterized by occlusions, dynamic objects, and varying illumination -\nremains challenging. NeRFs can adapt to such conditions easily through\nper-image embedding vectors, but 3DGS struggles due to its explicit\nrepresentation and lack of shared parameters. To address this, we introduce\nWildGaussians, a novel approach to handle occlusions and appearance changes\nwith 3DGS. By leveraging robust DINO features and integrating an appearance\nmodeling module within 3DGS, our method achieves state-of-the-art results. We\ndemonstrate that WildGaussians matches the real-time rendering speed of 3DGS\nwhile surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all\nwithin a simple architectural framework.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08447",
        "PDF Link": "https://arxiv.org/pdf/2407.08447",
        "Upvotes": "6",
        "Date": "2024-07-16"
    },
    {
        "Title": "OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects",
        "Abstract": "We propose OmniNOCS, a large-scale monocular dataset with 3D Normalized\nObject Coordinate Space (NOCS) maps, object masks, and 3D bounding box\nannotations for indoor and outdoor scenes. OmniNOCS has 20 times more object\nclasses and 200 times more instances than existing NOCS datasets (NOCS-Real275,\nWild6D). We use OmniNOCS to train a novel, transformer-based monocular NOCS\nprediction model (NOCSformer) that can predict accurate NOCS, instance masks\nand poses from 2D object detections across diverse classes. It is the first\nNOCS model that can generalize to a broad range of classes when prompted with\n2D boxes. We evaluate our model on the task of 3D oriented bounding box\nprediction, where it achieves comparable results to state-of-the-art 3D\ndetection methods such as Cube R-CNN. Unlike other 3D detection methods, our\nmodel also provides detailed and accurate 3D object shape and segmentation. We\npropose a novel benchmark for the task of NOCS prediction based on OmniNOCS,\nwhich we hope will serve as a useful baseline for future work in this area. Our\ndataset and code will be at the project website: https://omninocs.github.io.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08711",
        "PDF Link": "https://arxiv.org/pdf/2407.08711",
        "Upvotes": "5",
        "Date": "2024-07-16"
    },
    {
        "Title": "Scaling Up Personalized Aesthetic Assessment via Task Vector\n  Customization",
        "Abstract": "The task of personalized image aesthetic assessment seeks to tailor aesthetic\nscore prediction models to match individual preferences with just a few\nuser-provided inputs. However, the scalability and generalization capabilities\nof current approaches are considerably restricted by their reliance on an\nexpensive curated database. To overcome this long-standing scalability\nchallenge, we present a unique approach that leverages readily available\ndatabases for general image aesthetic assessment and image quality assessment.\nSpecifically, we view each database as a distinct image score regression task\nthat exhibits varying degrees of personalization potential. By determining\noptimal combinations of task vectors, known to represent specific traits of\neach database, we successfully create personalized models for individuals. This\napproach of integrating multiple models allows us to harness a substantial\namount of data. Our extensive experiments demonstrate the effectiveness of our\napproach in generalizing to previously unseen domains-a challenge previous\napproaches have struggled to achieve-making it highly applicable to real-world\nscenarios. Our novel approach significantly advances the field by offering\nscalable solutions for personalized aesthetic assessment and establishing high\nstandards for future research.\nhttps://yeolj00.github.io/personal-projects/personalized-aesthetics/",
        "ArXiv Link": "https://arxiv.org/abs/2407.07176",
        "PDF Link": "https://arxiv.org/pdf/2407.07176",
        "Upvotes": "3",
        "Date": "2024-07-16"
    },
    {
        "Title": "PaliGemma: A versatile 3B VLM for transfer",
        "Abstract": "PaliGemma is an open Vision-Language Model (VLM) that is based on the\nSigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to\nbe a versatile and broadly knowledgeable base model that is effective to\ntransfer. It achieves strong performance on a wide variety of open-world tasks.\nWe evaluate PaliGemma on almost 40 diverse tasks including standard VLM\nbenchmarks, but also more specialized tasks such as remote-sensing and\nsegmentation.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07726",
        "PDF Link": "https://arxiv.org/pdf/2407.07726",
        "Upvotes": "54",
        "Date": "2024-07-16"
    },
    {
        "Title": "Inference Performance Optimization for Large Language Models on CPUs",
        "Abstract": "Large language models (LLMs) have shown exceptional performance and vast\npotential across diverse tasks. However, the deployment of LLMs with high\nperformance in low-resource environments has garnered significant attention in\nthe industry. When GPU hardware resources are limited, we can explore\nalternative options on CPUs. To mitigate the financial burden and alleviate\nconstraints imposed by hardware resources, optimizing inference performance is\nnecessary. In this paper, we introduce an easily deployable inference\nperformance optimization solution aimed at accelerating LLMs on CPUs. In this\nsolution, we implement an effective way to reduce the KV cache size while\nensuring precision. We propose a distributed inference optimization approach\nand implement it based on oneAPI Collective Communications Library.\nFurthermore, we propose optimization approaches for LLMs on CPU, and conduct\ntailored optimizations for the most commonly used models. The code is\nopen-sourced at https://github.com/intel/xFasterTransformer.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07304",
        "PDF Link": "https://arxiv.org/pdf/2407.07304",
        "Upvotes": "46",
        "Date": "2024-07-16"
    },
    {
        "Title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large\n  Multimodal Models",
        "Abstract": "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT",
        "ArXiv Link": "https://arxiv.org/abs/2407.07895",
        "PDF Link": "https://arxiv.org/pdf/2407.07895",
        "Upvotes": "33",
        "Date": "2024-07-16"
    },
    {
        "Title": "Controlling Space and Time with Diffusion Models",
        "Abstract": "We present 4DiM, a cascaded diffusion model for 4D novel view synthesis\n(NVS), conditioned on one or more images of a general scene, and a set of\ncamera poses and timestamps. To overcome challenges due to limited availability\nof 4D training data, we advocate joint training on 3D (with camera pose), 4D\n(pose+time) and video (time but no pose) data and propose a new architecture\nthat enables the same. We further advocate the calibration of SfM posed data\nusing monocular metric depth estimators for metric scale camera control. For\nmodel evaluation, we introduce new metrics to enrich and overcome shortcomings\nof current evaluation schemes, demonstrating state-of-the-art results in both\nfidelity and pose control compared to existing diffusion models for 3D NVS,\nwhile at the same time adding the ability to handle temporal dynamics. 4DiM is\nalso used for improved panorama stitching, pose-conditioned video to video\ntranslation, and several other tasks. For an overview see\nhttps://4d-diffusion.github.io",
        "ArXiv Link": "https://arxiv.org/abs/2407.07860",
        "PDF Link": "https://arxiv.org/pdf/2407.07860",
        "Upvotes": "15",
        "Date": "2024-07-16"
    },
    {
        "Title": "Video-to-Audio Generation with Hidden Alignment",
        "Abstract": "Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model VTA-LDM built\non a simple yet surprisingly effective intuition, we explore various vision\nencoders and auxiliary embeddings through ablation studies. Employing a\ncomprehensive evaluation pipeline that emphasizes generation quality and\nvideo-audio synchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07464",
        "PDF Link": "https://arxiv.org/pdf/2407.07464",
        "Upvotes": "11",
        "Date": "2024-07-16"
    },
    {
        "Title": "VEnhancer: Generative Space-Time Enhancement for Video Generation",
        "Abstract": "We present VEnhancer, a generative space-time enhancement framework that\nimproves the existing text-to-video results by adding more details in spatial\ndomain and synthetic detailed motion in temporal domain. Given a generated\nlow-quality video, our approach can increase its spatial and temporal\nresolution simultaneously with arbitrary up-sampling space and time scales\nthrough a unified video diffusion model. Furthermore, VEnhancer effectively\nremoves generated spatial artifacts and temporal flickering of generated\nvideos. To achieve this, basing on a pretrained video diffusion model, we train\na video ControlNet and inject it to the diffusion model as a condition on low\nframe-rate and low-resolution videos. To effectively train this video\nControlNet, we design space-time data augmentation as well as video-aware\nconditioning. Benefiting from the above designs, VEnhancer yields to be stable\nduring training and shares an elegant end-to-end training manner. Extensive\nexperiments show that VEnhancer surpasses existing state-of-the-art video\nsuper-resolution and space-time super-resolution methods in enhancing\nAI-generated videos. Moreover, with VEnhancer, exisiting open-source\nstate-of-the-art text-to-video method, VideoCrafter-2, reaches the top one in\nvideo generation benchmark -- VBench.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07667",
        "PDF Link": "https://arxiv.org/pdf/2407.07667",
        "Upvotes": "8",
        "Date": "2024-07-16"
    },
    {
        "Title": "Still-Moving: Customized Video Generation without Customized Video Data",
        "Abstract": "Customizing text-to-image (T2I) models has seen tremendous progress recently,\nparticularly in areas such as personalization, stylization, and conditional\ngeneration. However, expanding this progress to video generation is still in\nits infancy, primarily due to the lack of customized video data. In this work,\nwe introduce Still-Moving, a novel generic framework for customizing a\ntext-to-video (T2V) model, without requiring any customized video data. The\nframework applies to the prominent T2V design where the video model is built\nover a text-to-image (T2I) model (e.g., via inflation). We assume access to a\ncustomized version of the T2I model, trained only on still image data (e.g.,\nusing DreamBooth or StyleDrop). Naively plugging in the weights of the\ncustomized T2I model into the T2V model often leads to significant artifacts or\ninsufficient adherence to the customization data. To overcome this issue, we\ntrain lightweight Spatial Adapters that adjust the features produced\nby the injected T2I layers. Importantly, our adapters are trained on\n\"frozen videos\" (i.e., repeated images), constructed from image\nsamples generated by the customized T2I model. This training is facilitated by\na novel Motion Adapter module, which allows us to train on such\nstatic videos while preserving the motion prior of the video model. At test\ntime, we remove the Motion Adapter modules and leave in only the trained\nSpatial Adapters. This restores the motion prior of the T2V model while\nadhering to the spatial prior of the customized T2I model. We demonstrate the\neffectiveness of our approach on diverse tasks including personalized,\nstylized, and conditional generation. In all evaluated scenarios, our method\nseamlessly integrates the spatial prior of the customized T2I model with a\nmotion prior supplied by the T2V model.",
        "ArXiv Link": "https://arxiv.org/abs/2407.08674",
        "PDF Link": "https://arxiv.org/pdf/2407.08674",
        "Upvotes": "7",
        "Date": "2024-07-16"
    },
    {
        "Title": "Do Vision and Language Models Share Concepts? A Vector Space Alignment\n  Study",
        "Abstract": "Large-scale pretrained language models (LMs) are said to ``lack the ability\nto connect utterances to the world'' (Bender and Koller, 2020), because they do\nnot have ``mental models of the world' '(Mitchell and Krakauer, 2023). If so,\none would expect LM representations to be unrelated to representations induced\nby vision models. We present an empirical evaluation across four families of\nLMs (BERT, GPT-2, OPT and LLaMA-2) and three vision model architectures\n(ResNet, SegFormer, and MAE). Our experiments show that LMs partially converge\ntowards representations isomorphic to those of vision models, subject to\ndispersion, polysemy and frequency. This has important implications for both\nmulti-modal processing and the LM understanding debate (Mitchell and Krakauer,\n2023).",
        "ArXiv Link": "https://arxiv.org/abs/2302.06555",
        "PDF Link": "https://arxiv.org/pdf/2302.06555",
        "Upvotes": "7",
        "Date": "2024-07-16"
    },
    {
        "Title": "On Leakage of Code Generation Evaluation Datasets",
        "Abstract": "In this paper we consider contamination by code generation test sets, in\nparticular in their use in modern large language models. We discuss three\npossible sources of such contamination and show findings supporting each of\nthem: (i) direct data leakage, (ii) indirect data leakage through the use of\nsynthetic data and (iii) overfitting to evaluation sets during model selection.\n  Key to our findings is a new dataset of 161 prompts with their associated\npython solutions, dataset which is released at\nhttps://huggingface.co/datasets/CohereForAI/lbpp .",
        "ArXiv Link": "https://arxiv.org/abs/2407.07565",
        "PDF Link": "https://arxiv.org/pdf/2407.07565",
        "Upvotes": "4",
        "Date": "2024-07-16"
    },
    {
        "Title": "CosmoCLIP: Generalizing Large Vision-Language Models for Astronomical\n  Imaging",
        "Abstract": "Existing vision-text contrastive learning models enhance representation\ntransferability and support zero-shot prediction by matching paired image and\ncaption embeddings while pushing unrelated pairs apart. However, astronomical\nimage-label datasets are significantly smaller compared to general image and\nlabel datasets available from the internet. We introduce CosmoCLIP, an\nastronomical image-text contrastive learning framework precisely fine-tuned on\nthe pre-trained CLIP model using SpaceNet and BLIP-based captions. SpaceNet,\nattained via FLARE, constitutes ~13k optimally distributed images, while BLIP\nacts as a rich knowledge extractor. The rich semantics derived from this\nSpaceNet and BLIP descriptions, when learned contrastively, enable CosmoCLIP to\nachieve superior generalization across various in-domain and out-of-domain\ntasks. Our results demonstrate that CosmoCLIP is a straightforward yet powerful\nframework, significantly outperforming CLIP in zero-shot classification and\nimage-text retrieval tasks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07315",
        "PDF Link": "https://arxiv.org/pdf/2407.07315",
        "Upvotes": "4",
        "Date": "2024-07-16"
    },
    {
        "Title": "This&That: Language-Gesture Controlled Video Generation for Robot\n  Planning",
        "Abstract": "We propose a robot learning method for communicating, planning, and executing\na wide range of tasks, dubbed This&That. We achieve robot planning for general\ntasks by leveraging the power of video generative models trained on\ninternet-scale data containing rich physical and semantic context. In this\nwork, we tackle three fundamental challenges in video-based planning: 1)\nunambiguous task communication with simple human instructions, 2) controllable\nvideo generation that respects user intents, and 3) translating visual planning\ninto robot actions. We propose language-gesture conditioning to generate\nvideos, which is both simpler and clearer than existing language-only methods,\nespecially in complex and uncertain environments. We then suggest a behavioral\ncloning design that seamlessly incorporates the video plans. This&That\ndemonstrates state-of-the-art effectiveness in addressing the above three\nchallenges, and justifies the use of video generation as an intermediate\nrepresentation for generalizable task planning and execution. Project website:\nhttps://cfeng16.github.io/this-and-that/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.05530",
        "PDF Link": "https://arxiv.org/pdf/2407.05530",
        "Upvotes": "3",
        "Date": "2024-07-16"
    },
    {
        "Title": "An accurate detection is not all you need to combat label noise in\n  web-noisy datasets",
        "Abstract": "Training a classifier on web-crawled data demands learning algorithms that\nare robust to annotation errors and irrelevant examples. This paper builds upon\nthe recent empirical observation that applying unsupervised contrastive\nlearning to noisy, web-crawled datasets yields a feature representation under\nwhich the in-distribution (ID) and out-of-distribution (OOD) samples are\nlinearly separable. We show that direct estimation of the separating hyperplane\ncan indeed offer an accurate detection of OOD samples, and yet, surprisingly,\nthis detection does not translate into gains in classification accuracy.\nDigging deeper into this phenomenon, we discover that the near-perfect\ndetection misses a type of clean examples that are valuable for supervised\nlearning. These examples often represent visually simple images, which are\nrelatively easy to identify as clean examples using standard loss- or\ndistance-based methods despite being poorly separated from the OOD distribution\nusing unsupervised learning. Because we further observe a low correlation with\nSOTA metrics, this urges us to propose a hybrid solution that alternates\nbetween noise detection using linear separation and a state-of-the-art (SOTA)\nsmall-loss approach. When combined with the SOTA algorithm PLS, we\nsubstantially improve SOTA results for real-world image classification in the\npresence of web noise github.com/PaulAlbert31/LSA",
        "ArXiv Link": "https://arxiv.org/abs/2407.05528",
        "PDF Link": "https://arxiv.org/pdf/2407.05528",
        "Upvotes": "2",
        "Date": "2024-07-16"
    },
    {
        "Title": "CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation",
        "Abstract": "Crowd Motion Generation is essential in entertainment industries such as\nanimation and games as well as in strategic fields like urban simulation and\nplanning. This new task requires an intricate integration of control and\ngeneration to realistically synthesize crowd dynamics under specific spatial\nand semantic constraints, whose challenges are yet to be fully explored. On the\none hand, existing human motion generation models typically focus on individual\nbehaviors, neglecting the complexities of collective behaviors. On the other\nhand, recent methods for multi-person motion generation depend heavily on\npre-defined scenarios and are limited to a fixed, small number of inter-person\ninteractions, thus hampering their practicality. To overcome these challenges,\nwe introduce CrowdMoGen, a zero-shot text-driven framework that harnesses the\npower of Large Language Model (LLM) to incorporate the collective intelligence\ninto the motion generation framework as guidance, thereby enabling\ngeneralizable planning and generation of crowd motions without paired training\ndata. Our framework consists of two key components: 1) Crowd Scene Planner that\nlearns to coordinate motions and dynamics according to specific scene contexts\nor introduced perturbations, and 2) Collective Motion Generator that\nefficiently synthesizes the required collective motions based on the holistic\nplans. Extensive quantitative and qualitative experiments have validated the\neffectiveness of our framework, which not only fills a critical gap by\nproviding scalable and generalizable solutions for Crowd Motion Generation task\nbut also achieves high levels of realism and flexibility.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06188",
        "PDF Link": "https://arxiv.org/pdf/2407.06188",
        "Upvotes": "1",
        "Date": "2024-07-16"
    },
    {
        "Title": "BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark",
        "Abstract": "We introduce BiGym, a new benchmark and learning environment for mobile\nbi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set\nin home environments, ranging from simple target reaching to complex kitchen\ncleaning. To capture the real-world performance accurately, we provide\nhuman-collected demonstrations for each task, reflecting the diverse modalities\nfound in real-world robot trajectories. BiGym supports a variety of\nobservations, including proprioceptive data and visual inputs such as RGB, and\ndepth from 3 camera views. To validate the usability of BiGym, we thoroughly\nbenchmark the state-of-the-art imitation learning algorithms and demo-driven\nreinforcement learning algorithms within the environment and discuss the future\nopportunities.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07788",
        "PDF Link": "https://arxiv.org/pdf/2407.07788",
        "Upvotes": "1",
        "Date": "2024-07-16"
    },
    {
        "Title": "Vision language models are blind",
        "Abstract": "Large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro are powering countless image-text applications and scoring high\non many vision-understanding benchmarks. Yet, we find that VLMs fail on 7\nvisual tasks absurdly easy to humans such as identifying (a) whether two\ncircles overlap; (b) whether two lines intersect; (c) which letter is being\ncircled in a word; and (d) counting the number of circles in a Olympic-like\nlogo. The shockingly poor performance of four state-of-the-art VLMs suggests\ntheir vision is, at best, like of a person with myopia seeing fine details as\nblurry, and at worst, like an intelligent person that is blind making educated\nguesses. Code is available at: https://vlmsareblind.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2407.06581",
        "PDF Link": "https://arxiv.org/pdf/2407.06581",
        "Upvotes": "69",
        "Date": "2024-07-16"
    },
    {
        "Title": "AgentInstruct: Toward Generative Teaching with Agentic Flows",
        "Abstract": "Synthetic data is becoming increasingly important for accelerating the\ndevelopment of language models, both large and small. Despite several\nsuccessful use cases, researchers also raised concerns around model collapse\nand drawbacks of imitating other models. This discrepancy can be attributed to\nthe fact that synthetic data varies in quality and diversity. Effective use of\nsynthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data\nby powerful models to teach a new skill or behavior to another model, we refer\nto this setting as Generative Teaching. We introduce AgentInstruct, an\nextensible agentic framework for automatically creating large amounts of\ndiverse and high-quality synthetic data. AgentInstruct can create both the\nprompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post\ntraining dataset of 25M pairs to teach language models different skills, such\nas text editing, creative writing, tool usage, coding, reading comprehension,\netc. The dataset can be used for instruction tuning of any base model. We\npost-train Mistral-7b with the data. When comparing the resulting model Orca-3\nto Mistral-7b-Instruct (which uses the same base model), we observe significant\nimprovements across many benchmarks. For example, 40% improvement on AGIEval,\n19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and\n45% improvement on AlpacaEval. Additionally, it consistently outperforms other\nmodels such as LLAMA-8B-instruct and GPT-3.5-turbo.",
        "ArXiv Link": "https://arxiv.org/abs/2407.03502",
        "PDF Link": "https://arxiv.org/pdf/2407.03502",
        "Upvotes": "31",
        "Date": "2024-07-16"
    },
    {
        "Title": "Video-STaR: Self-Training Enables Video Instruction Tuning with Any\n  Supervision",
        "Abstract": "The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06189",
        "PDF Link": "https://arxiv.org/pdf/2407.06189",
        "Upvotes": "24",
        "Date": "2024-07-16"
    },
    {
        "Title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for\n  Collaborative Intelligence",
        "Abstract": "The rapid advancement of large language models (LLMs) has paved the way for\nthe development of highly capable autonomous agents. However, existing\nmulti-agent frameworks often struggle with integrating diverse capable\nthird-party agents due to reliance on agents defined within their own\necosystems. They also face challenges in simulating distributed environments,\nas most frameworks are limited to single-device setups. Furthermore, these\nframeworks often rely on hard-coded communication pipelines, limiting their\nadaptability to dynamic task requirements. Inspired by the concept of the\nInternet, we propose the Internet of Agents (IoA), a novel framework that\naddresses these limitations by providing a flexible and scalable platform for\nLLM-based multi-agent collaboration. IoA introduces an agent integration\nprotocol, an instant-messaging-like architecture design, and dynamic mechanisms\nfor agent teaming and conversation flow control. Through extensive experiments\non general assistant tasks, embodied AI tasks, and retrieval-augmented\ngeneration benchmarks, we demonstrate that IoA consistently outperforms\nstate-of-the-art baselines, showcasing its ability to facilitate effective\ncollaboration among heterogeneous agents. IoA represents a step towards linking\ndiverse agents in an Internet-like environment, where agents can seamlessly\ncollaborate to achieve greater intelligence and capabilities. Our codebase has\nbeen released at https://github.com/OpenBMB/IoA.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07061",
        "PDF Link": "https://arxiv.org/pdf/2407.07061",
        "Upvotes": "20",
        "Date": "2024-07-16"
    },
    {
        "Title": "Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary\n  and Instruction Capabilities",
        "Abstract": "Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07080",
        "PDF Link": "https://arxiv.org/pdf/2407.07080",
        "Upvotes": "18",
        "Date": "2024-07-16"
    },
    {
        "Title": "RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models",
        "Abstract": "We present RodinHD, which can generate high-fidelity 3D avatars from a\nportrait image. Existing methods fail to capture intricate details such as\nhairstyles which we tackle in this paper. We first identify an overlooked\nproblem of catastrophic forgetting that arises when fitting triplanes\nsequentially on many avatars, caused by the MLP decoder sharing scheme. To\novercome this issue, we raise a novel data scheduling strategy and a weight\nconsolidation regularization term, which improves the decoder's capability of\nrendering sharper details. Additionally, we optimize the guiding effect of the\nportrait image by computing a finer-grained hierarchical representation that\ncaptures rich 2D texture cues, and injecting them to the 3D diffusion model at\nmultiple layers via cross-attention. When trained on 46K avatars with a noise\nschedule optimized for triplanes, the resulting model can generate 3D avatars\nwith notably better details than previous methods and can generalize to\nin-the-wild portrait input.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06938",
        "PDF Link": "https://arxiv.org/pdf/2407.06938",
        "Upvotes": "18",
        "Date": "2024-07-16"
    },
    {
        "Title": "MiraData: A Large-Scale Video Dataset with Long Durations and Structured\n  Captions",
        "Abstract": "Sora's high-motion intensity and long consistent videos have significantly\nimpacted the field of video generation, attracting unprecedented attention.\nHowever, existing publicly available datasets are inadequate for generating\nSora-like videos, as they mainly contain short videos with low motion intensity\nand brief captions. To address these issues, we propose MiraData, a\nhigh-quality video dataset that surpasses previous ones in video duration,\ncaption detail, motion strength, and visual quality. We curate MiraData from\ndiverse, manually selected sources and meticulously process the data to obtain\nsemantically consistent clips. GPT-4V is employed to annotate structured\ncaptions, providing detailed descriptions from four different perspectives\nalong with a summarized dense caption. To better assess temporal consistency\nand motion intensity in video generation, we introduce MiraBench, which\nenhances existing benchmarks by adding 3D consistency and tracking-based motion\nstrength metrics. MiraBench includes 150 evaluation prompts and 17 metrics\ncovering temporal consistency, motion strength, 3D consistency, visual quality,\ntext-video alignment, and distribution similarity. To demonstrate the utility\nand effectiveness of MiraData, we conduct experiments using our DiT-based video\ngeneration model, MiraDiT. The experimental results on MiraBench demonstrate\nthe superiority of MiraData, especially in motion strength.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06358",
        "PDF Link": "https://arxiv.org/pdf/2407.06358",
        "Upvotes": "14",
        "Date": "2024-07-16"
    },
    {
        "Title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps",
        "Abstract": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.",
        "ArXiv Link": "https://arxiv.org/abs/2407.07071",
        "PDF Link": "https://arxiv.org/pdf/2407.07071",
        "Upvotes": "10",
        "Date": "2024-07-16"
    },
    {
        "Title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts",
        "Abstract": "Proving mathematical theorems using computer-verifiable formal languages like\nLean significantly impacts mathematical reasoning. One approach to formal\ntheorem proving involves generating complete proofs using Large Language Models\n(LLMs) based on Natural Language (NL) proofs. Similar methods have shown\npromising results in code generation. However, most modern LLMs exhibit\nsuboptimal performance due to the scarcity of aligned NL and Formal Language\n(FL) theorem-proving data. This scarcity results in a paucity of methodologies\nfor training LLMs and techniques to fully utilize their capabilities in\ncomposing formal proofs. To address the challenges, this paper proposes\n**TheoremLlama**, an end-to-end framework to train a general-purpose LLM to\nbecome a Lean4 expert. This framework encompasses NL-FL aligned dataset\ngeneration methods, training approaches for the LLM formal theorem prover, and\ntechniques for LLM Lean4 proof writing. Using the dataset generation method, we\nprovide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped\ndataset. A key innovation in this framework is the NL-FL bootstrapping method,\nwhere NL proofs are integrated into Lean4 code for training datasets,\nleveraging the NL reasoning ability of LLMs for formal reasoning. The\n**TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61%\non MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline\nof 22.95% and 25.41%. We have also open-sourced our model checkpoints and\ngenerated dataset, and will soon make all the code publicly available.",
        "ArXiv Link": "https://arxiv.org/abs/2407.03203",
        "PDF Link": "https://arxiv.org/pdf/2407.03203",
        "Upvotes": "9",
        "Date": "2024-07-16"
    },
    {
        "Title": "Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting\n  Region Captions",
        "Abstract": "Humans describe complex scenes with compositionality, using simple text\ndescriptions enriched with links and relationships. While vision-language\nresearch has aimed to develop models with compositional understanding\ncapabilities, this is not reflected yet in existing datasets which, for the\nmost part, still use plain text to describe images. In this work, we propose a\nnew annotation strategy, graph-based captioning (GBC) that describes an image\nusing a labelled graph structure, with nodes of various types. The nodes in GBC\nare created using, in a first stage, object detection and dense captioning\ntools nested recursively to uncover and describe entity nodes, further linked\ntogether in a second stage by highlighting, using new types of nodes,\ncompositions and relations among entities. Since all GBC nodes hold plain text\ndescriptions, GBC retains the flexibility found in natural language, but can\nalso encode hierarchical information in its edges. We demonstrate that GBC can\nbe produced automatically, using off-the-shelf multimodal LLMs and\nopen-vocabulary detection models, by building a new dataset, GBC10M, gathering\nGBC annotations for about 10M images of the CC12M dataset. We use GBC10M to\nshowcase the wealth of node captions uncovered by GBC, as measured with CLIP\ntraining. We show that using GBC nodes' annotations -- notably those stored in\ncomposition and relation nodes -- results in significant performance boost on\ndownstream models when compared to other dataset formats. To further explore\nthe opportunities provided by GBC, we also propose a new attention mechanism\nthat can leverage the entire GBC graph, with encouraging experimental results\nthat show the extra benefits of incorporating the graph structure. Our datasets\nare released at https://huggingface.co/graph-based-captions.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06723",
        "PDF Link": "https://arxiv.org/pdf/2407.06723",
        "Upvotes": "9",
        "Date": "2024-07-16"
    },
    {
        "Title": "BM25S: Orders of magnitude faster lexical search via eager sparse\n  scoring",
        "Abstract": "We introduce BM25S, an efficient Python-based implementation of BM25 that\nonly depends on Numpy and Scipy. BM25S achieves up to a 500x speedup compared\nto the most popular Python-based framework by eagerly computing BM25 scores\nduring indexing and storing them into sparse matrices. It also achieves\nconsiderable speedups compared to highly optimized Java-based implementations,\nwhich are used by popular commercial products. Finally, BM25S reproduces the\nexact implementation of five BM25 variants based on Kamphuis et al. (2020) by\nextending eager scoring to non-sparse variants using a novel score shifting\nmethod. The code can be found at https://github.com/xhluca/bm25s",
        "ArXiv Link": "https://arxiv.org/abs/2407.03618",
        "PDF Link": "https://arxiv.org/pdf/2407.03618",
        "Upvotes": "8",
        "Date": "2024-07-16"
    },
    {
        "Title": "Knowledge Composition using Task Vectors with Learned Anisotropic\n  Scaling",
        "Abstract": "Pre-trained models produce strong generic representations that can be adapted\nvia fine-tuning. The learned weight difference relative to the pre-trained\nmodel, known as a task vector, characterises the direction and stride of\nfine-tuning. The significance of task vectors is such that simple arithmetic\noperations on them can be used to combine diverse representations from\ndifferent domains. This paper builds on these properties of task vectors and\naims to answer (1) whether components of task vectors, particularly parameter\nblocks, exhibit similar characteristics, and (2) how such blocks can be used to\nenhance knowledge composition and transfer. To this end, we introduce aTLAS, an\nalgorithm that linearly combines parameter blocks with different learned\ncoefficients, resulting in anisotropic scaling at the task vector level. We\nshow that such linear combinations explicitly exploit the low intrinsic\ndimensionality of pre-trained models, with only a few coefficients being the\nlearnable parameters. Furthermore, composition of parameter blocks leverages\nthe already learned representations, thereby reducing the dependency on large\namounts of data. We demonstrate the effectiveness of our method in task\narithmetic, few-shot recognition and test-time adaptation, with supervised or\nunsupervised objectives. In particular, we show that (1) learned anisotropic\nscaling allows task vectors to be more disentangled, causing less interference\nin composition; (2) task vector composition excels with scarce or no labeled\ndata and is less prone to domain shift, thus leading to better\ngeneralisability; (3) mixing the most informative parameter blocks across\ndifferent task vectors prior to training can reduce the memory footprint and\nimprove the flexibility of knowledge transfer. Moreover, we show the potential\nof aTLAS as a PEFT method, particularly with less data, and demonstrate that\nits scalibility.",
        "ArXiv Link": "https://arxiv.org/abs/2407.02880",
        "PDF Link": "https://arxiv.org/pdf/2407.02880",
        "Upvotes": "8",
        "Date": "2024-07-16"
    },
    {
        "Title": "VIMI: Grounding Video Generation through Multi-modal Instruction",
        "Abstract": "Existing text-to-video diffusion models rely solely on text-only encoders for\ntheir pretraining. This limitation stems from the absence of large-scale\nmultimodal prompt video datasets, resulting in a lack of visual grounding and\nrestricting their versatility and application in multimodal integration. To\naddress this, we construct a large-scale multimodal prompt dataset by employing\nretrieval methods to pair in-context examples with the given text prompts and\nthen utilize a two-stage training strategy to enable diverse video generation\ntasks within the same model. In the first stage, we propose a multimodal\nconditional video generation framework for pretraining on these augmented\ndatasets, establishing a foundational model for grounded video generation.\nSecondly, we finetune the model from the first stage on three video generation\ntasks, incorporating multi-modal instructions. This process further refines the\nmodel's ability to handle diverse inputs and tasks, ensuring seamless\nintegration of multi-modal information. After this two-stage train-ing process,\nVIMI demonstrates multimodal understanding capabilities, producing contextually\nrich and personalized videos grounded in the provided inputs, as shown in\nFigure 1. Compared to previous visual grounded video generation methods, VIMI\ncan synthesize consistent and temporally coherent videos with large motion\nwhile retaining the semantic control. Lastly, VIMI also achieves\nstate-of-the-art text-to-video generation results on UCF101 benchmark.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06304",
        "PDF Link": "https://arxiv.org/pdf/2407.06304",
        "Upvotes": "7",
        "Date": "2024-07-16"
    },
    {
        "Title": "From Loops to Oops: Fallback Behaviors of Language Models Under\n  Uncertainty",
        "Abstract": "Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under uncertainty, and investigate the connection\nbetween them. We categorize fallback behaviors -- sequence repetitions,\ndegenerate text, and hallucinations -- and extensively analyze them in models\nfrom the same family that differ by the amount of pretraining tokens, parameter\ncount, or the inclusion of instruction-following training. Our experiments\nreveal a clear and consistent ordering of fallback behaviors, across all these\naxes: the more advanced an LLM is (i.e., trained on more tokens, has more\nparameters, or instruction-tuned), its fallback behavior shifts from sequence\nrepetitions, to degenerate text, and then to hallucinations. Moreover, the same\nordering is observed throughout a single generation, even for the\nbest-performing models; as uncertainty increases, models shift from generating\nhallucinations to producing degenerate text and then sequence repetitions.\nLastly, we demonstrate that while common decoding techniques, such as random\nsampling, might alleviate some unwanted behaviors like sequence repetitions,\nthey increase harder-to-detect hallucinations.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06071",
        "PDF Link": "https://arxiv.org/pdf/2407.06071",
        "Upvotes": "6",
        "Date": "2024-07-16"
    },
    {
        "Title": "How do you know that? Teaching Generative Language Models to Reference\n  Answers to Biomedical Questions",
        "Abstract": "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available.",
        "ArXiv Link": "https://arxiv.org/abs/2407.05015",
        "PDF Link": "https://arxiv.org/pdf/2407.05015",
        "Upvotes": "4",
        "Date": "2024-07-16"
    },
    {
        "Title": "LETS-C: Leveraging Language Embedding for Time Series Classification",
        "Abstract": "Recent advancements in language modeling have shown promising results when\napplied to time series data. In particular, fine-tuning pre-trained large\nlanguage models (LLMs) for time series classification tasks has achieved\nstate-of-the-art (SOTA) performance on standard benchmarks. However, these\nLLM-based models have a significant drawback due to the large model size, with\nthe number of trainable parameters in the millions. In this paper, we propose\nan alternative approach to leveraging the success of language modeling in the\ntime series domain. Instead of fine-tuning LLMs, we utilize a language\nembedding model to embed time series and then pair the embeddings with a simple\nclassification head composed of convolutional neural networks (CNN) and\nmultilayer perceptron (MLP). We conducted extensive experiments on\nwell-established time series classification benchmark datasets. We demonstrated\nLETS-C not only outperforms the current SOTA in classification accuracy but\nalso offers a lightweight solution, using only 14.5% of the trainable\nparameters on average compared to the SOTA model. Our findings suggest that\nleveraging language encoders to embed time series data, combined with a simple\nyet effective classification head, offers a promising direction for achieving\nhigh-performance time series classification while maintaining a lightweight\nmodel architecture.",
        "ArXiv Link": "https://arxiv.org/abs/2407.06533",
        "PDF Link": "https://arxiv.org/pdf/2407.06533",
        "Upvotes": "2",
        "Date": "2024-07-16"
    },
    {
        "Title": "NNsight and NDIF: Democratizing Access to Foundation Model Internals",
        "Abstract": "The enormous scale of state-of-the-art foundation models has limited their\naccessibility to scientists, because customized experiments at large model\nsizes require costly hardware and complex engineering that is impractical for\nmost researchers. To alleviate these problems, we introduce NNsight, an\nopen-source Python package with a simple, flexible API that can express\ninterventions on any PyTorch model by building computation graphs. We also\nintroduce NDIF, a collaborative research platform providing researchers access\nto foundation-scale LLMs via the NNsight API. Code, documentation, and\ntutorials are available at https://www.nnsight.net.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14561",
        "PDF Link": "https://arxiv.org/pdf/2407.14561",
        "Upvotes": "25",
        "Date": "2024-07-24"
    },
    {
        "Title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
        "Abstract": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15017",
        "PDF Link": "https://arxiv.org/pdf/2407.15017",
        "Upvotes": "22",
        "Date": "2024-07-24"
    },
    {
        "Title": "SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language\n  Models",
        "Abstract": "We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video\nlarge language model (LLM) that can jointly capture the detailed spatial\nsemantics and long-range temporal context without exceeding the token budget of\ncommonly used LLMs. This is realized by using a two-stream SlowFast design of\ninputs for Video LLMs to aggregate features from sampled video frames in an\neffective way. Specifically, the Slow pathway extracts features at a low frame\nrate while keeping as many spatial details as possible (e.g., with 24x24\ntokens), and the Fast pathway operates on a high frame rate but uses a larger\nspatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. As\na result, this design allows us to adequately capture both spatial and temporal\nfeatures that are beneficial for understanding details along the video.\nExperimental results show that SF-LLaVA outperforms existing training-free\nmethods on a wide range of video tasks. On some benchmarks, it achieves\ncomparable or even better performance compared to state-of-the-art Video LLMs\nthat are fine-tuned on video datasets.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15841",
        "PDF Link": "https://arxiv.org/pdf/2407.15841",
        "Upvotes": "16",
        "Date": "2024-07-24"
    },
    {
        "Title": "POGEMA: A Benchmark Platform for Cooperative Multi-Agent Navigation",
        "Abstract": "Multi-agent reinforcement learning (MARL) has recently excelled in solving\nchallenging cooperative and competitive multi-agent problems in various\nenvironments with, mostly, few agents and full observability. Moreover, a range\nof crucial robotics-related tasks, such as multi-robot navigation and obstacle\navoidance, that have been conventionally approached with the classical\nnon-learnable methods (e.g., heuristic search) is currently suggested to be\nsolved by the learning-based or hybrid methods. Still, in this domain, it is\nhard, not to say impossible, to conduct a fair comparison between classical,\nlearning-based, and hybrid approaches due to the lack of a unified framework\nthat supports both learning and evaluation. To this end, we introduce POGEMA, a\nset of comprehensive tools that includes a fast environment for learning, a\ngenerator of problem instances, the collection of pre-defined ones, a\nvisualization toolkit, and a benchmarking tool that allows automated\nevaluation. We introduce and specify an evaluation protocol defining a range of\ndomain-related metrics computed on the basics of the primary evaluation\nindicators (such as success rate and path length), allowing a fair multi-fold\ncomparison. The results of such a comparison, which involves a variety of\nstate-of-the-art MARL, search-based, and hybrid methods, are presented.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14931",
        "PDF Link": "https://arxiv.org/pdf/2407.14931",
        "Upvotes": "12",
        "Date": "2024-07-24"
    },
    {
        "Title": "LongVideoBench: A Benchmark for Long-context Interleaved Video-Language\n  Understanding",
        "Abstract": "Large multimodal models (LMMs) are processing increasingly longer and richer\ninputs. Albeit the progress, few public benchmark is available to measure such\ndevelopment. To mitigate this gap, we introduce LongVideoBench, a\nquestion-answering benchmark that features video-language interleaved inputs up\nto an hour long. Our benchmark includes 3,763 varying-length web-collected\nvideos with their subtitles across diverse themes, designed to comprehensively\nevaluate LMMs on long-term multimodal understanding. To achieve this, we\ninterpret the primary challenge as to accurately retrieve and reason over\ndetailed multimodal information from long inputs. As such, we formulate a novel\nvideo question-answering task termed referring reasoning. Specifically, as part\nof the question, it contains a referring query that references related video\ncontexts, called referred context. The model is then required to reason over\nrelevant video details from the referred context. Following the paradigm of\nreferring reasoning, we curate 6,678 human-annotated multiple-choice questions\nin 17 fine-grained categories, establishing one of the most comprehensive\nbenchmarks for long-form video understanding. Evaluations suggest that the\nLongVideoBench presents significant challenges even for the most advanced\nproprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their\nopen-source counterparts show an even larger performance gap. In addition, our\nresults indicate that model performance on the benchmark improves only when\nthey are capable of processing more frames, positioning LongVideoBench as a\nvaluable benchmark for evaluating future-generation long-context LMMs.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15754",
        "PDF Link": "https://arxiv.org/pdf/2407.15754",
        "Upvotes": "11",
        "Date": "2024-07-24"
    },
    {
        "Title": "VideoGameBunny: Towards vision assistants for video games",
        "Abstract": "Large multimodal models (LMMs) hold substantial promise across various\ndomains, from personal assistance in daily tasks to sophisticated applications\nlike medical diagnostics. However, their capabilities have limitations in the\nvideo game domain, such as challenges with scene understanding, hallucinations,\nand inaccurate descriptions of video game content, especially in open-source\nmodels. This paper describes the development of VideoGameBunny, a LLaVA-style\nmodel based on Bunny, specifically tailored for understanding images from video\ngames. We release intermediate checkpoints, training logs, and an extensive\ndataset comprising 185,259 video game images from 413 titles, along with\n389,565 image-instruction pairs that include image captions, question-answer\npairs, and a JSON representation of 16 elements of 136,974 images. Our\nexperiments show that our high quality game-related data has the potential to\nmake a relatively small model outperform the much larger state-of-the-art model\nLLaVa-1.6-34b (which has more than 4x the number of parameters). Our study\npaves the way for future research in video game understanding on tasks such as\nplaying, commentary, and debugging. Code and data are available at\nhttps://videogamebunny.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2407.15295",
        "PDF Link": "https://arxiv.org/pdf/2407.15295",
        "Upvotes": "10",
        "Date": "2024-07-24"
    },
    {
        "Title": "BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis\n  in Large-scale Scenes",
        "Abstract": "While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality,\ntheir protracted training duration remains a limitation. Generalizable and\nMVS-based NeRFs, although capable of mitigating training time, often incur\ntradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFs\nto enhance the rendering quality of MVS-based NeRFs in large-scale scenes. We\nfirst identify limitations in MVS-based NeRF methods, such as restricted\nviewport coverage and artifacts due to limited input views. Then, we address\nthese limitations by proposing a new method that selects and combines multiple\ncost volumes during volume rendering. Our method does not require training and\ncan adapt to any MVS-based NeRF methods in a feed-forward fashion to improve\nrendering quality. Furthermore, our approach is also end-to-end trainable,\nallowing fine-tuning on specific scenes. We demonstrate the effectiveness of\nour method through experiments on large-scale datasets, showing significant\nrendering quality improvements in large-scale scenes and unbounded outdoor\nscenarios. We release the source code of BoostMVSNeRFs at\nhttps://su-terry.github.io/BoostMVSNeRFs/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15848",
        "PDF Link": "https://arxiv.org/pdf/2407.15848",
        "Upvotes": "9",
        "Date": "2024-07-24"
    },
    {
        "Title": "Compact Language Models via Pruning and Knowledge Distillation",
        "Abstract": "Large language models (LLMs) targeting different deployment scales and sizes\nare currently produced by training each variant from scratch; this is extremely\ncompute-intensive. In this paper, we investigate if pruning an existing LLM and\nthen re-training it with a fraction (<3%) of the original training data can be\na suitable alternative to repeated, full retraining. To this end, we develop a\nset of practical and effective compression best practices for LLMs that combine\ndepth, width, attention and MLP pruning with knowledge distillation-based\nretraining; we arrive at these best practices through a detailed empirical\nexploration of pruning strategies for each axis, methods to combine axes,\ndistillation strategies, and search techniques for arriving at optimal\ncompressed architectures. We use this guide to compress the Nemotron-4 family\nof LLMs by a factor of 2-4x, and compare their performance to similarly-sized\nmodels on a variety of language modeling tasks. Deriving 8B and 4B models from\nan already pretrained 15B model using our approach requires up to 40x fewer\ntraining tokens per model compared to training from scratch; this results in\ncompute cost savings of 1.8x for training the full model family (15B, 8B, and\n4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to\ntraining from scratch, perform comparably to other community models such as\nMistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art\ncompression techniques from the literature. We have open-sourced Minitron model\nweights on Huggingface, with corresponding supplementary material including\nexample code available on GitHub.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14679",
        "PDF Link": "https://arxiv.org/pdf/2407.14679",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "BOND: Aligning LLMs with Best-of-N Distillation",
        "Abstract": "Reinforcement learning from human feedback (RLHF) is a key driver of quality\nand safety in state-of-the-art large language models. Yet, a surprisingly\nsimple and strong inference-time strategy is Best-of-N sampling that selects\nthe best generation among N candidates. In this paper, we propose Best-of-N\nDistillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but\nwithout its significant computational overhead at inference time. Specifically,\nBOND is a distribution matching algorithm that forces the distribution of\ngenerations from the policy to get closer to the Best-of-N distribution. We use\nthe Jeffreys divergence (a linear combination of forward and backward KL) to\nbalance between mode-covering and mode-seeking behavior, and derive an\niterative formulation that utilizes a moving anchor for efficiency. We\ndemonstrate the effectiveness of our approach and several design choices\nthrough experiments on abstractive summarization and Gemma models. Aligning\nGemma policies with BOND outperforms other RLHF algorithms by improving results\non several benchmarks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14622",
        "PDF Link": "https://arxiv.org/pdf/2407.14622",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "HoloDreamer: Holistic 3D Panoramic World Generation from Text\n  Descriptions",
        "Abstract": "3D scene generation is in high demand across various domains, including\nvirtual reality, gaming, and the film industry. Owing to the powerful\ngenerative capabilities of text-to-image diffusion models that provide reliable\npriors, the creation of 3D scenes using only text prompts has become viable,\nthereby significantly advancing researches in text-driven 3D scene generation.\nIn order to obtain multiple-view supervision from 2D diffusion models,\nprevailing methods typically employ the diffusion model to generate an initial\nlocal image, followed by iteratively outpainting the local image using\ndiffusion models to gradually generate scenes. Nevertheless, these\noutpainting-based approaches prone to produce global inconsistent scene\ngeneration results without high degree of completeness, restricting their\nbroader applications. To tackle these problems, we introduce HoloDreamer, a\nframework that first generates high-definition panorama as a holistic\ninitialization of the full 3D scene, then leverage 3D Gaussian Splatting\n(3D-GS) to quickly reconstruct the 3D scene, thereby facilitating the creation\nof view-consistent and fully enclosed 3D scenes. Specifically, we propose\nStylized Equirectangular Panorama Generation, a pipeline that combines multiple\ndiffusion models to enable stylized and detailed equirectangular panorama\ngeneration from complex text prompts. Subsequently, Enhanced Two-Stage Panorama\nReconstruction is introduced, conducting a two-stage optimization of 3D-GS to\ninpaint the missing region and enhance the integrity of the scene.\nComprehensive experiments demonstrated that our method outperforms prior works\nin terms of overall visual consistency and harmony as well as reconstruction\nquality and rendering robustness when generating fully enclosed scenes.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15187",
        "PDF Link": "https://arxiv.org/pdf/2407.15187",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?",
        "Abstract": "Language agents, built on top of language models (LMs), are systems that can\ninteract with complex environments, such as the open web. In this work, we\nexamine whether such agents can perform realistic and time-consuming tasks on\nthe web, e.g., monitoring real-estate markets or locating relevant nearby\nbusinesses. We introduce AssistantBench, a challenging new benchmark consisting\nof 214 realistic tasks that can be automatically evaluated, covering different\nscenarios and domains. We find that AssistantBench exposes the limitations of\ncurrent systems, including language models and retrieval-augmented language\nmodels, as no model reaches an accuracy of more than 25 points. While\nclosed-book LMs perform well, they exhibit low precision since they tend to\nhallucinate facts. State-of-the-art web agents reach a score of near zero.\nAdditionally, we introduce SeePlanAct (SPA), a new web agent that significantly\noutperforms previous agents, and an ensemble of SPA and closed-book models\nreaches the best overall performance. Moreover, we analyze failures of current\nsystems and highlight that web navigation remains a major challenge.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15711",
        "PDF Link": "https://arxiv.org/pdf/2407.15711",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "Cinemo: Consistent and Controllable Image Animation with Motion\n  Diffusion Models",
        "Abstract": "Diffusion models have achieved great progress in image animation due to\npowerful generative capabilities. However, maintaining spatio-temporal\nconsistency with detailed information from the input static image over time\n(e.g., style, background, and object of the input static image) and ensuring\nsmoothness in animated video narratives guided by textual prompts still remains\nchallenging. In this paper, we introduce Cinemo, a novel image animation\napproach towards achieving better motion controllability, as well as stronger\ntemporal consistency and smoothness. In general, we propose three effective\nstrategies at the training and inference stages of Cinemo to accomplish our\ngoal. At the training stage, Cinemo focuses on learning the distribution of\nmotion residuals, rather than directly predicting subsequent via a motion\ndiffusion model. Additionally, a structural similarity index-based strategy is\nproposed to enable Cinemo to have better controllability of motion intensity.\nAt the inference stage, a noise refinement technique based on discrete cosine\ntransformation is introduced to mitigate sudden motion changes. Such three\nstrategies enable Cinemo to produce highly consistent, smooth, and\nmotion-controllable results. Compared to previous methods, Cinemo offers\nsimpler and more precise user controllability. Extensive experiments against\nseveral state-of-the-art methods, including both commercial tools and research\napproaches, across multiple metrics, demonstrate the effectiveness and\nsuperiority of our proposed approach.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15642",
        "PDF Link": "https://arxiv.org/pdf/2407.15642",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "Conditioned Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning",
        "Abstract": "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15762",
        "PDF Link": "https://arxiv.org/pdf/2407.15762",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "MIBench: Evaluating Multimodal Large Language Models over Multiple\n  Images",
        "Abstract": "Built on the power of LLMs, numerous multimodal large language models (MLLMs)\nhave recently achieved remarkable performance on various vision-language tasks\nacross multiple benchmarks. However, most existing MLLMs and benchmarks\nprimarily focus on single-image input scenarios, leaving the performance of\nMLLMs when handling realistic multiple images remain underexplored. Although a\nfew benchmarks consider multiple images, their evaluation dimensions and\nsamples are very limited. Therefore, in this paper, we propose a new benchmark\nMIBench, to comprehensively evaluate fine-grained abilities of MLLMs in\nmulti-image scenarios. Specifically, MIBench categorizes the multi-image\nabilities into three scenarios: multi-image instruction (MII), multimodal\nknowledge-seeking (MKS) and multimodal in-context learning (MIC), and\nconstructs 13 tasks with a total of 13K annotated samples. During data\nconstruction, for MII and MKS, we extract correct options from manual\nannotations and create challenging distractors to obtain multiple-choice\nquestions. For MIC, to enable an in-depth evaluation, we set four sub-tasks and\ntransform the original datasets into in-context learning formats. We evaluate\nseveral open-source MLLMs and close-source MLLMs on the proposed MIBench. The\nresults reveal that although current models excel in single-image tasks, they\nexhibit significant shortcomings when faced with multi-image inputs, such as\nconfused fine-grained perception, limited multi-image reasoning, and unstable\nin-context learning. The annotated data in MIBench is available at\nhttps://huggingface.co/datasets/StarBottle/MIBench.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15272",
        "PDF Link": "https://arxiv.org/pdf/2407.15272",
        "Upvotes": "4",
        "Date": "2024-07-24"
    },
    {
        "Title": "Local All-Pair Correspondence for Point Tracking",
        "Abstract": "We introduce LocoTrack, a highly accurate and efficient model designed for\nthe task of tracking any point (TAP) across video sequences. Previous\napproaches in this task often rely on local 2D correlation maps to establish\ncorrespondences from a point in the query image to a local region in the target\nimage, which often struggle with homogeneous regions or repetitive features,\nleading to matching ambiguities. LocoTrack overcomes this challenge with a\nnovel approach that utilizes all-pair correspondences across regions, i.e.,\nlocal 4D correlation, to establish precise correspondences, with bidirectional\ncorrespondence and matching smoothness significantly enhancing robustness\nagainst ambiguities. We also incorporate a lightweight correlation encoder to\nenhance computational efficiency, and a compact Transformer architecture to\nintegrate long-term temporal information. LocoTrack achieves unmatched accuracy\non all TAP-Vid benchmarks and operates at a speed almost 6 times faster than\nthe current state-of-the-art.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15420",
        "PDF Link": "https://arxiv.org/pdf/2407.15420",
        "Upvotes": "4",
        "Date": "2024-07-24"
    },
    {
        "Title": "Temporal Residual Jacobians For Rig-free Motion Transfer",
        "Abstract": "We introduce Temporal Residual Jacobians as a novel representation to enable\ndata-driven motion transfer. Our approach does not assume access to any rigging\nor intermediate shape keyframes, produces geometrically and temporally\nconsistent motions, and can be used to transfer long motion sequences. Central\nto our approach are two coupled neural networks that individually predict local\ngeometric and temporal changes that are subsequently integrated, spatially and\ntemporally, to produce the final animated meshes. The two networks are jointly\ntrained, complement each other in producing spatial and temporal signals, and\nare supervised directly with 3D positional information. During inference, in\nthe absence of keyframes, our method essentially solves a motion extrapolation\nproblem. We test our setup on diverse meshes (synthetic and scanned shapes) to\ndemonstrate its superiority in generating realistic and natural-looking\nanimations on unseen body shapes against SoTA alternatives. Supplemental video\nand code are available at https://temporaljacobians.github.io/ .",
        "ArXiv Link": "https://arxiv.org/abs/2407.14958",
        "PDF Link": "https://arxiv.org/pdf/2407.14958",
        "Upvotes": "4",
        "Date": "2024-07-24"
    },
    {
        "Title": "Artist: Aesthetically Controllable Text-Driven Stylization without\n  Training",
        "Abstract": "Diffusion models entangle content and style generation during the denoising\nprocess, leading to undesired content modification when directly applied to\nstylization tasks. Existing methods struggle to effectively control the\ndiffusion model to meet the aesthetic-level requirements for stylization. In\nthis paper, we introduce Artist, a training-free approach that\naesthetically controls the content and style generation of a pretrained\ndiffusion model for text-driven stylization. Our key insight is to disentangle\nthe denoising of content and style into separate diffusion processes while\nsharing information between them. We propose simple yet effective content and\nstyle control methods that suppress style-irrelevant content generation,\nresulting in harmonious stylization results. Extensive experiments demonstrate\nthat our method excels at achieving aesthetic-level stylization requirements,\npreserving intricate details in the content image and aligning well with the\nstyle prompt. Furthermore, we showcase the highly controllability of the\nstylization strength from various perspectives. Code will be released, project\nhome page: https://DiffusionArtist.github.io",
        "ArXiv Link": "https://arxiv.org/abs/2407.15842",
        "PDF Link": "https://arxiv.org/pdf/2407.15842",
        "Upvotes": "3",
        "Date": "2024-07-24"
    },
    {
        "Title": "ThermalNeRF: Thermal Radiance Fields",
        "Abstract": "Thermal imaging has a variety of applications, from agricultural monitoring\nto building inspection to imaging under poor visibility, such as in low light,\nfog, and rain. However, reconstructing thermal scenes in 3D presents several\nchallenges due to the comparatively lower resolution and limited features\npresent in long-wave infrared (LWIR) images. To overcome these challenges, we\npropose a unified framework for scene reconstruction from a set of LWIR and RGB\nimages, using a multispectral radiance field to represent a scene viewed by\nboth visible and infrared cameras, thus leveraging information across both\nspectra. We calibrate the RGB and infrared cameras with respect to each other,\nas a preprocessing step using a simple calibration target. We demonstrate our\nmethod on real-world sets of RGB and LWIR photographs captured from a handheld\nthermal camera, showing the effectiveness of our method at scene representation\nacross the visible and infrared spectra. We show that our method is capable of\nthermal super-resolution, as well as visually removing obstacles to reveal\nobjects that are occluded in either the RGB or thermal channels. Please see\nhttps://yvette256.github.io/thermalnerf for video results as well as our code\nand dataset release.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15337",
        "PDF Link": "https://arxiv.org/pdf/2407.15337",
        "Upvotes": "3",
        "Date": "2024-07-24"
    },
    {
        "Title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music\n  Generation",
        "Abstract": "Existing text-to-music models can produce high-quality audio with great\ndiversity. However, textual prompts alone cannot precisely control temporal\nmusical features such as chords and rhythm of the generated music. To address\nthis challenge, we introduce MusiConGen, a temporally-conditioned\nTransformer-based text-to-music model that builds upon the pretrained MusicGen\nframework. Our innovation lies in an efficient finetuning mechanism, tailored\nfor consumer-grade GPUs, that integrates automatically-extracted rhythm and\nchords as the condition signal. During inference, the condition can either be\nmusical features extracted from a reference audio signal, or be user-defined\nsymbolic chord sequence, BPM, and textual prompts. Our performance evaluation\non two datasets -- one derived from extracted features and the other from\nuser-created inputs -- demonstrates that MusiConGen can generate realistic\nbacking track music that aligns well with the specified conditions. We\nopen-source the code and model checkpoints, and provide audio examples online,\nhttps://musicongen.github.io/musicongen_demo/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15060",
        "PDF Link": "https://arxiv.org/pdf/2407.15060",
        "Upvotes": "3",
        "Date": "2024-07-24"
    },
    {
        "Title": "Consent in Crisis: The Rapid Decline of the AI Data Commons",
        "Abstract": "General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how consent preferences to use it are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crisis in data consent, foreclosing\nmuch of the open web, not only for commercial AI, but non-commercial AI and\nacademic purposes.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14933",
        "PDF Link": "https://arxiv.org/pdf/2407.14933",
        "Upvotes": "2",
        "Date": "2024-07-24"
    },
    {
        "Title": "Visual Haystacks: Answering Harder Questions About Sets of Images",
        "Abstract": "Recent advancements in Large Multimodal Models (LMMs) have made significant\nprogress in the field of single-image visual question answering. However, these\nmodels face substantial challenges when tasked with queries that span extensive\ncollections of images, similar to real-world scenarios like searching through\nlarge photo albums, finding specific information across the internet, or\nmonitoring environmental changes through satellite imagery. This paper explores\nthe task of Multi-Image Visual Question Answering (MIQA): given a large set of\nimages and a natural language query, the task is to generate a relevant and\ngrounded response. We propose a new public benchmark, dubbed \"Visual Haystacks\n(VHs),\" specifically designed to evaluate LMMs' capabilities in visual\nretrieval and reasoning over sets of unrelated images, where we perform\ncomprehensive evaluations demonstrating that even robust closed-source models\nstruggle significantly. Towards addressing these shortcomings, we introduce\nMIRAGE (Multi-Image Retrieval Augmented Generation), a novel retrieval/QA\nframework tailored for LMMs that confronts the challenges of MIQA with marked\nefficiency and accuracy improvements over baseline methods. Our evaluation\nshows that MIRAGE surpasses closed-source GPT-4o models by up to 11% on the VHs\nbenchmark and offers up to 3.4x improvements in efficiency over text-focused\nmulti-stage approaches.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13766",
        "PDF Link": "https://arxiv.org/pdf/2407.13766",
        "Upvotes": "1",
        "Date": "2024-07-24"
    },
    {
        "Title": "CGB-DM: Content and Graphic Balance Layout Generation with\n  Transformer-based Diffusion Model",
        "Abstract": "Layout generation is the foundation task of intelligent design, which\nrequires the integration of visual aesthetics and harmonious expression of\ncontent delivery. However, existing methods still face challenges in generating\nprecise and visually appealing layouts, including blocking, overlap, or spatial\nmisalignment between layouts, which are closely related to the spatial\nstructure of graphic layouts. We find that these methods overly focus on\ncontent information and lack constraints on layout spatial structure, resulting\nin an imbalance of learning content-aware and graphic-aware features. To tackle\nthis issue, we propose Content and Graphic Balance Layout Generation with\nTransformer-based Diffusion Model (CGB-DM). Specifically, we first design a\nregulator that balances the predicted content and graphic weight, overcoming\nthe tendency of paying more attention to the content on canvas. Secondly, we\nintroduce a graphic constraint of saliency bounding box to further enhance the\nalignment of geometric features between layout representations and images. In\naddition, we adapt a transformer-based diffusion model as the backbone, whose\npowerful generation capability ensures the quality in layout generation.\nExtensive experimental results indicate that our method has achieved\nstate-of-the-art performance in both quantitative and qualitative evaluations.\nOur model framework can also be expanded to other graphic design fields.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15233",
        "PDF Link": "https://arxiv.org/pdf/2407.15233",
        "Upvotes": "1",
        "Date": "2024-07-24"
    },
    {
        "Title": "Discrete Flow Matching",
        "Abstract": "Despite Flow Matching and diffusion models having emerged as powerful\ngenerative paradigms for continuous variables such as images and videos, their\napplication to high-dimensional discrete data, such as language, is still\nlimited. In this work, we present Discrete Flow Matching, a novel discrete flow\nparadigm designed specifically for generating discrete data. Discrete Flow\nMatching offers several key contributions: (i) it works with a general family\nof probability paths interpolating between source and target distributions;\n(ii) it allows for a generic formula for sampling from these probability paths\nusing learned posteriors such as the probability denoiser (x-prediction) and\nnoise-prediction (epsilon-prediction); (iii) practically, focusing on\nspecific probability paths defined with different schedulers considerably\nimproves generative perplexity compared to previous discrete diffusion and flow\nmodels; and (iv) by scaling Discrete Flow Matching models up to 1.7B\nparameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1\nand 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of\ngenerating high-quality discrete data in a non-autoregressive fashion,\nsignificantly closing the gap between autoregressive models and discrete flow\nmodels.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15595",
        "PDF Link": "https://arxiv.org/pdf/2407.15595",
        "Upvotes": "1",
        "Date": "2024-07-24"
    },
    {
        "Title": "GET-Zero: Graph Embodiment Transformer for Zero-shot Embodiment\n  Generalization",
        "Abstract": "This paper introduces GET-Zero, a model architecture and training procedure\nfor learning an embodiment-aware control policy that can immediately adapt to\nnew hardware changes without retraining. To do so, we present Graph Embodiment\nTransformer (GET), a transformer model that leverages the embodiment graph\nconnectivity as a learned structural bias in the attention mechanism. We use\nbehavior cloning to distill demonstration data from embodiment-specific expert\npolicies into an embodiment-aware GET model that conditions on the hardware\nconfiguration of the robot to make control decisions. We conduct a case study\non a dexterous in-hand object rotation task using different configurations of a\nfour-fingered robot hand with joints removed and with link length extensions.\nUsing the GET model along with a self-modeling loss enables GET-Zero to\nzero-shot generalize to unseen variation in graph structure and link length,\nyielding a 20% improvement over baseline methods. All code and qualitative\nvideo results are on https://get-zero-paper.github.io",
        "ArXiv Link": "https://arxiv.org/abs/2407.15002",
        "PDF Link": "https://arxiv.org/pdf/2407.15002",
        "Upvotes": "1",
        "Date": "2024-07-24"
    },
    {
        "Title": "Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey",
        "Abstract": "Large language models (LLMs) are expected to respond accurately but often\nexhibit deficient reasoning or generate hallucinatory content. To address\nthese, studies prefixed with ``Self-'' such as Self-Consistency, Self-Improve,\nand Self-Refine have been initiated. They share a commonality: involving LLMs\nevaluating and updating itself to mitigate the issues. Nonetheless, these\nefforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization without examining the motivations behind\nthese works.\n  In this paper, we summarize a theoretical framework, termed Internal\nConsistency, which offers unified explanations for phenomena such as the lack\nof reasoning and the presence of hallucinations. Internal Consistency assesses\nthe coherence among LLMs' latent layer, decoding layer, and response layer\nbased on sampling methodologies. Expanding upon the Internal Consistency\nframework, we introduce a streamlined yet effective theoretical framework\ncapable of mining Internal Consistency, named Self-Feedback. The Self-Feedback\nframework consists of two modules: Self-Evaluation and Self-Update. This\nframework has been employed in numerous studies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, ``Does Self-Feedback Really Work?'' We propose several critical\nviewpoints, including the ``Hourglass Evolution of Internal Consistency'',\n``Consistency Is (Almost) Correctness'' hypothesis, and ``The Paradox of Latent\nand Explicit Reasoning''. Furthermore, we outline promising directions for\nfuture research. We have open-sourced the experimental code, reference list,\nand statistical data, available at\nhttps://github.com/IAAR-Shanghai/ICSFSurvey.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14507",
        "PDF Link": "https://arxiv.org/pdf/2407.14507",
        "Upvotes": "37",
        "Date": "2024-07-24"
    },
    {
        "Title": "EVLM: An Efficient Vision-Language Model for Visual Understanding",
        "Abstract": "In the field of multi-modal language models, the majority of methods are\nbuilt on an architecture similar to LLaVA. These models use a single-layer ViT\nfeature as a visual prompt, directly feeding it into the language models\nalongside textual tokens. However, when dealing with long sequences of visual\nsignals or inputs such as videos, the self-attention mechanism of language\nmodels can lead to significant computational overhead. Additionally, using\nsingle-layer ViT features makes it challenging for large language models to\nperceive visual signals fully. This paper proposes an efficient multi-modal\nlanguage model to minimize computational costs while enabling the model to\nperceive visual signals as comprehensively as possible. Our method primarily\nincludes: (1) employing cross-attention to image-text interaction similar to\nFlamingo. (2) utilize hierarchical ViT features. (3) introduce the Mixture of\nExperts (MoE) mechanism to enhance model effectiveness. Our model achieves\ncompetitive scores on public multi-modal benchmarks and performs well in tasks\nsuch as image captioning and video captioning.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14177",
        "PDF Link": "https://arxiv.org/pdf/2407.14177",
        "Upvotes": "31",
        "Date": "2024-07-24"
    },
    {
        "Title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
        "Abstract": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14057",
        "PDF Link": "https://arxiv.org/pdf/2407.14057",
        "Upvotes": "28",
        "Date": "2024-07-24"
    },
    {
        "Title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities",
        "Abstract": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14482",
        "PDF Link": "https://arxiv.org/pdf/2407.14482",
        "Upvotes": "18",
        "Date": "2024-07-24"
    },
    {
        "Title": "Stable Audio Open",
        "Abstract": "Open generative models are vitally important for the community, allowing for\nfine-tunes and serving as baselines when presenting new models. However, most\ncurrent text-to-audio models are private and not accessible for artists and\nresearchers to build upon. Here we describe the architecture and training\nprocess of a new open-weights text-to-audio model trained with Creative Commons\ndata. Our evaluation shows that the model's performance is competitive with the\nstate-of-the-art across various metrics. Notably, the reported FDopenl3 results\n(measuring the realism of the generations) showcase its potential for\nhigh-quality stereo sound synthesis at 44.1kHz.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14358",
        "PDF Link": "https://arxiv.org/pdf/2407.14358",
        "Upvotes": "14",
        "Date": "2024-07-24"
    },
    {
        "Title": "VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document\n  Understanding",
        "Abstract": "In recent years, notable advancements have been made in the domain of visual\ndocument understanding, with the prevailing architecture comprising a cascade\nof vision and language models. The text component can either be extracted\nexplicitly with the use of external OCR models in OCR-based approaches, or\nalternatively, the vision model can be endowed with reading capabilities in\nOCR-free approaches. Typically, the queries to the model are input exclusively\nto the language component, necessitating the visual features to encompass the\nentire document. In this paper, we present VisFocus, an OCR-free method\ndesigned to better exploit the vision encoder's capacity by coupling it\ndirectly with the language prompt. To do so, we replace the down-sampling\nlayers with layers that receive the input prompt and allow highlighting\nrelevant parts of the document, while disregarding others. We pair the\narchitecture enhancements with a novel pre-training task, using language\nmasking on a snippet of the document text fed to the visual encoder in place of\nthe prompt, to empower the model with focusing capabilities. Consequently,\nVisFocus learns to allocate its attention to text patches pertinent to the\nprovided prompt. Our experiments demonstrate that this prompt-guided visual\nencoding approach significantly improves performance, achieving\nstate-of-the-art results on various benchmarks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12594",
        "PDF Link": "https://arxiv.org/pdf/2407.12594",
        "Upvotes": "11",
        "Date": "2024-07-24"
    },
    {
        "Title": "The Vision of Autonomic Computing: Can LLMs Make It a Reality?",
        "Abstract": "The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14402",
        "PDF Link": "https://arxiv.org/pdf/2407.14402",
        "Upvotes": "9",
        "Date": "2024-07-24"
    },
    {
        "Title": "SciCode: A Research Coding Benchmark Curated by Scientists",
        "Abstract": "Since language models (LMs) now outperform average humans on many challenging\ntasks, it has become increasingly difficult to develop challenging,\nhigh-quality, and realistic evaluations. We address this issue by examining\nLMs' capabilities to generate code for solving real scientific research\nproblems. Incorporating input from scientists and AI researchers in 16 diverse\nnatural science sub-fields, including mathematics, physics, chemistry, biology,\nand materials science, we created a scientist-curated coding benchmark,\nSciCode. The problems in SciCode naturally factorize into multiple subproblems,\neach involving knowledge recall, reasoning, and code synthesis. In total,\nSciCode contains 338 subproblems decomposed from 80 challenging main problems.\nIt offers optional descriptions specifying useful scientific background\ninformation and scientist-annotated gold-standard solutions and test cases for\nevaluation. Claude3.5-Sonnet, the best-performing model among those tested, can\nsolve only 4.6% of the problems in the most realistic setting. We believe that\nSciCode demonstrates both contemporary LMs' progress towards becoming helpful\nscientific assistants and sheds light on the development and evaluation of\nscientific AI in the future.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13168",
        "PDF Link": "https://arxiv.org/pdf/2407.13168",
        "Upvotes": "7",
        "Date": "2024-07-24"
    },
    {
        "Title": "Phi-3 Safety Post-Training: Aligning Language Models with a \"Break-Fix\"\n  Cycle",
        "Abstract": "Recent innovations in language model training have demonstrated that it is\npossible to create highly performant models that are small enough to run on a\nsmartphone. As these models are deployed in an increasing number of domains, it\nis critical to ensure that they are aligned with human preferences and safety\nconsiderations. In this report, we present our methodology for safety aligning\nthe Phi-3 series of language models. We utilized a \"break-fix\" cycle,\nperforming multiple rounds of dataset curation, safety post-training,\nbenchmarking, red teaming, and vulnerability identification to cover a variety\nof harm areas in both single and multi-turn scenarios. Our results indicate\nthat this approach iteratively improved the performance of the Phi-3 models\nacross a wide range of responsible AI benchmarks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13833",
        "PDF Link": "https://arxiv.org/pdf/2407.13833",
        "Upvotes": "7",
        "Date": "2024-07-24"
    },
    {
        "Title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
        "Abstract": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.",
        "ArXiv Link": "https://arxiv.org/abs/2407.10960",
        "PDF Link": "https://arxiv.org/pdf/2407.10960",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "Visual Text Generation in the Wild",
        "Abstract": "Recently, with the rapid advancements of generative models, the field of\nvisual text generation has witnessed significant progress. However, it is still\nchallenging to render high-quality text images in real-world scenarios, as\nthree critical criteria should be satisfied: (1) Fidelity: the generated text\nimages should be photo-realistic and the contents are expected to be the same\nas specified in the given conditions; (2) Reasonability: the regions and\ncontents of the generated text should cohere with the scene; (3) Utility: the\ngenerated text images can facilitate related tasks (e.g., text detection and\nrecognition). Upon investigation, we find that existing methods, either\nrendering-based or diffusion-based, can hardly meet all these aspects\nsimultaneously, limiting their application range. Therefore, we propose in this\npaper a visual text generator (termed SceneVTG), which can produce high-quality\ntext images in the wild. Following a two-stage paradigm, SceneVTG leverages a\nMultimodal Large Language Model to recommend reasonable text regions and\ncontents across multiple scales and levels, which are used by a conditional\ndiffusion model as conditions to generate text images. Extensive experiments\ndemonstrate that the proposed SceneVTG significantly outperforms traditional\nrendering-based methods and recent diffusion-based methods in terms of fidelity\nand reasonability. Besides, the generated images provide superior utility for\ntasks involving text detection and text recognition. Code and datasets are\navailable at AdvancedLiterateMachinery.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14138",
        "PDF Link": "https://arxiv.org/pdf/2407.14138",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse\n  Autoencoders",
        "Abstract": "Sparse autoencoders (SAEs) are a promising unsupervised approach for\nidentifying causally relevant and interpretable linear features in a language\nmodel's (LM) activations. To be useful for downstream tasks, SAEs need to\ndecompose LM activations faithfully; yet to be interpretable the decomposition\nmust be sparse -- two objectives that are in tension. In this paper, we\nintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity\nat a given sparsity level on Gemma 2 9B activations, compared to other recent\nadvances such as Gated and TopK SAEs. We also show that this improvement does\nnot come at the cost of interpretability through manual and automated\ninterpretability studies. JumpReLU SAEs are a simple modification of vanilla\n(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU\nactivation function -- and are similarly efficient to train and run. By\nutilising straight-through-estimators (STEs) in a principled manner, we show\nhow it is possible to train JumpReLU SAEs effectively despite the discontinuous\nJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs\nto directly train L0 to be sparse, instead of training on proxies such as L1,\navoiding problems like shrinkage.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14435",
        "PDF Link": "https://arxiv.org/pdf/2407.14435",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting\n  Recognition",
        "Abstract": "Arabic Optical Character Recognition (OCR) and Handwriting Recognition (HWR)\npose unique challenges due to the cursive and context-sensitive nature of the\nArabic script. This study introduces Qalam, a novel foundation model designed\nfor Arabic OCR and HWR, built on a SwinV2 encoder and RoBERTa decoder\narchitecture. Our model significantly outperforms existing methods, achieving a\nWord Error Rate (WER) of just 0.80% in HWR tasks and 1.18% in OCR tasks. We\ntrain Qalam on a diverse dataset, including over 4.5 million images from Arabic\nmanuscripts and a synthetic dataset comprising 60k image-text pairs. Notably,\nQalam demonstrates exceptional handling of Arabic diacritics, a critical\nfeature in Arabic scripts. Furthermore, it shows a remarkable ability to\nprocess high-resolution inputs, addressing a common limitation in current OCR\nsystems. These advancements underscore Qalam's potential as a leading solution\nfor Arabic script recognition, offering a significant leap in accuracy and\nefficiency.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13559",
        "PDF Link": "https://arxiv.org/pdf/2407.13559",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "PlacidDreamer: Advancing Harmony in Text-to-3D Generation",
        "Abstract": "Recently, text-to-3D generation has attracted significant attention,\nresulting in notable performance enhancements. Previous methods utilize\nend-to-end 3D generation models to initialize 3D Gaussians, multi-view\ndiffusion models to enforce multi-view consistency, and text-to-image diffusion\nmodels to refine details with score distillation algorithms. However, these\nmethods exhibit two limitations. Firstly, they encounter conflicts in\ngeneration directions since different models aim to produce diverse 3D assets.\nSecondly, the issue of over-saturation in score distillation has not been\nthoroughly investigated and solved. To address these limitations, we propose\nPlacidDreamer, a text-to-3D framework that harmonizes initialization,\nmulti-view generation, and text-conditioned generation with a single multi-view\ndiffusion model, while simultaneously employing a novel score distillation\nalgorithm to achieve balanced saturation. To unify the generation direction, we\nintroduce the Latent-Plane module, a training-friendly plug-in extension that\nenables multi-view diffusion models to provide fast geometry reconstruction for\ninitialization and enhanced multi-view images to personalize the text-to-image\ndiffusion model. To address the over-saturation problem, we propose to view\nscore distillation as a multi-objective optimization problem and introduce the\nBalanced Score Distillation algorithm, which offers a Pareto Optimal solution\nthat achieves both rich details and balanced saturation. Extensive experiments\nvalidate the outstanding capabilities of our PlacidDreamer. The code is\navailable at https://github.com/HansenHuang0823/PlacidDreamer.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13976",
        "PDF Link": "https://arxiv.org/pdf/2407.13976",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided\n  Geometric Linearization",
        "Abstract": "We present a novel approach for recovering 3D shape and view dependent\nappearance from a few colored images, enabling efficient 3D reconstruction and\nnovel view synthesis. Our method learns an implicit neural representation in\nthe form of a Signed Distance Function (SDF) and a radiance field. The model is\ntrained progressively through ray marching enabled volumetric rendering, and\nregularized with learning-free multi-view stereo (MVS) cues. Key to our\ncontribution is a novel implicit neural shape function learning strategy that\nencourages our SDF field to be as linear as possible near the level-set, hence\nrobustifying the training against noise emanating from the supervision and\nregularization signals. Without using any pretrained priors, our method, called\nSparseCraft, achieves state-of-the-art performances both in novel-view\nsynthesis and reconstruction from sparse views in standard benchmarks, while\nrequiring less than 10 minutes for training.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14257",
        "PDF Link": "https://arxiv.org/pdf/2407.14257",
        "Upvotes": "3",
        "Date": "2024-07-24"
    },
    {
        "Title": "Efficient Audio Captioning with Encoder-Level Knowledge Distillation",
        "Abstract": "Significant improvement has been achieved in automated audio captioning (AAC)\nwith recent models. However, these models have become increasingly large as\ntheir performance is enhanced. In this work, we propose a knowledge\ndistillation (KD) framework for AAC. Our analysis shows that in the\nencoder-decoder based AAC models, it is more effective to distill knowledge\ninto the encoder as compared with the decoder. To this end, we incorporate\nencoder-level KD loss into training, in addition to the standard supervised\nloss and sequence-level KD loss. We investigate two encoder-level KD methods,\nbased on mean squared error (MSE) loss and contrastive loss, respectively.\nExperimental results demonstrate that contrastive KD is more robust than MSE\nKD, exhibiting superior performance in data-scarce situations. By leveraging\naudio-only data into training in the KD framework, our student model achieves\ncompetitive performance, with an inference speed that is 19 times\nfasterAn online demo is available at\n\\url{https://huggingface.co/spaces/wsntxxn/efficient_audio_captioning}.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14329",
        "PDF Link": "https://arxiv.org/pdf/2407.14329",
        "Upvotes": "2",
        "Date": "2024-07-24"
    },
    {
        "Title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
        "Abstract": "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. % Intuitively, larger vocabularies enable more efficient tokenization by\nrepresenting sentences with fewer tokens, but they also increase the risk of\nunder-fitting representations for rare tokens. We investigate how vocabulary\nsize impacts LLM scaling laws by training models ranging from 33M to 3B\nparameters on up to 500B characters with various vocabulary configurations. We\npropose three complementary approaches for predicting the compute-optimal\nvocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit\nof the loss function. Our approaches converge on the same result that the\noptimal vocabulary size depends on the available compute budget and that larger\nmodels deserve larger vocabularies. However, most LLMs use too small vocabulary\nsizes. For example, we predict that the optimal vocabulary size of Llama2-70B\nshould have been at least 216K, 7 times larger than its vocabulary of 32K. We\nvalidate our predictions empirically by training models with 3B parameters\nacross different FLOPs budgets. Adopting our predicted optimal vocabulary size\nconsistently improves downstream performance over commonly used vocabulary\nsizes. By increasing the vocabulary size from the conventional 32K to 43K, we\nimprove performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21\nFLOPs. Our work emphasizes the necessity of jointly considering model\nparameters and vocabulary size for efficient scaling.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13623",
        "PDF Link": "https://arxiv.org/pdf/2407.13623",
        "Upvotes": "42",
        "Date": "2024-07-24"
    },
    {
        "Title": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore",
        "Abstract": "Scaling laws with respect to the amount of training data and the number of\nparameters allow us to predict the cost-benefit trade-offs of pretraining\nlanguage models (LMs) in different configurations. In this paper, we consider\nanother dimension of scaling: the amount of data available at inference time.\nSpecifically, we find that increasing the size of the datastore used by a\nretrieval-based LM monotonically improves language modeling and several\ndownstream tasks without obvious saturation, such that a smaller model\naugmented with a large datastore outperforms a larger LM-only model on\nknowledge-intensive tasks. By plotting compute-optimal scaling curves with\nvaried datastore, model, and pretraining data sizes, we show that using larger\ndatastores can significantly improve model performance for the same training\ncompute budget. We carry out our study by constructing a 1.4 trillion-token\ndatastore named MassiveDS, which is the largest and the most diverse\nopen-sourced datastore for retrieval-based LMs to date, and designing an\nefficient pipeline for studying datastore scaling in a computationally\naccessible manner. Finally, we analyze the effect of improving the retriever,\ndatastore quality filtering, and other design choices on our observed scaling\ntrends. Overall, our results show that datastore size should be considered as\nan integral part of LM efficiency and performance trade-offs. To facilitate\nfuture research, we open-source our datastore and code at\nhttps://github.com/RulinShao/retrieval-scaling.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12854",
        "PDF Link": "https://arxiv.org/pdf/2407.12854",
        "Upvotes": "27",
        "Date": "2024-07-24"
    },
    {
        "Title": "Shape of Motion: 4D Reconstruction from a Single Video",
        "Abstract": "Monocular dynamic reconstruction is a challenging and long-standing vision\nproblem due to the highly ill-posed nature of the task. Existing approaches are\nlimited in that they either depend on templates, are effective only in\nquasi-static scenes, or fail to model 3D motion explicitly. In this work, we\nintroduce a method capable of reconstructing generic dynamic scenes, featuring\nexplicit, full-sequence-long 3D motion, from casually captured monocular\nvideos. We tackle the under-constrained nature of the problem with two key\ninsights: First, we exploit the low-dimensional structure of 3D motion by\nrepresenting scene motion with a compact set of SE3 motion bases. Each point's\nmotion is expressed as a linear combination of these bases, facilitating soft\ndecomposition of the scene into multiple rigidly-moving groups. Second, we\nutilize a comprehensive set of data-driven priors, including monocular depth\nmaps and long-range 2D tracks, and devise a method to effectively consolidate\nthese noisy supervisory signals, resulting in a globally consistent\nrepresentation of the dynamic scene. Experiments show that our method achieves\nstate-of-the-art performance for both long-range 3D/2D motion estimation and\nnovel view synthesis on dynamic scenes. Project Page:\nhttps://shape-of-motion.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2407.13764",
        "PDF Link": "https://arxiv.org/pdf/2407.13764",
        "Upvotes": "14",
        "Date": "2024-07-24"
    },
    {
        "Title": "Streetscapes: Large-scale Consistent Street View Generation Using\n  Autoregressive Video Diffusion",
        "Abstract": "We present a method for generating Streetscapes-long sequences of views\nthrough an on-the-fly synthesized city-scale scene. Our generation is\nconditioned by language input (e.g., city name, weather), as well as an\nunderlying map/layout hosting the desired trajectory. Compared to recent models\nfor video generation or 3D view synthesis, our method can scale to much\nlonger-range camera trajectories, spanning several city blocks, while\nmaintaining visual quality and consistency. To achieve this goal, we build on\nrecent work on video diffusion, used within an autoregressive framework that\ncan easily scale to long sequences. In particular, we introduce a new temporal\nimputation method that prevents our autoregressive approach from drifting from\nthe distribution of realistic city imagery. We train our Streetscapes system on\na compelling source of data-posed imagery from Google Street View, along with\ncontextual map data-which allows users to generate city views conditioned on\nany desired city layout, with controllable camera poses. Please see more\nresults at our project page at https://boyangdeng.com/streetscapes.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13759",
        "PDF Link": "https://arxiv.org/pdf/2407.13759",
        "Upvotes": "14",
        "Date": "2024-07-24"
    },
    {
        "Title": "Understanding Reference Policies in Direct Preference Optimization",
        "Abstract": "Direct Preference Optimization (DPO) has become a widely used training method\nfor the instruction fine-tuning of large language models (LLMs). In this work,\nwe explore an under-investigated aspect of DPO - its dependency on the\nreference model or policy. Such reference policies, typically instantiated as\nthe model to be further fine-tuned, are important since they can impose an\nupper limit on DPO's effectiveness. Therefore, we address three related\nresearch questions in this work. First, we explore the optimal strength of the\nKL-divergence constraint in DPO, which penalizes deviations from the reference\npolicy, and find that DPO is sensitive to this strength. Next, we examine the\nnecessity of reference policies for instruction fine-tuning by providing both\ntheoretical and empirical comparisons between DPO and related learning\nobjectives, demonstrating DPO's superiority. Additionally, we investigate\nwhether DPO benefits from stronger reference policies, finding that a stronger\nreference policy can lead to improved performance, but only when it is similar\nto the model being fine-tuned. Our findings highlight the confounding role of\nreference policies in DPO and offer insights for best practices, while also\nidentifying open research questions for future studies.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13709",
        "PDF Link": "https://arxiv.org/pdf/2407.13709",
        "Upvotes": "12",
        "Date": "2024-07-24"
    },
    {
        "Title": "Scaling Granite Code Models to 128K Context",
        "Abstract": "This paper introduces long-context Granite code models that support effective\ncontext windows of up to 128K tokens. Our solution for scaling context length\nof Granite 3B/8B code models from 2K/4K to 128K consists of a light-weight\ncontinual pretraining by gradually increasing its RoPE base frequency with\nrepository-level file packing and length-upsampled long-context data.\nAdditionally, we also release instruction-tuned models with long-context\nsupport which are derived by further finetuning the long context base models on\na mix of permissively licensed short and long-context instruction-response\npairs. While comparing to the original short-context Granite code models, our\nlong-context models achieve significant improvements on long-context tasks\nwithout any noticeable performance degradation on regular code completion\nbenchmarks (e.g., HumanEval). We release all our long-context Granite code\nmodels under an Apache 2.0 license for both research and commercial use.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13739",
        "PDF Link": "https://arxiv.org/pdf/2407.13739",
        "Upvotes": "12",
        "Date": "2024-07-24"
    },
    {
        "Title": "Benchmarking Trustworthiness of Multimodal Large Language Models: A\n  Comprehensive Study",
        "Abstract": "Despite the superior capabilities of Multimodal Large Language Models (MLLMs)\nacross diverse tasks, they still face significant trustworthiness challenges.\nYet, current literature on the assessment of trustworthy MLLMs remains limited,\nlacking a holistic evaluation to offer thorough insights into future\nimprovements. In this work, we establish MultiTrust, the first comprehensive\nand unified benchmark on the trustworthiness of MLLMs across five primary\naspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark\nemploys a rigorous evaluation strategy that addresses both multimodal risks and\ncross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.\nExtensive experiments with 21 modern MLLMs reveal some previously unexplored\ntrustworthiness issues and risks, highlighting the complexities introduced by\nthe multimodality and underscoring the necessity for advanced methodologies to\nenhance their reliability. For instance, typical proprietary models still\nstruggle with the perception of visually confusing images and are vulnerable to\nmultimodal jailbreaking and adversarial attacks; MLLMs are more inclined to\ndisclose privacy in text and reveal ideological and cultural biases even when\npaired with irrelevant images in inference, indicating that the multimodality\namplifies the internal risks from base LLMs. Additionally, we release a\nscalable toolbox for standardized trustworthiness research, aiming to\nfacilitate future advancements in this important field. Code and resources are\npublicly available at: https://multi-trust.github.io/.",
        "ArXiv Link": "https://arxiv.org/abs/2406.07057",
        "PDF Link": "https://arxiv.org/pdf/2406.07057",
        "Upvotes": "10",
        "Date": "2024-07-24"
    },
    {
        "Title": "Attention Overflow: Language Model Input Blur during Long-Context\n  Missing Items Recommendation",
        "Abstract": "Large language models (LLMs) can suggest missing elements from items listed\nin a prompt, which can be used for list completion or recommendations based on\nusers' history. However, their performance degrades when presented with too\nmany items, as they start to suggest items already included in the input list.\nThis occurs at around 100 items for mid-2024 flagship LLMs. We evaluate this\nphenomenon on both synthetic problems (e.g., finding missing numbers in a given\nrange of shuffled integers) and realistic movie recommendation scenarios. We\nrefer to this issue as attention overflow, as preventing repetition\nrequires attending to all items simultaneously. Although iterative loops can\nmitigate this problem, their costs increase with the repetition rate, affecting\nthe language models' ability to derive novelty from lengthy inputs.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13481",
        "PDF Link": "https://arxiv.org/pdf/2407.13481",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "CodeV: Empowering LLMs for Verilog Generation through Multi-Level\n  Summarization",
        "Abstract": "The increasing complexity and high costs associated with modern processor\ndesign have led to a surge in demand for processor design automation.\nInstruction-tuned large language models (LLMs) have demonstrated remarkable\nperformance in automatically generating code for general-purpose programming\nlanguages like Python. However, these methods fail on hardware description\nlanguages (HDLs) like Verilog due to the scarcity of high-quality instruction\ntuning data, as even advanced LLMs like GPT-3.5 exhibit limited performance on\nVerilog generation. Regarding this issue, we observe that (1) Verilog code\ncollected from the real world has higher quality than those generated by LLMs.\n(2) LLMs like GPT-3.5 excel in summarizing Verilog code rather than generating\nit. Based on these observations, this paper introduces CodeV, a series of\nopen-source instruction-tuned Verilog generation LLMs. Instead of generating\ndescriptions first and then getting the corresponding code from advanced LLMs,\nwe prompt the LLM with Verilog code and let the LLM generate the corresponding\nnatural language description by multi-level summarization. Experimental results\nshow that CodeV relatively surpasses the previous open-source SOTA by 14.4%\n(BetterV in VerilogEval) and 11.3% (RTLCoder in RTLLM) respectively, and also\nrelatively outperforms previous commercial SOTA GPT-4 by 22.1% in VerilogEval.",
        "ArXiv Link": "https://arxiv.org/abs/2407.10424",
        "PDF Link": "https://arxiv.org/pdf/2407.10424",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive\n  Retrieval",
        "Abstract": "Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398\nreal-world queries collected from diverse domains (such as economics,\npsychology, robotics, software engineering, earth sciences, etc.), sourced from\nnaturally occurring or carefully curated human data. Extensive evaluation\nreveals that even state-of-the-art retrieval models perform poorly on BRIGHT.\nThe leading model on the MTEB leaderboard [38 ], which achieves a score of 59.0\nnDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate\nthat augmenting queries with Chain-of-Thought reasoning generated by large\nlanguage models (LLMs) improves performance by up to 12.2 points. Moreover,\nBRIGHT is robust against data leakage during pretraining of the benchmarked\nmodels as we validate by showing similar performance even when documents from\nthe benchmark are included in the training data. We believe that BRIGHT paves\nthe way for future research on retrieval systems in more realistic and\nchallenging settings. Our code and data are available at\nhttps://brightbenchmark.github.io.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12883",
        "PDF Link": "https://arxiv.org/pdf/2407.12883",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "CLAY: A Controllable Large-scale Generative Model for Creating\n  High-quality 3D Assets",
        "Abstract": "In the realm of digital creativity, our potential to craft intricate 3D\nworlds from imagination is often hampered by the limitations of existing\ndigital tools, which demand extensive expertise and efforts. To narrow this\ndisparity, we introduce CLAY, a 3D geometry and material generator designed to\neffortlessly transform human imagination into intricate 3D digital structures.\nCLAY supports classic text or image inputs as well as 3D-aware controls from\ndiverse primitives (multi-view images, voxels, bounding boxes, point clouds,\nimplicit representations, etc). At its core is a large-scale generative model\ncomposed of a multi-resolution Variational Autoencoder (VAE) and a minimalistic\nlatent Diffusion Transformer (DiT), to extract rich 3D priors directly from a\ndiverse range of 3D geometries. Specifically, it adopts neural fields to\nrepresent continuous and complete surfaces and uses a geometry generative\nmodule with pure transformer blocks in latent space. We present a progressive\ntraining scheme to train CLAY on an ultra large 3D model dataset obtained\nthrough a carefully designed processing pipeline, resulting in a 3D native\ngeometry generator with 1.5 billion parameters. For appearance generation, CLAY\nsets out to produce physically-based rendering (PBR) textures by employing a\nmulti-view material diffusion model that can generate 2K resolution textures\nwith diffuse, roughness, and metallic modalities. We demonstrate using CLAY for\na range of controllable 3D asset creations, from sketchy conceptual designs to\nproduction ready assets with intricate details. Even first time users can\neasily use CLAY to bring their vivid 3D imaginations to life, unleashing\nunlimited creativity.",
        "ArXiv Link": "https://arxiv.org/abs/2406.13897",
        "PDF Link": "https://arxiv.org/pdf/2406.13897",
        "Upvotes": "4",
        "Date": "2024-07-24"
    },
    {
        "Title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
        "Abstract": "In the field of language modeling, models augmented with retrieval components\nhave emerged as a promising solution to address several challenges faced in the\nnatural language processing (NLP) field, including knowledge grounding,\ninterpretability, and scalability. Despite the primary focus on NLP, we posit\nthat the paradigm of retrieval-enhancement can be extended to a broader\nspectrum of machine learning (ML) such as computer vision, time series\nprediction, and computational biology. Therefore, this work introduces a formal\nframework of this paradigm, Retrieval-Enhanced Machine Learning (REML), by\nsynthesizing the literature in various domains in ML with consistent notations\nwhich is missing from the current literature. Also, we found that while a\nnumber of studies employ retrieval components to augment their models, there is\na lack of integration with foundational Information Retrieval (IR) research. We\nbridge this gap between the seminal IR research and contemporary REML studies\nby investigating each component that comprises the REML framework. Ultimately,\nthe goal of this work is to equip researchers across various disciplines with a\ncomprehensive, formally structured framework of retrieval-enhanced models,\nthereby fostering interdisciplinary future research.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12982",
        "PDF Link": "https://arxiv.org/pdf/2407.12982",
        "Upvotes": "4",
        "Date": "2024-07-24"
    },
    {
        "Title": "A Comparative Study on Automatic Coding of Medical Letters with\n  Explainability",
        "Abstract": "This study aims to explore the implementation of Natural Language Processing\n(NLP) and machine learning (ML) techniques to automate the coding of medical\nletters with visualised explainability and light-weighted local computer\nsettings. Currently in clinical settings, coding is a manual process that\ninvolves assigning codes to each condition, procedure, and medication in a\npatient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There\nare preliminary research on automatic coding in this field using\nstate-of-the-art ML models; however, due to the complexity and size of the\nmodels, the real-world deployment is not achieved. To further facilitate the\npossibility of automatic coding practice, we explore some solutions in a local\ncomputer setting; in addition, we explore the function of explainability for\ntransparency of AI models. We used the publicly available MIMIC-III database\nand the HAN/HLAN network models for ICD code prediction purposes. We also\nexperimented with the mapping between ICD and SNOMED CT knowledge bases. In our\nexperiments, the models provided useful information for 97.98\\% of codes. The\nresult of this investigation can shed some light on implementing automatic\nclinical coding in practice, such as in hospital settings, on the local\ncomputers used by clinicians , project page\nhttps://github.com/Glenj01/Medical-Coding.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13638",
        "PDF Link": "https://arxiv.org/pdf/2407.13638",
        "Upvotes": "4",
        "Date": "2024-07-24"
    },
    {
        "Title": "Benchmark Agreement Testing Done Right: A Guide for LLM Benchmark\n  Evaluation",
        "Abstract": "Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench",
        "ArXiv Link": "https://arxiv.org/abs/2407.13696",
        "PDF Link": "https://arxiv.org/pdf/2407.13696",
        "Upvotes": "3",
        "Date": "2024-07-24"
    },
    {
        "Title": "PM-LLM-Benchmark: Evaluating Large Language Models on Process Mining\n  Tasks",
        "Abstract": "Large Language Models (LLMs) have the potential to semi-automate some process\nmining (PM) analyses. While commercial models are already adequate for many\nanalytics tasks, the competitive level of open-source LLMs in PM tasks is\nunknown. In this paper, we propose PM-LLM-Benchmark, the first comprehensive\nbenchmark for PM focusing on domain knowledge (process-mining-specific and\nprocess-specific) and on different implementation strategies. We focus also on\nthe challenges in creating such a benchmark, related to the public availability\nof the data and on evaluation biases by the LLMs. Overall, we observe that most\nof the considered LLMs can perform some process mining tasks at a satisfactory\nlevel, but tiny models that would run on edge devices are still inadequate. We\nalso conclude that while the proposed benchmark is useful for identifying LLMs\nthat are adequate for process mining tasks, further research is needed to\novercome the evaluation biases and perform a more thorough ranking of the\ncompetitive LLMs.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13244",
        "PDF Link": "https://arxiv.org/pdf/2407.13244",
        "Upvotes": "2",
        "Date": "2024-07-24"
    },
    {
        "Title": "Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language\n  Models",
        "Abstract": "Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but unfortunately, it suffers from\nsignificant performance degradation below 4-bit precision. An alternative\napproach involves training compressed models directly at a low bitwidth (e.g.,\nbinary or ternary models). However, the performance, training dynamics, and\nscaling trends of such models are not yet well understood. To address this\nissue, we train and openly release the Spectra LLM suite consisting of 54\nlanguage models ranging from 99M to 3.9B parameters, trained on 300B tokens.\nSpectra includes FloatLMs, post-training quantized QuantLMs (3, 4, 6, and 8\nbits), and ternary LLMs (TriLMs) - our improved architecture for ternary\nlanguage modeling, which significantly outperforms previously proposed ternary\nmodels of a given size (in bits), matching half-precision models at scale. For\nexample, TriLM 3.9B is (bit-wise) smaller than the half-precision FloatLM 830M,\nbut matches half-precision FloatLM 3.9B in commonsense reasoning and knowledge\nbenchmarks. However, TriLM 3.9B is also as toxic and stereotyping as FloatLM\n3.9B, a model six times larger in size. Additionally, TriLM 3.9B lags behind\nFloatLM in perplexity on validation splits and web-based corpora but performs\nbetter on less noisy datasets like Lambada and PennTreeBank.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\nhttps://github.com/NolanoOrg/SpectraSuite{https://github.com/NolanoOrg/SpectraSuite}.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12327",
        "PDF Link": "https://arxiv.org/pdf/2407.12327",
        "Upvotes": "64",
        "Date": "2024-07-24"
    },
    {
        "Title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
        "Abstract": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12077",
        "PDF Link": "https://arxiv.org/pdf/2407.12077",
        "Upvotes": "45",
        "Date": "2024-07-24"
    },
    {
        "Title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge\n  Bases",
        "Abstract": "LLM agents have demonstrated remarkable performance across various\napplications, primarily due to their advanced capabilities in reasoning,\nutilizing external knowledge and tools, calling APIs, and executing actions to\ninteract with environments. Current agents typically utilize a memory module or\na retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and\ninstances with similar embeddings from knowledge bases to inform task planning\nand execution. However, the reliance on unverified knowledge bases raises\nsignificant concerns about their safety and trustworthiness. To uncover such\nvulnerabilities, we propose a novel red teaming approach AgentPoison, the first\nbackdoor attack targeting generic and RAG-based LLM agents by poisoning their\nlong-term memory or RAG knowledge base. In particular, we form the trigger\ngeneration process as a constrained optimization to optimize backdoor triggers\nby mapping the triggered instances to a unique embedding space, so as to ensure\nthat whenever a user instruction contains the optimized backdoor trigger, the\nmalicious demonstrations are retrieved from the poisoned memory or knowledge\nbase with high probability. In the meantime, benign instructions without the\ntrigger will still maintain normal performance. Unlike conventional backdoor\nattacks, AgentPoison requires no additional model training or fine-tuning, and\nthe optimized backdoor trigger exhibits superior transferability, in-context\ncoherence, and stealthiness. Extensive experiments demonstrate AgentPoison's\neffectiveness in attacking three types of real-world LLM agents: RAG-based\nautonomous driving agent, knowledge-intensive QA agent, and healthcare\nEHRAgent. On each agent, AgentPoison achieves an average attack success rate\nhigher than 80% with minimal impact on benign performance (less than 1%) with a\npoison rate less than 0.1%.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12784",
        "PDF Link": "https://arxiv.org/pdf/2407.12784",
        "Upvotes": "44",
        "Date": "2024-07-24"
    },
    {
        "Title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
        "Abstract": "Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12580",
        "PDF Link": "https://arxiv.org/pdf/2407.12580",
        "Upvotes": "34",
        "Date": "2024-07-24"
    },
    {
        "Title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models",
        "Abstract": "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12772",
        "PDF Link": "https://arxiv.org/pdf/2407.12772",
        "Upvotes": "27",
        "Date": "2024-07-24"
    },
    {
        "Title": "Patch-Level Training for Large Language Models",
        "Abstract": "As Large Language Models (LLMs) achieve remarkable progress in language\nunderstanding and generation, their training efficiency has become a critical\nconcern. Traditionally, LLMs are trained to predict the next token in a\nsequence. Despite the success of token-level training, it suffers from\nconsiderable computational costs due to the need to process an extensive number\nof tokens. To mitigate this issue, this paper introduces patch-level training\nfor LLMs, which reduces the sequence length by compressing multiple tokens into\na single patch. During patch-level training, we feed the language model shorter\nsequences of patches and train it to predict the next patch, thereby processing\nthe majority of the training data at a significantly reduced computational\ncost. Following this, the model continues token-level training on the remaining\ntraining data to align with the inference mode. Experiments on a diverse range\nof models (370M-2.7B parameters) demonstrate that patch-level training can\nreduce overall computational costs to 0.5times, without compromising the\nmodel performance compared to token-level training. Source code:\nhttps://github.com/shaochenze/PatchTrain.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12665",
        "PDF Link": "https://arxiv.org/pdf/2407.12665",
        "Upvotes": "14",
        "Date": "2024-07-24"
    },
    {
        "Title": "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control",
        "Abstract": "Modern text-to-video synthesis models demonstrate coherent, photorealistic\ngeneration of complex videos from a text description. However, most existing\nmodels lack fine-grained control over camera movement, which is critical for\ndownstream applications related to content creation, visual effects, and 3D\nvision. Recently, new methods demonstrate the ability to generate videos with\ncontrollable camera poses these techniques leverage pre-trained U-Net-based\ndiffusion models that explicitly disentangle spatial and temporal generation.\nStill, no existing approach enables camera control for new, transformer-based\nvideo diffusion models that process spatial and temporal information jointly.\nHere, we propose to tame video transformers for 3D camera control using a\nControlNet-like conditioning mechanism that incorporates spatiotemporal camera\nembeddings based on Plucker coordinates. The approach demonstrates\nstate-of-the-art performance for controllable video generation after\nfine-tuning on the RealEstate10K dataset. To the best of our knowledge, our\nwork is the first to enable camera control for transformer-based video\ndiffusion models.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12781",
        "PDF Link": "https://arxiv.org/pdf/2407.12781",
        "Upvotes": "10",
        "Date": "2024-07-24"
    },
    {
        "Title": "Case2Code: Learning Inductive Reasoning with Synthetic Data",
        "Abstract": "Complex reasoning is an impressive ability shown by large language models\n(LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought\nprompting or iterative tool-using to solve challenging tasks step-by-step. In\nthis paper, we hope to focus on evaluating and teaching LLMs to conduct\ninductive reasoning, that is, LLMs are supposed to infer underlying rules by\nobserving examples or sequential transformations. However, collecting\nlarge-scale and diverse human-generated inductive data is challenging. We focus\non data synthesis in the code domain and propose a Case2Code task by\nexploiting the expressiveness and correctness of programs. Specifically, we\ncollect a diverse set of executable programs, synthesize input-output\ntransformations for each program, and force LLMs to infer the underlying code\nimplementations based on the synthetic I/O cases. We first evaluate\nrepresentative LLMs on the synthesized Case2Code task and demonstrate that the\nCase-to-code induction is challenging for LLMs. Then, we synthesize large-scale\nCase2Code training samples to train LLMs to perform inductive reasoning.\nExperimental results show that such induction training benefits not only in\ndistribution Case2Code performance but also enhances various coding abilities\nof trained LLMs, demonstrating the great potential of learning inductive\nreasoning via synthetic data.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12504",
        "PDF Link": "https://arxiv.org/pdf/2407.12504",
        "Upvotes": "7",
        "Date": "2024-07-24"
    },
    {
        "Title": "IMAGDressing-v1: Customizable Virtual Dressing",
        "Abstract": "Latest advances have achieved realistic virtual try-on (VTON) through\nlocalized garment inpainting using latent diffusion models, significantly\nenhancing consumers' online shopping experience. However, existing VTON\ntechnologies neglect the need for merchants to showcase garments\ncomprehensively, including flexible control over garments, optional faces,\nposes, and scenes. To address this issue, we define a virtual dressing (VD)\ntask focused on generating freely editable human images with fixed garments and\noptional conditions. Meanwhile, we design a comprehensive affinity metric index\n(CAMI) to evaluate the consistency between generated images and reference\ngarments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet\nthat captures semantic features from CLIP and texture features from VAE. We\npresent a hybrid attention module, including a frozen self-attention and a\ntrainable cross-attention, to integrate garment features from the garment UNet\ninto a frozen denoising UNet, ensuring users can control different scenes\nthrough text. IMAGDressing-v1 can be combined with other extension plugins,\nsuch as ControlNet and IP-Adapter, to enhance the diversity and controllability\nof generated images. Furthermore, to address the lack of data, we release the\ninteractive garment pairing (IGPair) dataset, containing over 300,000 pairs of\nclothing and dressed images, and establish a standard pipeline for data\nassembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves\nstate-of-the-art human image synthesis performance under various controlled\nconditions. The code and model will be available at\nhttps://github.com/muzishen/IMAGDressing.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12705",
        "PDF Link": "https://arxiv.org/pdf/2407.12705",
        "Upvotes": "7",
        "Date": "2024-07-24"
    },
    {
        "Title": "Goldfish: Vision-Language Understanding of Arbitrarily Long Videos",
        "Abstract": "Most current LLM-based models for video understanding can process videos\nwithin minutes. However, they struggle with lengthy videos due to challenges\nsuch as \"noise and redundancy\", as well as \"memory and computation\"\nconstraints. In this paper, we present Goldfish, a methodology tailored for\ncomprehending videos of arbitrary lengths. We also introduce the TVQA-long\nbenchmark, specifically designed to evaluate models' capabilities in\nunderstanding long videos with questions in both vision and text content.\nGoldfish approaches these challenges with an efficient retrieval mechanism that\ninitially gathers the top-k video clips relevant to the instruction before\nproceeding to provide the desired response. This design of the retrieval\nmechanism enables the Goldfish to efficiently process arbitrarily long video\nsequences, facilitating its application in contexts such as movies or\ntelevision series. To facilitate the retrieval process, we developed\nMiniGPT4-Video that generates detailed descriptions for the video clips. In\naddressing the scarcity of benchmarks for long video evaluation, we adapted the\nTVQA short video benchmark for extended content analysis by aggregating\nquestions from entire episodes, thereby shifting the evaluation from partial to\nfull episode comprehension. We attained a 41.78% accuracy rate on the TVQA-long\nbenchmark, surpassing previous methods by 14.94%. Our MiniGPT4-Video also shows\nexceptional performance in short video comprehension, exceeding existing\nstate-of-the-art methods by 3.23%, 2.03%, 16.5% and 23.59% on the MSVD, MSRVTT,\nTGIF, and TVQA short video benchmarks, respectively. These results indicate\nthat our models have significant improvements in both long and short-video\nunderstanding. Our models and code have been made publicly available at\nhttps://vision-cair.github.io/Goldfish_website/",
        "ArXiv Link": "https://arxiv.org/abs/2407.12679",
        "PDF Link": "https://arxiv.org/pdf/2407.12679",
        "Upvotes": "6",
        "Date": "2024-07-24"
    },
    {
        "Title": "AUITestAgent: Automatic Requirements Oriented GUI Function Testing",
        "Abstract": "The Graphical User Interface (GUI) is how users interact with mobile apps. To\nensure it functions properly, testing engineers have to make sure it functions\nas intended, based on test requirements that are typically written in natural\nlanguage. While widely adopted manual testing and script-based methods are\neffective, they demand substantial effort due to the vast number of GUI pages\nand rapid iterations in modern mobile apps. This paper introduces AUITestAgent,\nthe first automatic, natural language-driven GUI testing tool for mobile apps,\ncapable of fully automating the entire process of GUI interaction and function\nverification. Since test requirements typically contain interaction commands\nand verification oracles. AUITestAgent can extract GUI interactions from test\nrequirements via dynamically organized agents. Then, AUITestAgent employs a\nmulti-dimensional data extraction strategy to retrieve data relevant to the\ntest requirements from the interaction trace and perform verification.\nExperiments on customized benchmarks demonstrate that AUITestAgent outperforms\nexisting tools in the quality of generated GUI interactions and achieved the\naccuracy of verifications of 94%. Moreover, field deployment in Meituan has\nshown AUITestAgent's practical usability, with it detecting 4 new functional\nbugs during 10 regression tests in two months.",
        "ArXiv Link": "https://arxiv.org/abs/2407.09018",
        "PDF Link": "https://arxiv.org/pdf/2407.09018",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features",
        "Abstract": "While most music generation models use textual or parametric conditioning\n(e.g. tempo, harmony, musical genre), we propose to condition a language model\nbased music generation system with audio input. Our exploration involves two\ndistinct strategies. The first strategy, termed textual inversion, leverages a\npre-trained text-to-music model to map audio input to corresponding\n\"pseudowords\" in the textual embedding space. For the second model we train a\nmusic language model from scratch jointly with a text conditioner and a\nquantized audio feature extractor. At inference time, we can mix textual and\naudio conditioning and balance them thanks to a novel double classifier free\nguidance method. We conduct automatic and human studies that validates our\napproach. We will release the code and we provide music samples on\nhttps://musicgenstyle.github.io in order to show the quality of our model.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12563",
        "PDF Link": "https://arxiv.org/pdf/2407.12563",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "Splatfacto-W: A Nerfstudio Implementation of Gaussian Splatting for\n  Unconstrained Photo Collections",
        "Abstract": "Novel view synthesis from unconstrained in-the-wild image collections remains\na significant yet challenging task due to photometric variations and transient\noccluders that complicate accurate scene reconstruction. Previous methods have\napproached these issues by integrating per-image appearance features embeddings\nin Neural Radiance Fields (NeRFs). Although 3D Gaussian Splatting (3DGS) offers\nfaster training and real-time rendering, adapting it for unconstrained image\ncollections is non-trivial due to the substantially different architecture. In\nthis paper, we introduce Splatfacto-W, an approach that integrates per-Gaussian\nneural color features and per-image appearance embeddings into the\nrasterization process, along with a spherical harmonics-based background model\nto represent varying photometric appearances and better depict backgrounds. Our\nkey contributions include latent appearance modeling, efficient transient\nobject handling, and precise background modeling. Splatfacto-W delivers\nhigh-quality, real-time novel view synthesis with improved scene consistency in\nin-the-wild scenarios. Our method improves the Peak Signal-to-Noise Ratio\n(PSNR) by an average of 5.3 dB compared to 3DGS, enhances training speed by 150\ntimes compared to NeRF-based methods, and achieves a similar rendering speed to\n3DGS. Additional video results and code integrated into Nerfstudio are\navailable at https://kevinxu02.github.io/splatfactow/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12306",
        "PDF Link": "https://arxiv.org/pdf/2407.12306",
        "Upvotes": "5",
        "Date": "2024-07-24"
    },
    {
        "Title": "The Art of Saying No: Contextual Noncompliance in Language Models",
        "Abstract": "Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12043",
        "PDF Link": "https://arxiv.org/pdf/2407.12043",
        "Upvotes": "4",
        "Date": "2024-07-24"
    },
    {
        "Title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large\n  Vision-Language Models",
        "Abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs),\nthere is a burgeoning initiative to harness LLMs for instruction following\nrobotic navigation. Such a trend underscores the potential of LLMs to\ngeneralize navigational reasoning and diverse language understanding. However,\na significant discrepancy in agent performance is observed when integrating\nLLMs in the Vision-and-Language navigation (VLN) tasks compared to previous\ndownstream specialist models. Furthermore, the inherent capacity of language to\ninterpret and facilitate communication in agent interactions is often\nunderutilized in these integrations. In this work, we strive to bridge the\ndivide between VLN-specialized models and LLM-based navigation paradigms, while\nmaintaining the interpretative prowess of LLMs in generating linguistic\nnavigational reasoning. By aligning visual content in a frozen LLM, we\nencompass visual observation comprehension for LLMs and exploit a way to\nincorporate LLMs and navigation policy networks for effective action\npredictions and navigational reasoning. We demonstrate the data efficiency of\nthe proposed methods and eliminate the gap between LM-based agents and\nstate-of-the-art VLN specialists.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12366",
        "PDF Link": "https://arxiv.org/pdf/2407.12366",
        "Upvotes": "3",
        "Date": "2024-07-24"
    },
    {
        "Title": "ThinkGrasp: A Vision-Language System for Strategic Part Grasping in\n  Clutter",
        "Abstract": "Robotic grasping in cluttered environments remains a significant challenge\ndue to occlusions and complex object arrangements. We have developed\nThinkGrasp, a plug-and-play vision-language grasping system that makes use of\nGPT-4o's advanced contextual reasoning for heavy clutter environment grasping\nstrategies. ThinkGrasp can effectively identify and generate grasp poses for\ntarget objects, even when they are heavily obstructed or nearly invisible, by\nusing goal-oriented language to guide the removal of obstructing objects. This\napproach progressively uncovers the target object and ultimately grasps it with\na few steps and a high success rate. In both simulated and real experiments,\nThinkGrasp achieved a high success rate and significantly outperformed\nstate-of-the-art methods in heavily cluttered environments or with diverse\nunseen objects, demonstrating strong generalization capabilities.",
        "ArXiv Link": "https://arxiv.org/abs/2407.11298",
        "PDF Link": "https://arxiv.org/pdf/2407.11298",
        "Upvotes": "3",
        "Date": "2024-07-24"
    },
    {
        "Title": "Practical Unlearning for Large Language Models",
        "Abstract": "While LLMs have demonstrated impressive performance across various domains\nand tasks, their security issues have become increasingly severe. Machine\nunlearning (MU) has emerged as a promising solution to address these issues by\nremoving the influence of undesired data on the target model without\ncompromising its utility in other aspects. MU typically assumes full access to\nthe original training data to preserve utility, which is difficult to achieve\nin LLM unlearning. Existing LLM unlearning methods often assume access to data\nmost affected by undesired data unlearning. However, this assumption\nunderestimates the entanglement among various LLM capabilities and ignores data\naccess limitations due to various issues. Moreover, these LLM unlearning\nmethods do not sufficiently consider that unlearning requests in real-world\nscenarios are continuously emerging. To overcome these challenges and achieve\npractical LLM unlearning, we propose the O3 framework. The O3 framework\nincludes an Out-Of-Distribution (OOD) detector to measure the similarity\nbetween input and unlearning data, and an Orthogonal low-rank adapter (LoRA)\nfor continuously unlearning requested data. The OOD detector is trained with a\nnovel contrastive entropy loss and utilizes a local-global layer-aggregated\nscoring mechanism. The orthogonal LoRA achieves parameter disentanglement among\ncontinual unlearning requests. During inference, our O3 framework can smartly\ndecide whether and to what extent to load the unlearning LoRA based on the OOD\ndetector's predictions. Notably, O3's effectiveness does not rely on any\nretained data. We conducted extensive experiments on O3 and state-of-the-art\nLLM unlearning methods across three tasks and seven datasets. The results\nindicate that O3 consistently achieves the best trade-off between unlearning\neffectiveness and utility preservation, especially when facing continuous\nunlearning requests.",
        "ArXiv Link": "https://arxiv.org/abs/2407.10223",
        "PDF Link": "https://arxiv.org/pdf/2407.10223",
        "Upvotes": "3",
        "Date": "2024-07-24"
    },
    {
        "Title": "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in\n  Grammatical Error Detection",
        "Abstract": "Grammatical Error Detection (GED) methods rely heavily on human annotated\nerror corpora. However, these annotations are unavailable in many low-resource\nlanguages. In this paper, we investigate GED in this context. Leveraging the\nzero-shot cross-lingual transfer capabilities of multilingual pre-trained\nlanguage models, we train a model using data from a diverse set of languages to\ngenerate synthetic errors in other languages. These synthetic error corpora are\nthen used to train a GED model. Specifically we propose a two-stage fine-tuning\npipeline where the GED model is first fine-tuned on multilingual synthetic data\nfrom target languages followed by fine-tuning on human-annotated GED corpora\nfrom source languages. This approach outperforms current state-of-the-art\nannotation-free GED methods. We also analyse the errors produced by our method\nand other strong baselines, finding that our approach produces errors that are\nmore diverse and more similar to human errors.",
        "ArXiv Link": "https://arxiv.org/abs/2407.11854",
        "PDF Link": "https://arxiv.org/pdf/2407.11854",
        "Upvotes": "2",
        "Date": "2024-07-24"
    },
    {
        "Title": "Towards Understanding Unsafe Video Generation",
        "Abstract": "Video generation models (VGMs) have demonstrated the capability to synthesize\nhigh-quality output. It is important to understand their potential to produce\nunsafe content, such as violent or terrifying videos. In this work, we provide\na comprehensive understanding of unsafe video generation.\n  First, to confirm the possibility that these models could indeed generate\nunsafe videos, we choose unsafe content generation prompts collected from 4chan\nand Lexica, and three open-source SOTA VGMs to generate unsafe videos. After\nfiltering out duplicates and poorly generated content, we created an initial\nset of 2112 unsafe videos from an original pool of 5607 videos. Through\nclustering and thematic coding analysis of these generated videos, we identify\n5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic,\nViolent/Bloody, and Political. With IRB approval, we then recruit online\nparticipants to help label the generated videos. Based on the annotations\nsubmitted by 403 participants, we identified 937 unsafe videos from the initial\nvideo set. With the labeled information and the corresponding prompts, we\ncreated the first dataset of unsafe videos generated by VGMs.\n  We then study possible defense mechanisms to prevent the generation of unsafe\nvideos. Existing defense methods in image generation focus on filtering either\ninput prompt or output results. We propose a new approach called Latent\nVariable Defense (LVD), which works within the model's internal sampling\nprocess. LVD can achieve 0.90 defense accuracy while reducing time and\ncomputing resources by 10x when sampling a large number of unsafe prompts.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12581",
        "PDF Link": "https://arxiv.org/pdf/2407.12581",
        "Upvotes": "-",
        "Date": "2024-07-24"
    },
    {
        "Title": "AppWorld: A Controllable World of Apps and People for Benchmarking\n  Interactive Coding Agents",
        "Abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering\ngroceries for a household), must not only operate multiple apps (e.g., notes,\nmessaging, shopping app) via APIs, but also generate rich code with complex\ncontrol flow in an iterative manner based on their interaction with the\nenvironment. However, existing benchmarks for tool use are inadequate, as they\nonly cover tasks that require a simple sequence of API calls.\n  To remedy this gap, we built AppWorld Engine, a high-quality\nexecution environment (60K lines of code) of 9 day-to-day apps operable via 457\nAPIs and populated with realistic digital activities simulating the lives of\n~100 fictitious users. We then created AppWorld Benchmark (40K lines\nof code), a suite of 750 natural, diverse, and challenging autonomous agent\ntasks requiring rich and interactive code generation. It supports robust\nprogrammatic evaluation with state-based unit tests, allowing for different\nways of completing a task while also checking for unexpected changes, i.e.,\ncollateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our\n'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least\n16% fewer. This highlights the benchmark's difficulty and AppWorld's potential\nto push the frontiers of interactive coding agents. The project website is\navailable at https://appworld.dev/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18901",
        "PDF Link": "https://arxiv.org/pdf/2407.18901",
        "Upvotes": "3",
        "Date": "2024-07-29"
    },
    {
        "Title": "Floating No More: Object-Ground Reconstruction from a Single Image",
        "Abstract": "Recent advancements in 3D object reconstruction from single images have\nprimarily focused on improving the accuracy of object shapes. Yet, these\ntechniques often fail to accurately capture the inter-relation between the\nobject, ground, and camera. As a result, the reconstructed objects often appear\nfloating or tilted when placed on flat surfaces. This limitation significantly\naffects 3D-aware image editing applications like shadow rendering and object\npose manipulation. To address this issue, we introduce ORG (Object\nReconstruction with Ground), a novel task aimed at reconstructing 3D object\ngeometry in conjunction with the ground surface. Our method uses two compact\npixel-level representations to depict the relationship between camera, object,\nand ground. Experiments show that the proposed ORG model can effectively\nreconstruct object-ground geometry on unseen data, significantly enhancing the\nquality of shadow generation and pose manipulation compared to conventional\nsingle-image 3D reconstruction techniques.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18914",
        "PDF Link": "https://arxiv.org/pdf/2407.18914",
        "Upvotes": "2",
        "Date": "2024-07-29"
    },
    {
        "Title": "Wolf: Captioning Everything with a World Summarization Framework",
        "Abstract": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18908",
        "PDF Link": "https://arxiv.org/pdf/2407.18908",
        "Upvotes": "1",
        "Date": "2024-07-29"
    },
    {
        "Title": "SHIC: Shape-Image Correspondences with no Keypoint Supervision",
        "Abstract": "Canonical surface mapping generalizes keypoint detection by assigning each\npixel of an object to a corresponding point in a 3D template. Popularised by\nDensePose for the analysis of humans, authors have since attempted to apply the\nconcept to more categories, but with limited success due to the high cost of\nmanual supervision. In this work, we introduce SHIC, a method to learn\ncanonical maps without manual supervision which achieves better results than\nsupervised methods for most categories. Our idea is to leverage foundation\ncomputer vision models such as DINO and Stable Diffusion that are open-ended\nand thus possess excellent priors over natural categories. SHIC reduces the\nproblem of estimating image-to-template correspondences to predicting\nimage-to-image correspondences using features from the foundation models. The\nreduction works by matching images of the object to non-photorealistic renders\nof the template, which emulates the process of collecting manual annotations\nfor this task. These correspondences are then used to supervise high-quality\ncanonical maps for any object of interest. We also show that image generators\ncan further improve the realism of the template views, which provide an\nadditional source of supervision for the model.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18907",
        "PDF Link": "https://arxiv.org/pdf/2407.18907",
        "Upvotes": "1",
        "Date": "2024-07-29"
    },
    {
        "Title": "Lessons from Learning to Spin \"Pens\"",
        "Abstract": "In-hand manipulation of pen-like objects is an important skill in our daily\nlives, as many tools such as hammers and screwdrivers are similarly shaped.\nHowever, current learning-based methods struggle with this task due to a lack\nof high-quality demonstrations and the significant gap between simulation and\nthe real world. In this work, we push the boundaries of learning-based in-hand\nmanipulation systems by demonstrating the capability to spin pen-like objects.\nWe first use reinforcement learning to train an oracle policy with privileged\ninformation and generate a high-fidelity trajectory dataset in simulation. This\nserves two purposes: 1) pre-training a sensorimotor policy in simulation; 2)\nconducting open-loop trajectory replay in the real world. We then fine-tune the\nsensorimotor policy using these real-world trajectories to adapt it to the real\nworld dynamics. With less than 50 trajectories, our policy learns to rotate\nmore than ten pen-like objects with different physical properties for multiple\nrevolutions. We present a comprehensive analysis of our design choices and\nshare the lessons learned during development.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18902",
        "PDF Link": "https://arxiv.org/pdf/2407.18902",
        "Upvotes": "-",
        "Date": "2024-07-29"
    },
    {
        "Title": "VSSD: Vision Mamba with Non-Casual State Space Duality",
        "Abstract": "Vision transformers have significantly advanced the field of computer vision,\noffering robust modeling capabilities and global receptive field. However,\ntheir high computational demands limit their applicability in processing long\nsequences. To tackle this issue, State Space Models (SSMs) have gained\nprominence in vision tasks as they offer linear computational complexity.\nRecently, State Space Duality (SSD), an improved variant of SSMs, was\nintroduced in Mamba2 to enhance model performance and efficiency. However, the\ninherent causal nature of SSD/SSMs restricts their applications in non-causal\nvision tasks. To address this limitation, we introduce Visual State Space\nDuality (VSSD) model, which has a non-causal format of SSD. Specifically, we\npropose to discard the magnitude of interactions between the hidden state and\ntokens while preserving their relative weights, which relieves the dependencies\nof token contribution on previous tokens. Together with the involvement of\nmulti-scan strategies, we show that the scanning results can be integrated to\nachieve non-causality, which not only improves the performance of SSD in vision\ntasks but also enhances its efficiency. We conduct extensive experiments on\nvarious benchmarks including image classification, detection, and segmentation,\nwhere VSSD surpasses existing state-of-the-art SSM-based models. Code and\nweights are available at https://github.com/YuHengsss/VSSD.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18559",
        "PDF Link": "https://arxiv.org/pdf/2407.18559",
        "Upvotes": "-",
        "Date": "2024-07-29"
    },
    {
        "Title": "Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model",
        "Abstract": "This paper addresses an important problem of object addition for images with\nonly text guidance. It is challenging because the new object must be integrated\nseamlessly into the image with consistent visual context, such as lighting,\ntexture, and spatial location. While existing text-guided image inpainting\nmethods can add objects, they either fail to preserve the background\nconsistency or involve cumbersome human intervention in specifying bounding\nboxes or user-scribbled masks. To tackle this challenge, we introduce Diffree,\na Text-to-Image (T2I) model that facilitates text-guided object addition with\nonly text control. To this end, we curate OABench, an exquisite synthetic\ndataset by removing objects with advanced image inpainting techniques. OABench\ncomprises 74K real-world tuples of an original image, an inpainted image with\nthe object removed, an object mask, and object descriptions. Trained on OABench\nusing the Stable Diffusion model with an additional mask prediction module,\nDiffree uniquely predicts the position of the new object and achieves object\naddition with guidance from only text. Extensive experiments demonstrate that\nDiffree excels in adding new objects with a high success rate while maintaining\nbackground consistency, spatial appropriateness, and object relevance and\nquality.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16982",
        "PDF Link": "https://arxiv.org/pdf/2407.16982",
        "Upvotes": "33",
        "Date": "2024-07-29"
    },
    {
        "Title": "LAMBDA: A Large Model Based Data Agent",
        "Abstract": "We introduce ``LAMBDA,\" a novel open-source, code-free multi-agent data\nanalysis system that that harnesses the power of large models. LAMBDA is\ndesigned to address data analysis challenges in complex data-driven\napplications through the use of innovatively designed data agents that operate\niteratively and generatively using natural language. At the core of LAMBDA are\ntwo key agent roles: the programmer and the inspector, which are engineered to\nwork together seamlessly. Specifically, the programmer generates code based on\nthe user's instructions and domain-specific knowledge, enhanced by advanced\nmodels. Meanwhile, the inspector debugs the code when necessary. To ensure\nrobustness and handle adverse scenarios, LAMBDA features a user interface that\nallows direct user intervention in the operational loop. Additionally, LAMBDA\ncan flexibly integrate external models and algorithms through our knowledge\nintegration mechanism, catering to the needs of customized data analysis.\nLAMBDA has demonstrated strong performance on various machine learning\ndatasets. It has the potential to enhance data science practice and analysis\nparadigm by seamlessly integrating human and artificial intelligence, making it\nmore accessible, effective, and efficient for individuals from diverse\nbackgrounds. The strong performance of LAMBDA in solving data science problems\nis demonstrated in several case studies, which are presented at\nhttps://www.polyu.edu.hk/ama/cmfai/lambda.html.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17535",
        "PDF Link": "https://arxiv.org/pdf/2407.17535",
        "Upvotes": "26",
        "Date": "2024-07-29"
    },
    {
        "Title": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents",
        "Abstract": "AI agents have drawn increasing attention mostly on their ability to perceive\nenvironments, understand tasks, and autonomously achieve goals. To advance\nresearch on AI agents in mobile scenarios, we introduce the Android\nMulti-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for\ngeneralist mobile GUI-control agents. Their capabilities of completing complex\ntasks by directly interacting with the graphical user interface (GUI) on mobile\ndevices are trained and evaluated with the proposed dataset. AMEX comprises\nover 104K high-resolution screenshots from 110 popular mobile applications,\nwhich are annotated at multiple levels. Unlike existing mobile device-control\ndatasets, e.g., MoTIF, AitW, etc., AMEX includes three levels of annotations:\nGUI interactive element grounding, GUI screen and element functionality\ndescriptions, and complex natural language instructions, each averaging 13\nsteps with stepwise GUI-action chains. We develop this dataset from a more\ninstructive and detailed perspective, complementing the general settings of\nexisting datasets. Additionally, we develop a baseline model SPHINX Agent and\ncompare its performance across state-of-the-art agents trained on other\ndatasets. To facilitate further research, we open-source our dataset, models,\nand relevant evaluation tools. The project is available at\nhttps://yuxiangchai.github.io/AMEX/",
        "ArXiv Link": "https://arxiv.org/abs/2407.17490",
        "PDF Link": "https://arxiv.org/pdf/2407.17490",
        "Upvotes": "24",
        "Date": "2024-07-29"
    },
    {
        "Title": "Very Large-Scale Multi-Agent Simulation in AgentScope",
        "Abstract": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, centralized workflow\norchestration, and both inter-agent and agent-environment interactions among\nagents. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of the\nproposed enhancements in AgentScope, and provide detailed observations and\ndiscussions to highlight the great potential of applying multi-agent systems in\nlarge-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope to inspire further research and\ndevelopment in large-scale multi-agent simulations.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17789",
        "PDF Link": "https://arxiv.org/pdf/2407.17789",
        "Upvotes": "18",
        "Date": "2024-07-29"
    },
    {
        "Title": "BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular\n  Depth Estimation",
        "Abstract": "By training over large-scale datasets, zero-shot monocular depth estimation\n(MDE) methods show robust performance in the wild but often suffer from\ninsufficiently precise details. Although recent diffusion-based MDE approaches\nexhibit appealing detail extraction ability, they still struggle in\ngeometrically challenging scenes due to the difficulty of gaining robust\ngeometric priors from diverse datasets. To leverage the complementary merits of\nboth worlds, we propose BetterDepth to efficiently achieve geometrically\ncorrect affine-invariant MDE performance while capturing fine-grained details.\nSpecifically, BetterDepth is a conditional diffusion-based refiner that takes\nthe prediction from pre-trained MDE models as depth conditioning, in which the\nglobal depth context is well-captured, and iteratively refines details based on\nthe input image. For the training of such a refiner, we propose global\npre-alignment and local patch masking methods to ensure the faithfulness of\nBetterDepth to depth conditioning while learning to capture fine-grained scene\ndetails. By efficient training on small-scale synthetic datasets, BetterDepth\nachieves state-of-the-art zero-shot MDE performance on diverse public datasets\nand in-the-wild scenes. Moreover, BetterDepth can improve the performance of\nother MDE models in a plug-and-play manner without additional re-training.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17952",
        "PDF Link": "https://arxiv.org/pdf/2407.17952",
        "Upvotes": "17",
        "Date": "2024-07-29"
    },
    {
        "Title": "Course-Correction: Safety Alignment Using Synthetic Preferences",
        "Abstract": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of course-correction,\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the C^2-Eval benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\nC^2-Syn, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, Llama2-Chat 7B and\nQwen2 7B, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16637",
        "PDF Link": "https://arxiv.org/pdf/2407.16637",
        "Upvotes": "16",
        "Date": "2024-07-29"
    },
    {
        "Title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?",
        "Abstract": "The pretraining data of today's strongest language models is opaque. In\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about the pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16607",
        "PDF Link": "https://arxiv.org/pdf/2407.16607",
        "Upvotes": "15",
        "Date": "2024-07-29"
    },
    {
        "Title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
        "Abstract": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
        "ArXiv Link": "https://arxiv.org/abs/2407.18121",
        "PDF Link": "https://arxiv.org/pdf/2407.18121",
        "Upvotes": "13",
        "Date": "2024-07-29"
    },
    {
        "Title": "LKCell: Efficient Cell Nuclei Instance Segmentation with Large\n  Convolution Kernels",
        "Abstract": "The segmentation of cell nuclei in tissue images stained with the blood dye\nhematoxylin and eosin (H&E) is essential for various clinical applications\nand analyses. Due to the complex characteristics of cellular morphology, a\nlarge receptive field is considered crucial for generating high-quality\nsegmentation. However, previous methods face challenges in achieving a balance\nbetween the receptive field and computational burden. To address this issue, we\npropose LKCell, a high-accuracy and efficient cell segmentation method. Its\ncore insight lies in unleashing the potential of large convolution kernels to\nachieve computationally efficient large receptive fields. Specifically, (1) We\ntransfer pre-trained large convolution kernel models to the medical domain for\nthe first time, demonstrating their effectiveness in cell segmentation. (2) We\nanalyze the redundancy of previous methods and design a new segmentation\ndecoder based on large convolution kernels. It achieves higher performance\nwhile significantly reducing the number of parameters. We evaluate our method\non the most challenging benchmark and achieve state-of-the-art results (0.5080\nmPQ) in cell nuclei instance segmentation with only 21.6% FLOPs compared with\nthe previous leading method. Our source code and models are available at\nhttps://github.com/hustvl/LKCell.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18054",
        "PDF Link": "https://arxiv.org/pdf/2407.18054",
        "Upvotes": "7",
        "Date": "2024-07-29"
    },
    {
        "Title": "Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic",
        "Abstract": "Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18129",
        "PDF Link": "https://arxiv.org/pdf/2407.18129",
        "Upvotes": "7",
        "Date": "2024-07-29"
    },
    {
        "Title": "The FIGNEWS Shared Task on News Media Narratives",
        "Abstract": "We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed.",
        "ArXiv Link": "https://arxiv.org/abs/2407.18147",
        "PDF Link": "https://arxiv.org/pdf/2407.18147",
        "Upvotes": "6",
        "Date": "2024-07-29"
    },
    {
        "Title": "Text-Driven Neural Collaborative Filtering Model for Paper Source\n  Tracing",
        "Abstract": "Identifying significant references within the complex interrelations of a\ncitation knowledge graph is challenging, which encompasses connections through\ncitations, authorship, keywords, and other relational attributes. The Paper\nSource Tracing (PST) task seeks to automate the identification of pivotal\nreferences for given scholarly articles utilizing advanced data mining\ntechniques. In the KDD CUP 2024, we design a recommendation-based framework\ntailored for the PST task. This framework employs the Neural Collaborative\nFiltering (NCF) model to generate final predictions. To process the textual\nattributes of the papers and extract input features for the model, we utilize\nSciBERT, a pre-trained language model. According to the experimental results,\nour method achieved a score of 0.37814 on the Mean Average Precision (MAP)\nmetric, outperforming baseline models and ranking 11th among all participating\nteams. The source code is publicly available at\nhttps://github.com/MyLove-XAB/KDDCupFinal.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17722",
        "PDF Link": "https://arxiv.org/pdf/2407.17722",
        "Upvotes": "3",
        "Date": "2024-07-29"
    },
    {
        "Title": "OpenDevin: An Open Platform for AI Software Developers as Generalist\n  Agents",
        "Abstract": "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenDevin, a platform for the development of powerful and\nflexible AI agents that interact with the world in similar ways to those of a\nhuman developer: by writing code, interacting with a command line, and browsing\nthe web. We describe how the platform allows for the implementation of new\nagents, safe interaction with sandboxed environments for code execution,\ncoordination between multiple agents, and incorporation of evaluation\nbenchmarks. Based on our currently incorporated benchmarks, we perform an\nevaluation of agents over 15 challenging tasks, including software engineering\n(e.g., SWE-Bench) and web browsing (e.g., WebArena), among others. Released\nunder the permissive MIT license, OpenDevin is a community project spanning\nacademia and industry with more than 1.3K contributions from over 160\ncontributors and will improve going forward.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16741",
        "PDF Link": "https://arxiv.org/pdf/2407.16741",
        "Upvotes": "56",
        "Date": "2024-07-29"
    },
    {
        "Title": "VILA^2: VILA Augmented VILA",
        "Abstract": "Visual language models (VLMs) have rapidly progressed, driven by the success\nof large language models (LLMs). While model architectures and training\ninfrastructures advance rapidly, data curation remains under-explored. When\ndata quantity and quality become a bottleneck, existing work either directly\ncrawls more raw data from the Internet that does not have a guarantee of data\nquality or distills from black-box commercial models (e.g., GPT-4V / Gemini)\ncausing the performance upper bounded by that model. In this work, we introduce\na novel approach that includes a self-augment step and a specialist-augment\nstep to iteratively improve data quality and model performance. In the\nself-augment step, a VLM recaptions its own pretraining data to enhance data\nquality, and then retrains from scratch using this refined dataset to improve\nmodel performance. This process can iterate for several rounds. Once\nself-augmentation saturates, we employ several specialist VLMs finetuned from\nthe self-augmented VLM with domain-specific expertise, to further infuse\nspecialist knowledge into the generalist VLM through task-oriented recaptioning\nand retraining. With the combined self-augmented and specialist-augmented\ntraining, we introduce VILA^2 (VILA-augmented-VILA), a VLM family that\nconsistently improves the accuracy on a wide range of tasks over prior art, and\nachieves new state-of-the-art results on MMMU leaderboard among open-sourced\nmodels.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17453",
        "PDF Link": "https://arxiv.org/pdf/2407.17453",
        "Upvotes": "33",
        "Date": "2024-07-29"
    },
    {
        "Title": "HumanVid: Demystifying Training Data for Camera-controllable Human Image\n  Animation",
        "Abstract": "Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation.To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at https://github.com/zhenzhiwang/HumanVid/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17438",
        "PDF Link": "https://arxiv.org/pdf/2407.17438",
        "Upvotes": "19",
        "Date": "2024-07-29"
    },
    {
        "Title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models",
        "Abstract": "Despite the advanced intelligence abilities of large language models (LLMs)\nin various applications, they still face significant computational and storage\ndemands. Knowledge Distillation (KD) has emerged as an effective strategy to\nimprove the performance of a smaller LLM (i.e., the student model) by\ntransferring knowledge from a high-performing LLM (i.e., the teacher model).\nPrevailing techniques in LLM distillation typically use a black-box model API\nto generate high-quality pretrained and aligned datasets, or utilize white-box\ndistillation by altering the loss function to better transfer knowledge from\nthe teacher LLM. However, these methods ignore the knowledge differences\nbetween the student and teacher LLMs across domains. This results in excessive\nfocus on domains with minimal performance gaps and insufficient attention to\ndomains with large gaps, reducing overall performance. In this paper, we\nintroduce a new LLM distillation framework called DDK, which dynamically\nadjusts the composition of the distillation dataset in a smooth manner\naccording to the domain performance differences between the teacher and student\nmodels, making the distillation process more stable and effective. Extensive\nevaluations show that DDK significantly improves the performance of student\nmodels, outperforming both continuously pretrained baselines and existing\nknowledge distillation methods by a large margin.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16154",
        "PDF Link": "https://arxiv.org/pdf/2407.16154",
        "Upvotes": "16",
        "Date": "2024-07-29"
    },
    {
        "Title": "PERSONA: A Reproducible Testbed for Pluralistic Alignment",
        "Abstract": "The rapid advancement of language models (LMs) necessitates robust alignment\nwith diverse user values. However, current preference optimization approaches\noften fail to capture the plurality of user opinions, instead reinforcing\nmajority viewpoints and marginalizing minority perspectives. We introduce\nPERSONA, a reproducible test bed designed to evaluate and improve pluralistic\nalignment of LMs. We procedurally generate diverse user profiles from US census\ndata, resulting in 1,586 synthetic personas with varied demographic and\nidiosyncratic attributes. We then generate a large-scale evaluation dataset\ncontaining 3,868 prompts and 317,200 feedback pairs obtained from our synthetic\npersonas. Leveraging this dataset, we systematically evaluate LM capabilities\nin role-playing diverse users, verified through human judges, and the\nestablishment of both a benchmark, PERSONA Bench, for pluralistic alignment\napproaches as well as an extensive dataset to create new and future benchmarks.\nThe full dataset and benchmarks are available here:\nhttps://www.synthlabs.ai/research/persona.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17387",
        "PDF Link": "https://arxiv.org/pdf/2407.17387",
        "Upvotes": "15",
        "Date": "2024-07-29"
    },
    {
        "Title": "Longhorn: State Space Models are Amortized Online Learners",
        "Abstract": "The most fundamental capability of modern AI methods such as Large Language\nModels (LLMs) is the ability to predict the next token in a long sequence of\ntokens, known as ``sequence modeling.\" Although the Transformers model is the\ncurrent dominant approach to sequence modeling, its quadratic computational\ncost with respect to sequence length is a significant drawback. State-space\nmodels (SSMs) offer a promising alternative due to their linear decoding\nefficiency and high parallelizability during training. However, existing SSMs\noften rely on seemingly ad hoc linear recurrence designs. In this work, we\nexplore SSM design through the lens of online learning, conceptualizing SSMs as\nmeta-modules for specific online learning problems. This approach links SSM\ndesign to formulating precise online learning objectives, with state transition\nrules derived from optimizing these objectives. Based on this insight, we\nintroduce a novel deep SSM architecture based on the implicit update for\noptimizing an online regression objective. Our experimental results show that\nour models outperform state-of-the-art SSMs, including the Mamba model, on\nstandard sequence modeling benchmarks and language modeling tasks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14207",
        "PDF Link": "https://arxiv.org/pdf/2407.14207",
        "Upvotes": "14",
        "Date": "2024-07-29"
    },
    {
        "Title": "SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View\n  Consistency",
        "Abstract": "We present Stable Video 4D (SV4D), a latent video diffusion model for\nmulti-frame and multi-view consistent dynamic 3D content generation. Unlike\nprevious methods that rely on separately trained generative models for video\ngeneration and novel view synthesis, we design a unified diffusion model to\ngenerate novel view videos of dynamic 3D objects. Specifically, given a\nmonocular reference video, SV4D generates novel views for each video frame that\nare temporally consistent. We then use the generated novel view videos to\noptimize an implicit 4D representation (dynamic NeRF) efficiently, without the\nneed for cumbersome SDS-based optimization used in most prior works. To train\nour unified novel view video generation model, we curated a dynamic 3D object\ndataset from the existing Objaverse dataset. Extensive experimental results on\nmultiple datasets and user studies demonstrate SV4D's state-of-the-art\nperformance on novel-view video synthesis as well as 4D generation compared to\nprior works.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17470",
        "PDF Link": "https://arxiv.org/pdf/2407.17470",
        "Upvotes": "12",
        "Date": "2024-07-29"
    },
    {
        "Title": "Learning to Manipulate Anywhere: A Visual Generalizable Framework For\n  Reinforcement Learning",
        "Abstract": "Can we endow visuomotor robots with generalization capabilities to operate in\ndiverse open-world scenarios? In this paper, we propose Maniwhere, a\ngeneralizable framework tailored for visual reinforcement learning, enabling\nthe trained robot policies to generalize across a combination of multiple\nvisual disturbance types. Specifically, we introduce a multi-view\nrepresentation learning approach fused with Spatial Transformer Network (STN)\nmodule to capture shared semantic information and correspondences among\ndifferent viewpoints. In addition, we employ a curriculum-based randomization\nand augmentation approach to stabilize the RL training process and strengthen\nthe visual generalization ability. To exhibit the effectiveness of Maniwhere,\nwe meticulously design 8 tasks encompassing articulate objects, bi-manual, and\ndexterous hand manipulation tasks, demonstrating Maniwhere's strong visual\ngeneralization and sim2real transfer abilities across 3 hardware platforms. Our\nexperiments show that Maniwhere significantly outperforms existing\nstate-of-the-art methods. Videos are provided at\nhttps://gemcollector.github.io/maniwhere/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15815",
        "PDF Link": "https://arxiv.org/pdf/2407.15815",
        "Upvotes": "10",
        "Date": "2024-07-29"
    },
    {
        "Title": "ViPer: Visual Personalization of Generative Models via Individual\n  Preference Learning",
        "Abstract": "Different users find different images generated for the same prompt\ndesirable. This gives rise to personalized image generation which involves\ncreating images aligned with an individual's visual preference. Current\ngenerative models are, however, unpersonalized, as they are tuned to produce\noutputs that appeal to a broad audience. Using them to generate images aligned\nwith individual users relies on iterative manual prompt engineering by the user\nwhich is inefficient and undesirable. We propose to personalize the image\ngeneration process by first capturing the generic preferences of the user in a\none-time process by inviting them to comment on a small selection of images,\nexplaining why they like or dislike each. Based on these comments, we infer a\nuser's structured liked and disliked visual attributes, i.e., their visual\npreference, using a large language model. These attributes are used to guide a\ntext-to-image model toward producing images that are tuned towards the\nindividual user's visual preference. Through a series of user studies and large\nlanguage model guided evaluations, we demonstrate that the proposed method\nresults in generations that are well aligned with individual users' visual\npreferences.",
        "ArXiv Link": "https://arxiv.org/abs/2407.17365",
        "PDF Link": "https://arxiv.org/pdf/2407.17365",
        "Upvotes": "10",
        "Date": "2024-07-29"
    },
    {
        "Title": "MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent\n  Reinforcement Learning",
        "Abstract": "Many challenging tasks such as managing traffic systems, electricity grids,\nor supply chains involve complex decision-making processes that must balance\nmultiple conflicting objectives and coordinate the actions of various\nindependent decision-makers (DMs). One perspective for formalising and\naddressing such tasks is multi-objective multi-agent reinforcement learning\n(MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple\nagents each needing to consider multiple objectives in their learning process.\nIn reinforcement learning research, benchmarks are crucial in facilitating\nprogress, evaluation, and reproducibility. The significance of benchmarks is\nunderscored by the existence of numerous benchmark frameworks developed for\nvarious RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent\nRL (e.g., PettingZoo), and single-agent multi-objective RL (e.g.,\nMO-Gymnasium). To support the advancement of the MOMARL field, we introduce\nMOMAland, the first collection of standardised environments for multi-objective\nmulti-agent reinforcement learning. MOMAland addresses the need for\ncomprehensive benchmarking in this emerging field, offering over 10 diverse\nenvironments that vary in the number of agents, state representations, reward\nstructures, and utility considerations. To provide strong baselines for future\nresearch, MOMAland also includes algorithms capable of learning policies in\nsuch settings.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16312",
        "PDF Link": "https://arxiv.org/pdf/2407.16312",
        "Upvotes": "9",
        "Date": "2024-07-29"
    },
    {
        "Title": "Scalify: scale propagation for efficient low-precision LLM training",
        "Abstract": "Low-precision formats such as float8 have been introduced in machine learning\naccelerated hardware to improve computational efficiency for large language\nmodels training and inference. Nevertheless, adoption by the ML community has\nbeen slowed down by the complex, and sometimes brittle, techniques required to\nmatch higher precision training accuracy. In this work, we present Scalify, a\nend-to-end scale propagation paradigm for computational graphs, generalizing\nand formalizing existing tensor scaling methods. Experiment results show that\nScalify supports out-of-the-box float8 matrix multiplication and gradients\nrepresentation, as well as float16 optimizer state storage. Our JAX\nimplementation of Scalify is open-sourced at\nhttps://github.com/graphcore-research/jax-scalify",
        "ArXiv Link": "https://arxiv.org/abs/2407.17353",
        "PDF Link": "https://arxiv.org/pdf/2407.17353",
        "Upvotes": "9",
        "Date": "2024-07-29"
    },
    {
        "Title": "DistilDIRE: A Small, Fast, Cheap and Lightweight Diffusion Synthesized\n  Deepfake Detection",
        "Abstract": "A dramatic influx of diffusion-generated images has marked recent years,\nposing unique challenges to current detection technologies. While the task of\nidentifying these images falls under binary classification, a seemingly\nstraightforward category, the computational load is significant when employing\nthe \"reconstruction then compare\" technique. This approach, known as DIRE\n(Diffusion Reconstruction Error), not only identifies diffusion-generated\nimages but also detects those produced by GANs, highlighting the technique's\nbroad applicability. To address the computational challenges and improve\nefficiency, we propose distilling the knowledge embedded in diffusion models to\ndevelop rapid deepfake detection models. Our approach, aimed at creating a\nsmall, fast, cheap, and lightweight diffusion synthesized deepfake detector,\nmaintains robust performance while significantly reducing operational demands.\nMaintaining performance, our experimental results indicate an inference speed\n3.2 times faster than the existing DIRE framework. This advance not only\nenhances the practicality of deploying these systems in real-world settings but\nalso paves the way for future research endeavors that seek to leverage\ndiffusion model knowledge.",
        "ArXiv Link": "https://arxiv.org/abs/2406.00856",
        "PDF Link": "https://arxiv.org/pdf/2406.00856",
        "Upvotes": "8",
        "Date": "2024-07-29"
    },
    {
        "Title": "DreamCar: Leveraging Car-specific Prior for in-the-wild 3D Car\n  Reconstruction",
        "Abstract": "Self-driving industries usually employ professional artists to build\nexquisite 3D cars. However, it is expensive to craft large-scale digital\nassets. Since there are already numerous datasets available that contain a vast\nnumber of images of cars, we focus on reconstructing high-quality 3D car models\nfrom these datasets. However, these datasets only contain one side of cars in\nthe forward-moving scene. We try to use the existing generative models to\nprovide more supervision information, but they struggle to generalize well in\ncars since they are trained on synthetic datasets not car-specific. In\naddition, The reconstructed 3D car texture misaligns due to a large error in\ncamera pose estimation when dealing with in-the-wild images. These restrictions\nmake it challenging for previous methods to reconstruct complete 3D cars. To\naddress these problems, we propose a novel method, named DreamCar, which can\nreconstruct high-quality 3D cars given a few images even a single image. To\ngeneralize the generative model, we collect a car dataset, named Car360, with\nover 5,600 vehicles. With this dataset, we make the generative model more\nrobust to cars. We use this generative prior specific to the car to guide its\nreconstruction via Score Distillation Sampling. To further complement the\nsupervision information, we utilize the geometric and appearance symmetry of\ncars. Finally, we propose a pose optimization method that rectifies poses to\ntackle texture misalignment. Extensive experiments demonstrate that our method\nsignificantly outperforms existing methods in reconstructing high-quality 3D\ncars. https://xiaobiaodu.github.io/dreamcar-project/{Our code is\navailable.}",
        "ArXiv Link": "https://arxiv.org/abs/2407.16988",
        "PDF Link": "https://arxiv.org/pdf/2407.16988",
        "Upvotes": "6",
        "Date": "2024-07-29"
    },
    {
        "Title": "CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis",
        "Abstract": "The field of medical diagnosis has undergone a significant transformation\nwith the advent of large language models (LLMs), yet the challenges of\ninterpretability within these models remain largely unaddressed. This study\nintroduces Chain-of-Diagnosis (CoD) to enhance the interpretability of\nLLM-based medical diagnostics. CoD transforms the diagnostic process into a\ndiagnostic chain that mirrors a physician's thought process, providing a\ntransparent reasoning pathway. Additionally, CoD outputs the disease confidence\ndistribution to ensure transparency in decision-making. This interpretability\nmakes model diagnostics controllable and aids in identifying critical symptoms\nfor inquiry through the entropy reduction of confidences. With CoD, we\ndeveloped DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental\nresults demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic\nbenchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring\ncontrollability in diagnostic rigor.",
        "ArXiv Link": "https://arxiv.org/abs/2407.13301",
        "PDF Link": "https://arxiv.org/pdf/2407.13301",
        "Upvotes": "52",
        "Date": "2024-07-29"
    },
    {
        "Title": "KAN or MLP: A Fairer Comparison",
        "Abstract": "This paper does not introduce a novel method. Instead, it offers a fairer and\nmore comprehensive comparison of KAN and MLP models across various tasks,\nincluding machine learning, computer vision, audio processing, natural language\nprocessing, and symbolic formula representation. Specifically, we control the\nnumber of parameters and FLOPs to compare the performance of KAN and MLP. Our\nmain observation is that, except for symbolic formula representation tasks, MLP\ngenerally outperforms KAN. We also conduct ablation studies on KAN and find\nthat its advantage in symbolic formula representation mainly stems from its\nB-spline activation function. When B-spline is applied to MLP, performance in\nsymbolic formula representation significantly improves, surpassing or matching\nthat of KAN. However, in other tasks where MLP already excels over KAN,\nB-spline does not substantially enhance MLP's performance. Furthermore, we find\nthat KAN's forgetting issue is more severe than that of MLP in a standard\nclass-incremental continual learning setting, which differs from the findings\nreported in the KAN paper. We hope these results provide insights for future\nresearch on KAN and other MLP alternatives. Project link:\nhttps://github.com/yu-rp/KANbeFair",
        "ArXiv Link": "https://arxiv.org/abs/2407.16674",
        "PDF Link": "https://arxiv.org/pdf/2407.16674",
        "Upvotes": "34",
        "Date": "2024-07-29"
    },
    {
        "Title": "MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence",
        "Abstract": "Recent advancements in video generation have primarily leveraged diffusion\nmodels for short-duration content. However, these approaches often fall short\nin modeling complex narratives and maintaining character consistency over\nextended periods, which is essential for long-form video production like\nmovies. We propose MovieDreamer, a novel hierarchical framework that integrates\nthe strengths of autoregressive models with diffusion-based rendering to\npioneer long-duration video generation with intricate plot progressions and\nhigh visual fidelity. Our approach utilizes autoregressive models for global\nnarrative coherence, predicting sequences of visual tokens that are\nsubsequently transformed into high-quality video frames through diffusion\nrendering. This method is akin to traditional movie production processes, where\ncomplex stories are factorized down into manageable scene capturing. Further,\nwe employ a multimodal script that enriches scene descriptions with detailed\ncharacter information and visual style, enhancing continuity and character\nidentity across scenes. We present extensive experiments across various movie\ngenres, demonstrating that our approach not only achieves superior visual and\nnarrative quality but also effectively extends the duration of generated\ncontent significantly beyond current capabilities. Homepage:\nhttps://aim-uofa.github.io/MovieDreamer/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16655",
        "PDF Link": "https://arxiv.org/pdf/2407.16655",
        "Upvotes": "25",
        "Date": "2024-07-29"
    },
    {
        "Title": "T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video\n  Generation",
        "Abstract": "Text-to-video (T2V) generation models have advanced significantly, yet their\nability to compose different objects, attributes, actions, and motions into a\nvideo remains unexplored. Previous text-to-video benchmarks also neglect this\nimportant ability for evaluation. In this work, we conduct the first systematic\nstudy on compositional text-to-video generation. We propose T2V-CompBench, the\nfirst benchmark tailored for compositional text-to-video generation.\nT2V-CompBench encompasses diverse aspects of compositionality, including\nconsistent attribute binding, dynamic attribute binding, spatial relationships,\nmotion binding, action binding, object interactions, and generative numeracy.\nWe further carefully design evaluation metrics of MLLM-based metrics,\ndetection-based metrics, and tracking-based metrics, which can better reflect\nthe compositional text-to-video generation quality of seven proposed categories\nwith 700 text prompts. The effectiveness of the proposed metrics is verified by\ncorrelation with human evaluations. We also benchmark various text-to-video\ngenerative models and conduct in-depth analysis across different models and\ndifferent compositional categories. We find that compositional text-to-video\ngeneration is highly challenging for current models, and we hope that our\nattempt will shed light on future research in this direction.",
        "ArXiv Link": "https://arxiv.org/abs/2407.14505",
        "PDF Link": "https://arxiv.org/pdf/2407.14505",
        "Upvotes": "21",
        "Date": "2024-07-29"
    },
    {
        "Title": "OutfitAnyone: Ultra-high Quality Virtual Try-On for Any Clothing and Any\n  Person",
        "Abstract": "Virtual Try-On (VTON) has become a transformative technology, empowering\nusers to experiment with fashion without ever having to physically try on\nclothing. However, existing methods often struggle with generating\nhigh-fidelity and detail-consistent results. While diffusion models, such as\nStable Diffusion series, have shown their capability in creating high-quality\nand photorealistic images, they encounter formidable challenges in conditional\ngeneration scenarios like VTON. Specifically, these models struggle to maintain\na balance between control and consistency when generating images for virtual\nclothing trials. OutfitAnyone addresses these limitations by leveraging a\ntwo-stream conditional diffusion model, enabling it to adeptly handle garment\ndeformation for more lifelike results. It distinguishes itself with\nscalability-modulating factors such as pose, body shape and broad\napplicability, extending from anime to in-the-wild images. OutfitAnyone's\nperformance in diverse scenarios underscores its utility and readiness for\nreal-world deployment. For more details and animated results, please see\nhttps://humanaigc.github.io/outfit-anyone/.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16224",
        "PDF Link": "https://arxiv.org/pdf/2407.16224",
        "Upvotes": "20",
        "Date": "2024-07-29"
    },
    {
        "Title": "INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal\n  Large Language Model",
        "Abstract": "With advancements in data availability and computing resources, Multimodal\nLarge Language Models (MLLMs) have showcased capabilities across various\nfields. However, the quadratic complexity of the vision encoder in MLLMs\nconstrains the resolution of input images. Most current approaches mitigate\nthis issue by cropping high-resolution images into smaller sub-images, which\nare then processed independently by the vision encoder. Despite capturing\nsufficient local details, these sub-images lack global context and fail to\ninteract with one another. To address this limitation, we propose a novel MLLM,\nINF-LLaVA, designed for effective high-resolution image perception. INF-LLaVA\nincorporates two innovative components. First, we introduce a Dual-perspective\nCropping Module (DCM), which ensures that each sub-image contains continuous\ndetails from a local perspective and comprehensive information from a global\nperspective. Second, we introduce Dual-perspective Enhancement Module (DEM) to\nenable the mutual enhancement of global and local features, allowing INF-LLaVA\nto effectively process high-resolution images by simultaneously capturing\ndetailed local information and comprehensive global context. Extensive ablation\nstudies validate the effectiveness of these components, and experiments on a\ndiverse set of benchmarks demonstrate that INF-LLaVA outperforms existing\nMLLMs. Code and pretrained model are available at\nhttps://github.com/WeihuangLin/INF-LLaVA.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16198",
        "PDF Link": "https://arxiv.org/pdf/2407.16198",
        "Upvotes": "12",
        "Date": "2024-07-29"
    },
    {
        "Title": "F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions",
        "Abstract": "Existing 3D human object interaction (HOI) datasets and models simply align\nglobal descriptions with the long HOI sequence, while lacking a detailed\nunderstanding of intermediate states and the transitions between states. In\nthis paper, we argue that fine-grained semantic alignment, which utilizes\nstate-level descriptions, offers a promising paradigm for learning semantically\nrich HOI representations. To achieve this, we introduce Semantic-HOI, a new\ndataset comprising over 20K paired HOI states with fine-grained descriptions\nfor each HOI state and the body movements that happen between two consecutive\nstates. Leveraging the proposed dataset, we design three state-level HOI tasks\nto accomplish fine-grained semantic alignment within the HOI sequence.\nAdditionally, we propose a unified model called F-HOI, designed to leverage\nmultimodal instructions and empower the Multi-modal Large Language Model to\nefficiently handle diverse HOI tasks. F-HOI offers multiple advantages: (1) It\nemploys a unified task formulation that supports the use of versatile\nmultimodal inputs. (2) It maintains consistency in HOI across 2D, 3D, and\nlinguistic spaces. (3) It utilizes fine-grained textual supervision for direct\noptimization, avoiding intricate modeling of HOI states. Extensive experiments\nreveal that F-HOI effectively aligns HOI states with fine-grained semantic\ndescriptions, adeptly tackling understanding, reasoning, generation, and\nreconstruction tasks.",
        "ArXiv Link": "https://arxiv.org/abs/2407.12435",
        "PDF Link": "https://arxiv.org/pdf/2407.12435",
        "Upvotes": "10",
        "Date": "2024-07-29"
    },
    {
        "Title": "A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data",
        "Abstract": "Despite the availability of international prize-money competitions, scaled\nvehicles, and simulation environments, research on autonomous racing and the\ncontrol of sports cars operating close to the limit of handling has been\nlimited by the high costs of vehicle acquisition and management, as well as the\nlimited physics accuracy of open-source simulators. In this paper, we propose a\nracing simulation platform based on the simulator Assetto Corsa to test,\nvalidate, and benchmark autonomous driving algorithms, including reinforcement\nlearning (RL) and classical Model Predictive Control (MPC), in realistic and\nchallenging scenarios. Our contributions include the development of this\nsimulation platform, several state-of-the-art algorithms tailored to the racing\nenvironment, and a comprehensive dataset collected from human drivers.\nAdditionally, we evaluate algorithms in the offline RL setting. All the\nnecessary code (including environment and benchmarks), working examples,\ndatasets, and videos are publicly released and can be found at:\nhttps://assetto-corsa-gym.github.io.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16680",
        "PDF Link": "https://arxiv.org/pdf/2407.16680",
        "Upvotes": "9",
        "Date": "2024-07-29"
    },
    {
        "Title": "SIGMA: Sinkhorn-Guided Masked Video Modeling",
        "Abstract": "Video-based pretraining offers immense potential for learning strong visual\nrepresentations on an unprecedented scale. Recently, masked video modeling\nmethods have shown promising scalability, yet fall short in capturing\nhigher-level semantics due to reconstructing predefined low-level targets such\nas pixels. To tackle this, we present Sinkhorn-guided Masked Video Modelling\n(SIGMA), a novel video pretraining method that jointly learns the video model\nin addition to a target feature space using a projection network. However, this\nsimple modification means that the regular L2 reconstruction loss will lead to\ntrivial solutions as both networks are jointly optimized. As a solution, we\ndistribute features of space-time tubes evenly across a limited number of\nlearnable clusters. By posing this as an optimal transport problem, we enforce\nhigh entropy in the generated features across the batch, infusing semantic and\ntemporal meaning into the feature space. The resulting cluster assignments are\nused as targets for a symmetric prediction task where the video model predicts\ncluster assignment of the projection network and vice versa. Experimental\nresults on ten datasets across three benchmarks validate the effectiveness of\nSIGMA in learning more performant, temporally-aware, and robust video\nrepresentations improving upon state-of-the-art methods. Our project website\nwith code is available at: https://quva-lab.github.io/SIGMA.",
        "ArXiv Link": "https://arxiv.org/abs/2407.15447",
        "PDF Link": "https://arxiv.org/pdf/2407.15447",
        "Upvotes": "5",
        "Date": "2024-07-29"
    },
    {
        "Title": "PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing",
        "Abstract": "Deploying language models (LMs) necessitates outputs to be both high-quality\nand compliant with safety guidelines. Although Inference-Time Guardrails (ITG)\noffer solutions that shift model output distributions towards compliance, we\nfind that current methods struggle in balancing safety with helpfulness. ITG\nMethods that safely address non-compliant queries exhibit lower helpfulness\nwhile those that prioritize helpfulness compromise on safety. We refer to this\ntrade-off as the guardrail tax, analogous to the alignment tax. To address\nthis, we propose PrimeGuard, a novel ITG method that utilizes structured\ncontrol flow.\n  PrimeGuard routes requests to different self-instantiations of the LM with\nvarying instructions, leveraging its inherent instruction-following\ncapabilities and in-context learning. Our tuning-free approach dynamically\ncompiles system-designer guidelines for each query. We construct and release\nsafe-eval, a diverse red-team safety benchmark. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax\nby (1) significantly increasing resistance to iterative jailbreak attacks and\n(2) achieving state-of-the-art results in safety guardrailing while (3)\nmatching helpfulness scores of alignment-tuned models. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, outperforms all competing\nbaselines and overcomes the guardrail tax by improving the fraction of safe\nresponses from 61% to 97% and increasing average helpfulness scores from 4.17\nto 4.29 on the largest models, while reducing attack success rate from 100% to\n8%.\n  PrimeGuard implementation is available at\nhttps://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at\nhttps://huggingface.co/datasets/dynamoai/safe_eval.",
        "ArXiv Link": "https://arxiv.org/abs/2407.16318",
        "PDF Link": "https://arxiv.org/pdf/2407.16318",
        "Upvotes": "4",
        "Date": "2024-07-29"
    },
    {
        "Title": "Cross Anything: General Quadruped Robot Navigation through Complex\n  Terrains",
        "Abstract": "The application of vision-language models (VLMs) has achieved impressive\nsuccess in various robotics tasks, but there are few explorations for\nfoundation models used in quadruped robot navigation. We introduce Cross\nAnything System (CAS), an innovative system composed of a high-level reasoning\nmodule and a low-level control policy, enabling the robot to navigate across\ncomplex 3D terrains and reach the goal position. For high-level reasoning and\nmotion planning, we propose a novel algorithmic system taking advantage of a\nVLM, with a design of task decomposition and a closed-loop sub-task execution\nmechanism. For low-level locomotion control, we utilize the Probability\nAnnealing Selection (PAS) method to train a control policy by reinforcement\nlearning. Numerous experiments show that our whole system can accurately and\nrobustly navigate across complex 3D terrains, and its strong generalization\nability ensures the applications in diverse indoor and outdoor scenarios and\nterrains. Project page: https://cross-anything.github.io/",
        "ArXiv Link": "https://arxiv.org/abs/2407.16412",
        "PDF Link": "https://arxiv.org/pdf/2407.16412",
        "Upvotes": "3",
        "Date": "2024-07-29"
    }
]